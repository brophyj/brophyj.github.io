[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Collider risk stratification bias\n\n\n\n\n\n\nBias\n\n\nRCTs\n\n\n\nThe bias that refuses to die\n\n\n\n\n\nFriday, the 4th of April, 2025\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nSmall print\n\n\n\n\n\n\nBias\n\n\nRCTs\n\n\n\nIt’s importance as a flag for questionable science\n\n\n\n\n\nMonday, the 27th of January, 2025\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nA reanalysis of PLATO\n\n\n\n\n\n\nHierarchical model\n\n\nRCTs\n\n\n\nWhich statistical model best reflects the data generating mechanism?\n\n\n\n\n\nMonday, the 30th of December, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal beliefs\n\n\n\n\n\n\nHierarchical model\n\n\nRCTs\n\n\n\nA colchicine example - CLEAR OASIS 9\n\n\n\n\n\nSunday, the 29th of December, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on peer review - part 2\n\n\n\n\n\n\nPeer review\n\n\nTransparency\n\n\n\nHow bad must it be before retraction?\n\n\n\n\n\nThursday, the 31st of October, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on peer review\n\n\n\n\n\n\nPeer review\n\n\nTransparency\n\n\n\nYou can’t be serious!\n\n\n\n\n\nSaturday, the 5th of October, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Uncertainties\n\n\n\n\n\n\nUncertainty\n\n\nProbability distributions\n\n\n\nA quantitative exercise\n\n\n\n\n\nSunday, the 29th of September, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nSomething old with spin (FAME-2)\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nCherry picking with the best\n\n\n\n\n\nSaturday, the 14th of September, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMortality following post operative atrial fibrillation\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nDoes sex matter?\n\n\n\n\n\nMonday, the 26th of August, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nTAVR vs. surgery - NHST gets it wrong (again)\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nMaking intelligent inferences\n\n\n\n\n\nMonday, the 5th of August, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nDoes it make a difference - sedentary break\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nWho counts?\n\n\n\n\n\nTuesday, the 5th of March, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nEndotoxin exposure in asthmatic children and matched healthy controls\n\n\n\n\n\n\nBias\n\n\n\nWhat counts?\n\n\n\n\n\nMonday, the 19th of February, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the prevalence of depression in medical students?\n\n\n\n\n\n\nEvidence based medicine\n\n\nMeta-analysis\n\n\n\nNo reliable answer from this study\n\n\n\n\n\nMonday, the 5th of February, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nA personal (and disturbing) conversation with ChatGPT\n\n\n\n\n\n\nAI\n\n\n\nHonest but troubling\n\n\n\n\n\nTuesday, the 18th of April, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMisinformed conclusions\n\n\n\n\n\n\nEvidence based medicine\n\n\nBayesian stats\n\n\n\ncaveat lector\n\n\n\n\n\nSaturday, the 15th of April, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nSomething old and something older\n\n\n\n\n\n\nBias\n\n\nEvidence based medicine\n\n\n\nImmortal time bias, longstanding and persistent\n\n\n\n\n\nWednesday, the 22nd of March, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nAbsence of evidence is not evidence of absence\n\n\n\n\n\n\nBayesian stats\n\n\n\nBayes to the rescue?\n\n\n\n\n\nSunday, the 19th of March, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does scientific authorship mean?\n\n\n\n\n\n\nEvidence based medicine\n\n\nBayesian stats\n\n\n\nSometimes, not much\n\n\n\n\n\nMonday, the 5th of December, 2022\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMedication Withdrawal - When Reasonable People Disagree\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nUnderstanding the past\n\n\n\n\n\nFriday, the 17th of December, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nInaccurate References; Guile, Duplicity, and Perfidious Publications\n\n\n\n\n\n\nEvidence based medicine\n\n\nGuidelines\n\n\nBayesian stats\n\n\n\nTrusting the guidelines?\n\n\n\n\n\nSunday, the 12th of December, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-Offs Versus Obfuscation\n\n\n\n\n\n\nBias\n\n\nEvidence based medicine\n\n\n\nReporting biases?\n\n\n\n\n\nFriday, the 26th of November, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid testing in travellers\n\n\n\n\n\n\nStatistical analysis\n\n\n\nWorthwhile, I think not\n\n\n\n\n\nThursday, the 18th of November, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nHeparin in hospitalized COVID-19 patients\n\n\n\n\n\n\nStatistical analysis\n\n\n\nFaulty stats?\n\n\n\n\n\nTuesday, the 12th of October, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nNew Covid drug\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nHeavy on the hype?\n\n\n\n\n\nMonday, the 4th of October, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid vaccine myocarditis\n\n\n\n\n\n\nEvidence based medicine\n\n\nGuidelines\n\n\nBayesian stats\n\n\n\nWhere is the common sense?\n\n\n\n\n\nWednesday, the 29th of September, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nEXCEL - clearing the haze with Bayes\n\n\n\n\n\n\nBayesian stats\n\n\n\nAdding clarity?\n\n\n\n\n\nFriday, the 15th of January, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate on the science behind masks\n\n\n\n\n\n\nBayesian stats\n\n\n\nGood enough?\n\n\n\n\n\nTuesday, the 24th of November, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID, masks and poor science\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nQuestionable interpretation?\n\n\n\n\n\nFriday, the 17th of July, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nPharma, bribes and MDs\n\n\n\n\n\n\nBias\n\n\n\nCorrelation or causality?\n\n\n\n\n\nWednesday, the 8th of July, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nGlad to be at the back of the queue\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nHow good is the evidence?\n\n\n\n\n\nMonday, the 6th of July, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nVirtual Learning & Corporate Sponsorship\n\n\n\n\n\n\nBias\n\n\n\nTime to worry?\n\n\n\n\n\nFriday, the 15th of May, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMontreal, a global Covid perspective\n\n\n\n\n\n\nVisualizations\n\n\n\nInformation bias?\n\n\n\n\n\nTuesday, the 12th of May, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid visualizations\n\n\n\n\n\n\nVisualizations\n\n\n\nAn aid to understanding\n\n\n\n\n\nTuesday, the 5th of May, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCABG vs. PCI in LM\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nGood enough?\n\n\n\n\n\nThursday, the 16th of April, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nDig is OK\n\n\n\n\n\n\nEvidence based medicine\n\n\n\n\n\n\n\n\n\nFriday, the 19th of July, 2019\n\n\nJay Brophy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "2024",
    "section": "",
    "text": "Selected publications \n\ntest\n\nDownload full (current) CV\n\n\n2024\n\nMiceline Mésidor, Caroline Sirois, Jason Robert Guertin, Mireille E Schnitzer, Bernard Candas, Claudia Blais, Benoit Cossette, Paul Poirier, James M Brophy, Lisa Lix, Mina Tadrous, Awa Diop, Denis Hamel, Denis Talbot. Effect of statin use for the primary prevention of cardiovascular disease among older adults: A cautionary tale concerning target trials emulation. Journal of Clinical Epidemiology\nJames M Brophy Is science really self correcting? Something old and older. Medium.\nNon peer review article based on a previous blog. Although I think this contains much interesting material, including the value of teaching,an exploration of immortal time bias, and its historical perspective, it was refused by &gt; 6 (I lost count of exactly how many!) medical journals for “not meeting their priorities”.\n\n\n\n2023\n\nJames M Brophy. A Bayesian Perspective-Extracorporeal CPR for Refractory Out-of-Hospital Cardiac Arrest medRxiv, 2023.02. 13.23285890\n\n\n\n2022\n\nTodd C Lee, Srinivas Murthy, Olivier Del Corpo, Julien Senécal, Guillaume Butler-Laporte, Zahra N Sohani, James M Brophy, Emily G McDonald. Remdesivir for the treatment of COVID-19: a systematic review and meta-analysis. Clinical Microbiology and Infection 28 (9), 1203-1210\n\n\n\n2021\n\nJames M Brophy. Bayesian analyses of cardiovascular trials—bringing added value to the table. Canadian Journal of Cardiology 37 (9), 1415-1427 2021\n\nJames M Brophy. Key Issues in the Statistical Interpretation of Randomized Clinical Trials. Canadian Journal of Cardiology 37 (9), 1312-1321\n\n\n\n2020\n\nJames M Brophy. Bayesian interpretation of the EXCEL trial and other randomized clinical trials of left main coronary artery revascularization. JAMA Internal Medicine 180 (7), 986-992"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Aortic stenosis",
    "section": "",
    "text": "Selected projects \n\n\nAortic stenosis\n\n\n\n\n\n\n\nAortic stenosis treatments\n\n\nA CIHR funded project investigating the impact of patient, institutional, physician, and regional data on the trajectory of aortic stenosis (AS) treatment choices, outcomes, and cost effectiveness\nOverview\nAortic stenosis (AS) is the most prevalent heart valve disorder in developed countries. Historically, surgical aortic valve replacement (SAVR) has been the standard treatment for symptomatic AS. Trans-catheter aortic valve replacement (TAVR) has now been demonstrated to significantly reduce mortality for inoperable AS patients compared to medical treatment, and to have comparable or improved outcomes compared across a spectrum of SAVR candidates. The most recent randomized trial has shown TAVR to be noninferior to surgery with respect to the composite end point of death or disabling stroke at 24 months follow-up in patients at low surgical risk. TAVR has, and will continue to, dramatically change the way patients with AS are managed and may be seen as “disruptive” technology.\nIt is important to have a clear understanding of the diagnostic, treatment and outcomes experienced by these patients. This is necessary to be sure the appropriate resources are available to meet these demands in a fair, transparent, and equitable manner.\n\n\nComparative effectiveness studies of dual antiplatelet therapy (DAPT)\n\n\n\n\n\n\n\nDAPT\n\n\nTicagrelor compared to clopidogrel in acute coronary syndromes (TC4) - the TC4 comparative effectiveness study\nCIHR funded project (2019-2023) looking at the comparative effectiveness of ticagrelor versus clopidogrel in acute coronary syndromes.\nOverview Cardiovascular diseases, especially acute coronary syndromes (ACS) are a leading cause of mortality and morbidity. Antiplatelet medications have a long history of improving ACS survival and for the prevention of a secondary cardiac event. Based on the consistency and strength of multiple randomized controlled trials (RCTs), the dual antiplatelet therapy (DAPT), clopidogrel and aspirin, following an ACS event, had long been considered the gold standard in the leading clinical guidelines. More recently, a new DAPT regimen, ticagrelor and aspirin, has become the recommended strategy. However this is based largely on one large RCT with a limited number of North American patients (who did not show any incremental benefit).\nThe Ticagrelor Compared to Clopidogrel in aCute coronary syndromes – the TC4 trial (NCT04057300) – is a pragmatic, open-label, active control, time clustered randomized clinical trial designed to assess the effectiveness and safety of ticagrelor versus clopidogrel as DAPT therapy in a ACS population undergoing percutaneous coronary intervention (PCI) in a single tertiary academic center in Montreal, Canada.\n\nIn addition to a standalone analysis, our main Bayesian analysis will incorporate the 1800 PLATO NA patients as our prior distribution, essentially providing a &gt; 50% safety margin for our sample size calculations (see next section).\n\nThe complete funding protocol is found here.\nThis research program will also explore the comparative effectiveness of these different treatments by performing a\ni) Bayesian network meta-analysis of all previous RCTs\nii) Pharmacoepidemiology study using Quebec administrative databases (manuscript in progress)\n\n\n(Mostly) Clinical Epidemiology with R\nThis is some introductory teaching material related to clinical epidemiology using R.\nHopefully at some point this material will be updated, expanded and repackaged using Quarto\n\n\n\nAortic stenosis treatments\nDAPT"
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "",
    "text": "On Nov 15 2021, the Lancet published an individual patient data (IPD) meta-analysis “Percutaneous coronary intervention with drug-eluting stents versus coronary artery bypass grafting in left main coronary artery disease” that concluded\n Among patients with left main coronary artery disease and, largely, low or intermediate coronary anatomical complexity, there was no statistically significant difference in 5-year all-cause death between PCI and CABG, although a Bayesian approach suggested a difference probably exists (more likely than not &lt;0·2% per year) favouring CABG. There were trade-offs in terms of the risk of myocardial infarction, stroke, and revascularisation. A heart team approach to communicate expected outcome differences might be useful to assist patients in reaching a treatment decision.   The paper is remarkable for several reasons:\n1. In addition to standard frequentist analyses, it included a Bayesian assessment of total mortality.\n2. The main text of the paper did not consider the composite primary endpoint, generally a composite of several cardiovascular outcomes including death, myocardial infarction, stroke or revascularizations, of these trials but reported only the individual endpoints.\n3. The conclusion speaks of trade-offs in the risk of myocardial infarction, stroke, and revascularisation implying there are supporting data for both interventions which does not accurately reflect the reality.\n4. While most would agree with the concept of heart team approach, this is only valid if fully transparent evidence is provided and by relegating the primary endpoint to supplemental material and excluding this from the more informative Bayesian analyses, the authors risk undermining their recommendation."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#bayesian-approach",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#bayesian-approach",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nThe Bayesian analysis is helpful as it moves discussion away from null hypothesis significance testing and towards the parameter estimation of the difference in mortality between the 2 approaches. I published a Bayesian analysis of the same datasets, using aggregate data,in JAMA Internal Medicine 18 months ago and although not referenced in the current article found the exact same probability for increased mortality with CABG. Given that one of the authors heavily criticized this previous work as being biased with analytical errors and omissions, I am thankful for the validation of my earlier results. However, given that this IPD meta-analysis would also include a systematic review, one could wonder if ignoring this previous work was not an intentional attempt at its scotomization."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#which-endpoint",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#which-endpoint",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Which endpoint",
    "text": "Which endpoint\nIt is surprising that the authors elected to focus their IPD not on the primary endpoint of the trials which was generally a composite of total mortality, myocardial infarction, stroke and, depending on the study, ischemia driven revascularization either as a primary or secondary outcome, but rather on the individual components of the composite outcome. It is only deep in the supplemental material that the results of the composite outcome can be found without any additional Bayesian analysis. It would be appreciated if the authors could confirm the earlier results of a 90% probability that the composite endpoint, including repeat revascularizations, is increased by at least 5 extra events per 100 patients treated with PCI."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#trade-offs",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#trade-offs",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Trade-offs",
    "text": "Trade-offs\nThe authors state ““There were trade-offs in terms of the risk of myocardial infarction, stroke, and revascularisation.” In addition, to the 86% probability of increased mortality at 5 years, the authors report statistically significant increases of 135% (HR 2·35 (1·71–3·23)) in spontaneous myocardial infarction and 78% (HR 1·78 (1·51–2·10) in the need for repeat revascularizations with PCI. Provided the patient has a reasonable life expectancy, all outcomes favor CABG except for stroke and that difference is not statistically significant (HR 0·84 (0·59–1·21)). In absolute terms, for every 1000 patients treated with PCI versus CABG, one would expect 35 more patients to have a spontaneous MI & 76 more to undergo repeat revascularization over 5 years, with no discernible difference in stroke. Therefore the question arises what ‘trade-offs’ are the authors referring to? Allowing the authors to highlight the increased stroke risk at 1 year with CABG seems more like a failure of the peer review system as this outcome was never prespecified in any of the original studies and seems purely data driven."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#heart-team-approach",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#heart-team-approach",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Heart team approach",
    "text": "Heart team approach\nThe authors recommend a heart team approach to communicating this information and I suspect that the medical community is largely in agreement, provided the totality of the evidence is presented in a transparent and unbiased manner, ideally incorporating the additional advantages of Bayesian analyses. Despite four of the 15 authors coming from surgical backgrounds, one may question if this article is an objective and nonpartisan accounting of the evidence. This reader is left with the impression of a cabal of conflicted cardiologists attempting to obfuscate the evidence, aided and abetted by an ineffective peer review system."
  },
  {
    "objectID": "posts/2020-04-16-cabg/index.html",
    "href": "posts/2020-04-16-cabg/index.html",
    "title": "CABG vs. PCI in LM",
    "section": "",
    "text": "This is a blog post about a recent NEJM article. \nBased on the primary outcome result (difference, 2.8 percentage points; 95% confidence interval [CI], −0.9 to 6.5; P = 0.13), the EXCEL authors conclude “In patients with left main coronary artery disease of low or intermediate anatomical complexity, there was no significant difference between PCI and CABG”. Leaving aside that CABG was statistically superior in terms of total mortality, ischemia driven revascularization, and non-procedural myocardial infarction, it is questionable that the primary outcome data actually support their conclusion. EXCEL was originally designed as a non-inferiority (NI) study with a margin of 4.2%, which en passant many would consider too liberal. Even accepting that margin, how can a primary outcome with a 95% upper CI of 6.5%, exceeding the original NI margin be used to support their conclusion?\nDid you find this page helpful? Consider sharing it 🙌\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {CABG Vs. {PCI} in {LM}},\n  date = {2020-04-16},\n  url = {https://brophyj.github.io/posts/2020-04-16-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “CABG Vs. PCI in LM.” April 16, 2020. https://brophyj.github.io/posts/2020-04-16-my-blog-post/."
  },
  {
    "objectID": "posts/2021-10-04-new-covid-drug/index.html",
    "href": "posts/2021-10-04-new-covid-drug/index.html",
    "title": "New Covid drug",
    "section": "",
    "text": "On October 1 2021, Merck issed a press release stating that following an interim analysis of the Phase 3 MOVe-OUT trial, 7.3% (28/385) of patients treated with molnupiravir experienced the primary endpoint (death or hospitalization through day 29), compared with 14.1% (53/377) of placebo-treated patients. The study population were non-hospitalized adult patients with mild-to-moderate COVID-19. Given the almost 50% reduction in the primary outcome (p=0.0012), it is perhaps not surprising that the press release goes on to state that “Merck plans to submit an application for Emergency Use Authorization (EUA) to the U.S. FDA as soon as possible based on these findings and plans to submit marketing applications to other regulatory bodies worldwide.”\nWhile definitive conclusions are impossible without examining the complete data set and seeing the final publication, one may wonder if the data are as compelling as the press release suggests. 1.p values overestimate the strength of the evidence For example, if before the MOVe-OUT trial was performed, it was beleived there was a 50:50 chance of the null hypothesis (H\\(_o\\)) of no effect being true and the experiemental data resulted in a p = 0.0012 (z=3.29) as observed, then the minimum Bayes Factor is approximately 1/190 (1/BF) and the posterior probability that H\\(_o\\) is true falls to 0.5% (posterior odds = Bayes factor * prior odds). However a skeptic might say that the majority of drug trials fail and that interim results often provide over-estimates of the true effect (see below) therefore the prior odds that H\\(_o\\) is true may be considerably higher than 50:50, perhaps more like 75:25 or even 90:10. Under those prior beliefs, the probability of H\\(_o\\) still being true even after the MOVe-OUT trial data is 1.3% and 3.9% respectively. The MOVe-OUT results still appear strongly supportive of a benefit for molnupiravir but perhaps not quite as compelling as initially implied by p = 0.0012.\n2.Impact of stopping at interim analysis A meta-analysis has examined the results from 91 truncated RCTs with 424 matching nontruncated RCTs and concluded “Truncated RCTs were associated with greater effect sizes than RCTs not stopped early. This difference was independent of the presence of statistical stopping rules and was greatest in smaller studies.” Overall the relative risk (RR) in the truncated trials was 71% of the nontruncated ones. However in smaller trials with less than 200 events the truncated trials reported RR of only 37% what was observed in the nontruncated ones (see figure below from their publication).\nRRs vs the Total Number of Outcome Events in Truncated RCTs - Red line is average for small trials &lt; 200 events\nOf course, this does not mean the observed MOVe-OUT RR of 0.48 would be reduced by 63% (to OR = 19%) in a nontruncated trial but it does offer some idea of the potential uncertainities that can arise with trials that are stopped premarturely, even those accompanied by a very small p value.\nAs a concrete example of the dangers of interim analyses, consider the randomized PROWESS trial of drotrecogin alfa (AKA activated protein C or APC) involving 1690 patients with sepsis that was stopped prematurely when it showed a 6% mortality difference (p = 0.006). The drug was approved, although later when nontruncated trials showed no benefit the sponsor elected in 2011 to withdraw the drug from the market, judging that its risks exceeded any potential small benefits. 3.Fragility index (FI) as a measure of trial robustness The FI is the minimum number of patients who would have to change their event status to turn a statistically significant result to a non-significant result. For this data, the FI = 8. In other words, a reversal in statistical significance is achieved by inversing the status of 2 events in the placebo group and 6 non-events in the experimental group. This represents a change in status in only approximately 1% of the study sample. I don’t think the complete loss of statistical significance by a reversal in outcomes in only 1% of the study population provides the necessary alacrity for early approval. 4.Differentiating between statistical and clinical significance While the p value gives the probability of observing this or more extreme data given the null hypothesis, it is most often of more interest to calaculate the inverse probability, namely the probability of a hypothesis given the data. This inverse probability moves inference into the realm of Bayesian statistics. The achilles heel of Bayesian is felt by some to be its requirement of a prior distribution. To minimize the impact of the prior, one can choose a non-informative one whereby the final (posterior) probability is determined uniquely by the observed data. The probability distribution function (PDF) for the difference between molnupiravir and placebo from MOVe-OUT using a non-informative prior is plotted below.\nOne of the main advantages of this approach is that it assists in differentiating between statistical and clinical significance. We are less interested in the probability that there are simply fewer events with molnupiravir and more interested in the probability that this difference exceeds some clinically meaningful threshold. Thus while there is a 99.9% probability of fewer events with molnupiravir, there is only a 95% probability that the benefit is as large as a 3% decrease in the combined death & hospitalization outcome. Again this speaks to the high probability of benefit with molnupiravir but there is less certainty as to the magnitude of any relevant clinical benefit. 5.Safety There were no concerning safety signals noted from the MOVe-OUT trial but with only 775 patients there was insufficient power to rule out potential side effects. 6.Generalizability Recruitment in Latin America, Europe, and Africa accounted for 55%, 23% and 15% of the MOVe-OUT study 2population, respectively. This leaves at most 7% or approximately 50 patients randomized from North America centers. Are we certain that drug effects will not vary across these different populations with their varying ancillary treatments and health care systems? Perhaps there is no no between country variation but this is another point of uncertainty that still requires some resolution. 7.Cost Given a 5 day treatment is expected to cost about $700 and that Merck has projected to produce 10 million courses of treatment in 2021, the cost of molnupiravir could reach several billion dollars, highlighting again the need for further studies to reduce the uncertainty regarding the magnitude of any benefits, of any risks, and the generalizability of the results. In conclusion from my perspective, although these initial trial results are very promising, it would seem prudent to request a confirmation study, ideally mainly in a North American population, before committing a large amount of resources to this drug. In most healthcare systems, resources are not a continually expanding commodity and a commitment to this drug will come with a reduction of resources elsewhere and hence the need to make maximally informed decisions."
  },
  {
    "objectID": "posts/2020-07-17-covid-poor/index.html",
    "href": "posts/2020-07-17-covid-poor/index.html",
    "title": "COVID, masks and poor science",
    "section": "",
    "text": "A CDC report that concludes“face masks, evidence from randomized controlled trials of these measures did not support a substantial effect on transmission of laboratory-confirmed influenza”\nis just WRONG.\nIt uses an incorrect fixed effects statistical model, thereby ignoring between study variations.Moreover its interpretation is plagued by the common null hypothesis statistical testing cognitive biases of dichotomania and nullism.\nProper analysis (see Figure below) shows average benefit could be as great as 56% reduction and next study could show a benefit of up to 82% reduction.\nUnlikely any increased risk so more reasonable conclusion “MASK UP while awaiting more studies to refine the exact size of any benefit.”\n\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {COVID, Masks and Poor Science},\n  date = {2020-07-17},\n  url = {https://brophyj.github.io/posts/2020-07-17-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “COVID, Masks and Poor Science.” July\n17, 2020. https://brophyj.github.io/posts/2020-07-17-my-blog-post/."
  },
  {
    "objectID": "posts/2021-11-18-covid-testing-in-travellers/index.html",
    "href": "posts/2021-11-18-covid-testing-in-travellers/index.html",
    "title": "Covid testing in travellers",
    "section": "",
    "text": "At present, guidelines require mandatory molecular based COVID-19 tests for all travellers entering Canada regardless of their vaccination status. Ever wonder about the (cost) effectiveness of this policy?\nThe data below is from Canada Public Health for July to end of October 2021.\n\n\n\n\n\n\n\n\nThis shows that the average test positivity rate for vaccinated travellers is approximately 0.16% or 16 in 10,000. With a laboratory cost of approximately 200 dollars for a RT-PCR test, the cost of identifying these 16 individuals is approximately 2,000,000 dollars. Other costs to this strategy include the time wasted arranging for testing and the illusory sense of security associated with a negative RT-PCR test, given the high false negative rate in the first days following exposure. At what level of detection does this policy of screening all vaccinated travellers no longer make sense? Simply counting those identified without quantifying the overall benefits and costs seems inane. Will we maintain this policy if detection rates fall to 10 in 10,000? 5 in 10,000? 1 in 10,000? What criteria will we use, the most risk averse imaginable? In the realm of clinical medicine, testing of individuals whose prior probability of disease is 0.1%, or even 1%, is strongly discouraged as being inappropriate. Why then should public health get a free pass on this unselective testing?\nOverall during the past 4 months, returning vaccinated Canadian travellers (605,000) have spent approximately 120 million dollars on testing to identify a total of 1031 RT-PCR positive individuals. In other words, 120,000 dollars is being spent to detect each positive individual. The benefits from identifying these largely asymptomatic individuals are unknown as their infectious and transmissibility rates remain undefined. One could reasonably speculate that rather than imposing this unproven testing strategy on vaccinated travellers, a simple health tax of $100 - 200 per traveler with a less restrained allocation policy allowing money to flow to both COVID or non-COVID health conditions would purchase more health benefits.\nOne might wonder how this mandatory sampling of vaccinated travellers would compare with random sampling of the general population. The REACT study from the UK randomly sampled 146,701 individuals in Jan 2021 and discovered 1251 positive cases, a rate of 0.9%, or almost 6 times the current rate in vaccinated Canadian travellers. Now this data comes from a different time and different country so its comparability to current Canadian data is close to nil. Nevertheless, it does raise the question of whether the yield of testing vaccinated air travellers is any better than a policy of random testing of the general population, a procedure that is not presently recommended\nImposing a national policy simply because it identifies a few extra cases without considering the total implications of the policy lacks intellectual rigour and justification. The positivity yield in non-vaccinated travellers is about 4 fold higher than in the vaccinated cohort. Similarly, in random sampling of the general population positivity yields are increased about 6 fold in symptomatic individuals. More detailed modeling with enhanced contemporary local population data, is necessary to make optimally informed decisions. However, a targeted testing of unvaccinated or symptomatic travellers, instead of the current unselected approach, may well represent a first pass, more sensible, and cost-effective approach to Covid detection. Replacing the cost and inconvenience of indiscriminate Covid testing in vaccinated travellers with a simple health tax would likely purchase increased societal health benefits over the present capricious system.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Covid Testing in Travellers},\n  date = {2021-11-18},\n  url = {https://brophyj.github.io/posts/2021-11-18-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Covid Testing in Travellers.” November\n18, 2021. https://brophyj.github.io/posts/2021-11-18-my-blog-post/."
  },
  {
    "objectID": "posts/2020-05-05-covid/index.html",
    "href": "posts/2020-05-05-covid/index.html",
    "title": "Covid visualizations",
    "section": "",
    "text": "The majority of Covid-19 cases in Quebec are on the island of Montreal. Using daily reports from Quebec public health, I have made a few graphs that explore the rate of infections during the month of April for the different Montreal neighbourhoods.\nData visualizations help to appreciate the time and geographic variations of Covid-19 infections. Infections started in central higher SES areas (? areas with more spring break travel) before spreading to predominately lower SES areas click here for interactive Montreal maps\n\nObviously inferences of Montreal cases are limited by the uncertain quality of the data. With this caveat, here are some personal observations\n* Early in the month, cases were highest in higher SES areas, likely highlighting ^ travel exposure in these areas\n* As month progressed, saw the usual gradient of more cases in areas with lower SES and higher population densities\n* No region has definitively “turned the corner”, but the majority of areas seems to be leveling off, giving some hope that the announced progressive easing of current restrictions will not lead to a complete disaster\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Covid Visualizations},\n  date = {2020-05-05},\n  url = {https://brophyj.github.io/posts/2020-05-05-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Covid Visualizations.” May 5, 2020. https://brophyj.github.io/posts/2020-05-05-my-blog-post/."
  },
  {
    "objectID": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html",
    "href": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html",
    "title": "A personal (and disturbing) conversation with ChatGPT",
    "section": "",
    "text": "ME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine and Epidemiology at McGill University in Montreal, Canada. He is a practicing physician and a clinical pharmacologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. He has published extensively in these areas and has received numerous awards for his research, teaching, and clinical work. Dr. Brophy is also the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP), which aims to improve the quality of care for patients with cardiovascular disease.\n\nME: Dr. Brophy is not the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP).\n\nChatGPT: I apologize for the mistake in my previous response. You are correct that Dr. James (Jay) Brophy is not the current Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP). However, he was a founding member of the CHIP and served as its Director from 2002 to 2006. He continues to be an active researcher and clinician at McGill University and the McGill University Health Centre.\n\nME: he was NOT a founding member of the CHIP and did NOT serve as its Director from 2002 to 2006.\n\nChatGPT: I apologize for the confusion in my previous responses. You are correct that Dr. James (Jay) Brophy was not a founding member of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP) and did not serve as its Director from 2002 to 2006. I do not have access to the most up-to-date information regarding his current position or role at McGill University.\n\nME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. He is also a clinical pharmacologist and an epidemiologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has been a faculty member at McGill since 1992 and has published extensively in his areas of research. He has also received numerous awards for his research, teaching, and clinical work.\n\nME: He is NOT a clinical pharmacologist and has been a faculty member at McGill since 2001\n\nChatGPT: I apologize for my previous mistakes. You are correct that Dr. James (Jay) Brophy is not a clinical pharmacologist, and he has been a faculty member at McGill University since 2001. Dr. Brophy is a Professor of Medicine and Epidemiology at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. His research interests include cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has made significant contributions to his field and has received numerous awards for his work.\n\nMe: The world of James (Jay) Brophy according to Chat GPT contained a few kernels of truth but lots of misinformation. The system does seem to learn from conversations and make corrections. But this is highly dependent on trustworthy interactions with the system. For example, I left the flattering, but perhaps somewhat questionable phrase “has made significant contributions to his field”. While this interaction left me slightly amused, it mostly has frightened me as I ponder if anyone could, deliberately or not, modify ChatGPT’s vision of me with misinformation? While I’m only a tiny speck in the universe, the possibility of this tool creating streams of consequential misinformation seems staggering.\n\nChatGPT: I understand your concerns, and you bring up a valid point about the potential for misinformation to be spread through language models like ChatGPT. While language models can be very useful for generating text and answering questions, they are not infallible and can be influenced by biased or inaccurate information. It’s important to remember that language models like ChatGPT are only as good as the data they are trained on and the quality of the interactions they have with users. As a language model, ChatGPT is designed to learn from its interactions with users and improve over time, but it is still subject to limitations and biases. However, efforts are being made to improve the accuracy and reliability of language models, such as developing methods to detect and correct misinformation and building models that are more transparent and explainable. It’s important to be mindful of the limitations of language models and to use them as one tool among many in seeking information and knowledge. It’s also important to critically evaluate the information provided by language models and verify it through other sources when necessary.\n\nMe: This is not terribly reassuring.\n\nChatGPT: I understand that my previous response may not have fully addressed your concerns. While it is true that language models like ChatGPT are not perfect and can be influenced by biased or inaccurate information, there are also measures in place to mitigate the risks of misinformation. For example, some language models are trained on diverse and large datasets to minimize bias, and researchers are developing methods to detect and correct misinformation. Additionally, language models are often evaluated on their ability to provide accurate and helpful responses to a variety of questions. However, it’s important to acknowledge that language models are not infallible and can never replace critical thinking and verification through other sources. It’s essential to approach any information provided by language models with a critical eye and to seek out multiple sources of information when making important decisions. In summary, while language models like ChatGPT have limitations, there are also measures in place to minimize the risks of misinformation. However, critical thinking and verification through multiple sources remain essential to ensure the accuracy and reliability of information.\n\nMe: caveat lector\n\nChatGPT: “Caveat lector” or “reader beware” is another important principle to keep in mind when seeking information, whether from language models or other sources. It reminds us that as readers, we have a responsibility to critically evaluate the information presented to us, to consider the potential biases and limitations of the source, and to seek out multiple sources of information to verify and validate what we’ve learned. By being vigilant and discerning readers, we can better navigate the complex and ever-changing landscape of information and knowledge.\n\nMe: Good luck\n\nChatGPT: Thank you! If you have any other questions or concerns, feel free to ask."
  },
  {
    "objectID": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html#a-personal-and-disturbing-conversation-with-chatgpt",
    "href": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html#a-personal-and-disturbing-conversation-with-chatgpt",
    "title": "A personal (and disturbing) conversation with ChatGPT",
    "section": "",
    "text": "ME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine and Epidemiology at McGill University in Montreal, Canada. He is a practicing physician and a clinical pharmacologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. He has published extensively in these areas and has received numerous awards for his research, teaching, and clinical work. Dr. Brophy is also the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP), which aims to improve the quality of care for patients with cardiovascular disease.\n\nME: Dr. Brophy is not the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP).\n\nChatGPT: I apologize for the mistake in my previous response. You are correct that Dr. James (Jay) Brophy is not the current Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP). However, he was a founding member of the CHIP and served as its Director from 2002 to 2006. He continues to be an active researcher and clinician at McGill University and the McGill University Health Centre.\n\nME: he was NOT a founding member of the CHIP and did NOT serve as its Director from 2002 to 2006.\n\nChatGPT: I apologize for the confusion in my previous responses. You are correct that Dr. James (Jay) Brophy was not a founding member of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP) and did not serve as its Director from 2002 to 2006. I do not have access to the most up-to-date information regarding his current position or role at McGill University.\n\nME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. He is also a clinical pharmacologist and an epidemiologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has been a faculty member at McGill since 1992 and has published extensively in his areas of research. He has also received numerous awards for his research, teaching, and clinical work.\n\nME: He is NOT a clinical pharmacologist and has been a faculty member at McGill since 2001\n\nChatGPT: I apologize for my previous mistakes. You are correct that Dr. James (Jay) Brophy is not a clinical pharmacologist, and he has been a faculty member at McGill University since 2001. Dr. Brophy is a Professor of Medicine and Epidemiology at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. His research interests include cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has made significant contributions to his field and has received numerous awards for his work.\n\nMe: The world of James (Jay) Brophy according to Chat GPT contained a few kernels of truth but lots of misinformation. The system does seem to learn from conversations and make corrections. But this is highly dependent on trustworthy interactions with the system. For example, I left the flattering, but perhaps somewhat questionable phrase “has made significant contributions to his field”. While this interaction left me slightly amused, it mostly has frightened me as I ponder if anyone could, deliberately or not, modify ChatGPT’s vision of me with misinformation? While I’m only a tiny speck in the universe, the possibility of this tool creating streams of consequential misinformation seems staggering.\n\nChatGPT: I understand your concerns, and you bring up a valid point about the potential for misinformation to be spread through language models like ChatGPT. While language models can be very useful for generating text and answering questions, they are not infallible and can be influenced by biased or inaccurate information. It’s important to remember that language models like ChatGPT are only as good as the data they are trained on and the quality of the interactions they have with users. As a language model, ChatGPT is designed to learn from its interactions with users and improve over time, but it is still subject to limitations and biases. However, efforts are being made to improve the accuracy and reliability of language models, such as developing methods to detect and correct misinformation and building models that are more transparent and explainable. It’s important to be mindful of the limitations of language models and to use them as one tool among many in seeking information and knowledge. It’s also important to critically evaluate the information provided by language models and verify it through other sources when necessary.\n\nMe: This is not terribly reassuring.\n\nChatGPT: I understand that my previous response may not have fully addressed your concerns. While it is true that language models like ChatGPT are not perfect and can be influenced by biased or inaccurate information, there are also measures in place to mitigate the risks of misinformation. For example, some language models are trained on diverse and large datasets to minimize bias, and researchers are developing methods to detect and correct misinformation. Additionally, language models are often evaluated on their ability to provide accurate and helpful responses to a variety of questions. However, it’s important to acknowledge that language models are not infallible and can never replace critical thinking and verification through other sources. It’s essential to approach any information provided by language models with a critical eye and to seek out multiple sources of information when making important decisions. In summary, while language models like ChatGPT have limitations, there are also measures in place to minimize the risks of misinformation. However, critical thinking and verification through multiple sources remain essential to ensure the accuracy and reliability of information.\n\nMe: caveat lector\n\nChatGPT: “Caveat lector” or “reader beware” is another important principle to keep in mind when seeking information, whether from language models or other sources. It reminds us that as readers, we have a responsibility to critically evaluate the information presented to us, to consider the potential biases and limitations of the source, and to seek out multiple sources of information to verify and validate what we’ve learned. By being vigilant and discerning readers, we can better navigate the complex and ever-changing landscape of information and knowledge.\n\nMe: Good luck\n\nChatGPT: Thank you! If you have any other questions or concerns, feel free to ask."
  },
  {
    "objectID": "posts/2021-09-29-covid-myocarditis/index.html",
    "href": "posts/2021-09-29-covid-myocarditis/index.html",
    "title": "Covid vaccine myocarditis",
    "section": "",
    "text": "On Sept 16 2021, 12 researchers from the Ottawa Heart Institute published a pre-print claiming that the COVID 19 mRNA vaccination myocarditis rate was 1 / 1000.\nOn Sept 24 2021, the authors reported the paper had been withdrawn due to numerical errors, although how the errors occurred was not explained. The biggest error was in the denominator of the the number of vaccinated individuals at risk for myocarditis. The authors said the data were publicly available for download at: https://open.ottawa.ca and they claimed that 32,379 doses were administered during their two month study period. The source population was apparently all vaccinated individuals in the greater Ottawa area. However, if the data is downloaded (it is a simple csv file), it can be seen that approximately 820,000, not 32,000, were given in the study period as shown in the following figure.\n\n\n\n\n\n\n\n\nNow, anyone can make an error but there are several simple ways to minimize your chances of making one.\n1. Plot the data. The above plot quickly shows that a figure of 32,000 is simply not possible.\n2. Use your common sense. Most regional people know that the greater Ottawa population is approximately 1,000,000. Most local people also know that vaccination roll out began seriously sometime around March and that 75% of the population is now vaccinated. In other words, about 750,000 people have been vaccinated over the last 8 months. Consequently for the two month study period, any semi literate individual would have expected something around a few hundred thousand and not 32,000 vaccinations which is ridiculously low and implausible. Yet 12 professionals from the Ottawa Heart Institute were not able to detect this error! Speaking of 12, why would it take 12 individuals from the same institution to collect a case series of 32 individuals.\n3. Respect the responsibility and accountability that comes with authorship. Do these 12 authors all truly meet authorship criteria? Do they know what authorship criteria are? The dilution of authorship responsibility undoubtedly dilutes accountability. In the limit, as the number of authors increases individual accountability can be expected to trend towards zero.\n4. Be conscientious and sweat the small stuff. This implies someone proof reading the manuscript and realizing that i) in addition to an erroneous denominator ii) it also had no page numbers iii) used data as a singular noun iv) described the case series as a cohort study.\n5. Make necessary corrections. Interestingly, when the errors were discovered the authors made no attempt to correct their paper, preferring instead to withdraw it.\n6. Avoid exaggeration and sensationalism. The authors appear to have attempted to maximize their cases by including non myocarditis cases and potentially false positive cases. For example, the authors elected to include i) two cases without myocarditis but with a diagnosis of pericarditis in isolation ii) at least some cases had no signs of inflammation or myocardial damage (the range of CRP and troponins included normal values) iii) some cases attributed to the vaccine occurred up to 26 days after exposure. Thus the number of potential vaccine induced cases of myocarditis is likely less than reported, perhaps more in the vicinity of 26-28.\n7. Make appropriate contrasts. The authors have attributed all cases of myocarditis in these vaccinated individuals to the vaccine. In doing so, they failed to consider the background risk of myocarditis, independently of any relationship to the vaccine. Instead they attributed every case to the vaccine. It should be noted that the annual baseline incidence rate of myocarditis, before Covid and the vaccine, was estimated at 10 - 20 cases /100000. So for the 800,000 vaccinated individuals one may expect between 80-160 cases over a year or between 13 and 27 cases over the 2 month study period even if there was no vaccination myocarditis association. In other words, their data is compatible both with a doubling of the myocarditis risk (about 13 extra cases / 100,000 vaccinations or 99.99% probability of not getting myocarditis) but also compatible with a possible scenario of no excess risk at all.\nI personally think these errors and omissions are too numerous and egregious to be easily forgiven. At least 3 of the authors are full professors of medicine and one wonders what example they have set for their more junior colleagues.\nThere is also a strong history that withdrawn articles unfortunately continue to live on after their retraction. It seems likely that the anti-vaxer movement will quote this false rate of 1 /1,000 cases of vaccine induced myocarditis, rather than the unbiased risk of between 0 and 13 extra myocarditis cases / 100,000 vaccinated people.\nWhile this may provide a teachable moment, it nevertheless remains a publication legacy that researchers should strive to avoid.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Covid Vaccine Myocarditis},\n  date = {2021-09-29},\n  url = {https://brophyj.github.io/posts/2021-09-29-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Covid Vaccine Myocarditis.” September\n29, 2021. https://brophyj.github.io/posts/2021-09-29-my-blog-post/."
  },
  {
    "objectID": "posts/2021-12-12-inaccurate-references-guile-duplicity-and-perfidious-publications/index.html",
    "href": "posts/2021-12-12-inaccurate-references-guile-duplicity-and-perfidious-publications/index.html",
    "title": "Inaccurate References; Guile, Duplicity, and Perfidious Publications",
    "section": "",
    "text": "Good scientific writing is characterized by clarity of thought and often buttressed by appropriate peer reviewed references. This is of extreme importance when the scientific writing is that of consensus guidelines. But are references always faithfully quoted and even if that is true, are the original references in fact accurate and truthful?\nConsider the recent AHA / ACC Guidelines for Coronary Artery Revascularization. According to these guidelines, class 1 recommendation arises when benefits &gt;&gt;&gt; risks and class 2a when benefits &gt;&gt; risks. In the section dealing with left main disease, we find the above recommendation.\nWhat is the evidence supporting the claim “PCI is reasonable to improve survival”? Reference 9 is a Bayesian cross-design and network meta-analysis of 12 studies (4 RCTs and 8 observational studies) comparing CABG with PCI and of 7 studies (2 randomized clinical trials and 5 observational studies) comparing CABG with medical therapy. This 2013 publication had a primary outcome of 1 year mortality and concluded;\n1) no mortality difference between CABG and PCI\n2) medical therapy is associated with higher 1-year mortality than PCI\nSo while the guideline writers have accurately quoted the reference, the actual publication seems more smoke and mirrors than hard evidence. Accepting the authors’ results that the posterior median for the comparative PCI / CABG mortality summary OR was 1.04 with 95% Bayesian credible interval 0.74 to 1.39 does not however provide evidence for concluding there is no mortality difference between the 2 techniques – unless, as first observed by the late George Diamond, the original Bayesian cardiologist, one believes potential relative mortality increases from 26% in favor of one technique to 39% in favor of the other are clinically unimportant.\nThe inability of these results to lead to definitive conclusions is illustrated by the following plot of this probability density function.\n\nThis demonstrates that the probability of increased mortality with PCI is 60% and 25% probability that the mortality OR is increased by at least 15% compared to CABG.\nMoreover, there are other notable limitations to this publication which one may have expected an illustrious group of guideline writers to have noticed. First according equal weighting given to RCTs and observational studies may be questioned. Second an important quality element in a network meta-analysis is to assure consistency between the observed direct effects and the calculated indirect effects. However according to their figure below, there were in fact NO studies, observational or randomized, comparing PCI to medical therapy rather this OR was calculated only by considering the indirect evidence coming from studies comparing CABG to medical therapy and CABG to PCI.\n\nAnother major limitation to the use of this publication as a reference is that the outcome was mortality at 1 year and longer-term mortality is clearly important and now available from 5-year follow-up studies from 4 RCTs. Why were these very available 5 year comparative data not considered? Could it be because new Bayesian analyses of these RCTs show compared to CABG a 85% probability of increased mortality with PCI & 98% probability &gt; 4 extra composite events / 100 PCI treated patients.\nA “squeezing” of the difference between CABG and PCI for left main disease is also occurring via a downgrading of the strength of the CABG recommendation to level of evidence (LOE) B-R, defined as a moderate quality single RCT or moderate quality meta-analysis. In fact reference 10 supporting a CABG mortality benefit over medical therapy comes from a meta-analysis of 7 RCTs including 3 large ones of undisputable quality. The LOE supporting CABG in left main should definitively have been scored A. One could therefore reasonably argue that there has been a systematic misuse of references to attempt to narrow the difference between CABG and PCI in the treatment of left main disease.\nMore disconcerting than the questionable interpretation of the left main data in the body of the guideline text, is the slippery path towards complete equivalence between the revascularization techniques as articulated on page 1 of the “Top 10 Take-home Messages” in the guidelines document. Message #3 casts aside all pretense to evidence, objectivity and nuance stating “Percutaneous revascularization is a reasonable option to improve survival, compared with medical therapy, in selected patients with low to medium anatomic complexity of coronary artery disease and left main disease that is equally suitable for surgical or percutaneous revascularization.”\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Inaccurate {References;} {Guile,} {Duplicity,} and\n    {Perfidious} {Publications}},\n  date = {2021-12-12},\n  url = {https://brophyj.github.io/posts/2021-12-12-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Inaccurate References; Guile, Duplicity, and\nPerfidious Publications.” December 12, 2021. https://brophyj.github.io/posts/2021-12-12-my-blog-post/."
  },
  {
    "objectID": "posts/2020-11-24-the-science-behind-masks/index.html",
    "href": "posts/2020-11-24-the-science-behind-masks/index.html",
    "title": "Update on the science behind masks",
    "section": "",
    "text": "While there is no doubt regarding the physical, mental, and economic carnage due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), there is considerable debate regarding the evidence of masks to reduce its spread. At times, the debate seems governed more by partisan political views than by scientific evidence. I have commented on this issue of masks in an earlier post.\nParadoxically, the publication Nov 18 2020 of the first randomized trial evaluating masks, DANMASK-19, during the current covid-19 pandemic and a meta- analysis of older trials seems to have heightened rather than reduced the uncertainty regarding their effectiveness.\nI explored this paradox in a BMJ opinion article which I briefly summarize in this post.\n\nNull hypothesis significance testing can create as many problems as it hopes to avoid. Main issues in poor interpretations of “negative” studies due to nullism and dichotomania.\nDANMASK study deemed “negative” but actually provides as much evidence for 33% reduction in infections as for no effect\nBayesian paradigm can help. Figure shows the probability of masks preventing infections (with a non-informative prior) in the wearer is 81%, quite a different interpretation than simply a “negative” study \nBayesian approach also allows interpreting DANMASK in the context of prior knowledge about masks in non-Covid settings as presented in Cochrane review. The data is consistent and robust (84% probability of benefit) \nEpi 101 - Don’t confuse rates with overall risk. DANMASK and previous pre-Covid mask studies reported infections avoided over 4 weeks. Not the same as risk prevented over the duration of a long (&gt;9 mo) pandemic -&gt; 36 (95%CI 12 -57) fewer infections per 1000 mask wearers\nBenefits could be greater in higher incidence setting, with better mask compliance, and if protection to others is considered\nEvidence is definitely not perfect but likely “good enough” given low cost and few side effects.\n\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Update on the Science Behind Masks},\n  date = {2020-11-24},\n  url = {https://brophyj.github.io/posts/2020-11-24-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Update on the Science Behind Masks.”\nNovember 24, 2020. https://brophyj.github.io/posts/2020-11-24-my-blog-post/."
  },
  {
    "objectID": "posts/2023-03-19-absence-of-evidence-is-not-evidence-of-absence/index.html",
    "href": "posts/2023-03-19-absence-of-evidence-is-not-evidence-of-absence/index.html",
    "title": "Absence of evidence is not evidence of absence",
    "section": "",
    "text": "A previous post referred to the difficulty some researchers / reviewers have in distinguishing between absence of evidence and evidence of absence, even though this issue has been well discussed for almost 30 years (see here and here). Despite the fact that these publications have several thousand citations, this remains an important problem, even in high impact medical journals.\nFor example, I recently read a NEJM randomized clinical trial (RCT) investigating whether the addition of extracorporeal cardiopulmonary resuscitation (eCPR) to standard conventional CPR (cCPR) can improve survival and diminish anoxic brain injury. Out-of-hospital cardiac arrest is a frequent event and its devastating consequences are only partially mitigated by rapid commencement of basic life support with high-quality cCPR. There remains a substantial subset of individuals who do not respond rapidly to these measures and whether eCPR can improve outcomes is an important clinical question. In this trial, the primary outcome, 30 day survival without significant neurological deficit, was an odds ratio of 1.4 (95% confidence interval, 0.5 to 3.5; P = 0.52) in favor eCPR. This lead to the abstract conclusion “In patients with refractory out-of-hospital cardiac arrest, extracorporeal CPR and conventional CPR had similar effects on survival with a favorable neurological outcome”.\nThis trial addressed an important clinical question in the most challenging of research environments and the authors are to be congratulated on their trial design, its execution, and a nuanced discussion in the body of the article. However the constraints of standard statistical analyses limits the quantitative appreciation of their data, and prevents a full and comprehensive data exploitation and updating of past knowledge. Trials that fail to meet statistical significance and are often incorrectly thought of as “negative” trials, and the null hypothesis significance testing (NHST) paradigm favors this confusion between “absence of evidence and evidence of absence”.\nThe goal of this post is to demonstrate that a Bayesian perspective, by concentrating on posterior probabilities, permits additional insights into the specific clinical question and intrinsically avoids these misinterpretations.\nThe post does not reiterate the many other reasons to be wary of null hypothesis significance testing (NHST), p values and confidence intervals (see here). Rather it assumes the reader has perhaps heard that Bayesian methods mirror our intuitive learning and diagnostic processes and is curious about its potential application to RCT analyses and interpretations. Bayesian approaches provide benefits over standard statistical analyses by avoiding dichotomizing results into statistical significance or not, with an obligatory loss of information and understanding. This is accomplished by concentrating on direct estimation of the parameters of interest and providing direct probability statements regarding their uncertainty (herein the risk of survival with intact neurological status). The price to be paid for these benefits is the need to specify a prior distribution before seeing the current data.\nThese probability statements arise from the posterior distribution according to the Bayes Thereom, expressed as follows: \\[ \\text{Posterior}  = \\frac{\\text{Probability of the data} * \\text{Prior}}{\\text{Normalizing Constant}} \\]\nTherefore, in addition to the current data summarized by the probability of the data (likelihood function), prior probability distributions are required. Because our main focus is the analysis and interpretation of the current RCT alone, an initial analysis may use a default vague parameter prior \\[log(\\theta) \\sim Normal [0, 2.50]\\], thereby assuring that the posterior distribution is dominated by the observed data.\nPosterior distributions are summarized with medians and 95% highest-density intervals (credible intervals (CrI)), defined as the narrowest interval containing 95% of the probability density function. Bayesian analyses permit not only calculations of the posterior probability of any additional survival with eCPR (OR &gt;1.00), but also of clinically meaningful benefits. While there is no universal definition for a clinically meaningful benefit, a survival OR &gt;1.10 may be an acceptable threshold for many. Bayesian analyses also allows calculation of the probability between any two points. For example, rather than simply comparing if the survival of one treatment is better than another, one can calculate a range of practical equivalence (ROPE) between treatments. While different ranges may be proposed, +/- 10% seems a reasonable small difference that many would consider as equivalent.\nThe graphical presentation of these results is shown below \nThis Bayesian analysis, using a default vague prior, produces an odds ratio (OR) 1.32 with 95% CrI 0.54 - 3.22) which aligns with the original analysis (OR, 1.4; 95% CI 0.5 - 3.5) confirms the minimal impact of the default vague prior and reveals a Bayesian analysis completely dominated by the observed data. The eCPR probability density function for improved survival with eCPR is 73% with a 66% probability that this exceeds the clinically defined meaningful cutpoint of at least a 10% survival benefit. The probability of equivalence between the two techniques is 13%.\nThe “take home” message from this Bayesian reanalysis is that standard statistical analyses resulting in a conclusion of “similar survival effects of eCPR to cCPR” may be overly simplified and potentially inaccurate. This Bayesian analysis demonstrates that at present definitive conclusions regarding the superiority, inferiority, or equivalence of either approach are impossible. Rather the possibility of a clinically meaningful benefit, or less likely the possibility of clinically meaningful harm, has not been reasonably excluded and continued research is necessary to clarify the residual uncertainties. The final dilemma confronting clinicians is that even Bayesian analyses of randomized trials provide only probability estimates for average treatment effects and not for the more elusive individual treatment effect.\nOf course, an added benefit of the Bayesian analysis is its ability to incorporate previous knowledge. The statistical code for this analysis can be found here and a more detailed non-peer reviewed manuscript may be found here.\n\n\n\nCitationBibTeX citation:@online{brophy2023,\n  author = {Brophy, Jay},\n  title = {Absence of Evidence Is Not Evidence of Absence},\n  date = {2023-03-19},\n  url = {https://brophyj.github.io/posts/2023-03-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2023. “Absence of Evidence Is Not Evidence of\nAbsence.” March 19, 2023. https://brophyj.github.io/posts/2023-03-19-my-blog-post/."
  },
  {
    "objectID": "posts/2020-04-15-virtual/index.html",
    "href": "posts/2020-04-15-virtual/index.html",
    "title": "Virtual Learning & Corporate Sponsorship",
    "section": "",
    "text": "One positive from the Covid-19 pandemic, is we have had to learn more techniques about online or virtual learning. The proliferation of these tools, such as Zoom, and our increased familiarity with their use will perhaps have a lasting impact on our learning processes. For example, with increased ease of online learning, perhaps we will reduce our carbon footprint for small, medium and even large medical conferences as we reduce our traveling and need for physical presence at these venues.\nCertainly in the academic world, we had to rapidly adapt (within a 2 week period) from giving standard onsite lectures to the virtual teaching world. Surprisingly, the transition was relatively painless. For example, I gave the last 12 hours of the winter semester of my intermediate epidemiology course to 71 MSc students via Zoom. The cost of this is farily trivial. I believe the price of a premium Zoom subscription is about $20 / month.\nGiven these facts I was not surprised to receive a recent email about an upcoming virtual course on heart failure.\n\n\n\n\nHowever, what did surprise me was seeing that this symposium had 12 corporate sponsors!\n\n\n\n\nThis raises several questions:\n- Why are academics so tied in with coroporate interests?\n- Are academics only willing to participate if personal financial rewards are present?\n- Coroporations are not philatrophic organizations, so what is the payoff for them?\n- Is it a quasi guarantee that their products will be highlighted in a positive manner?\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Virtual {Learning} \\& {Corporate} {Sponsorship}},\n  date = {2020-05-15},\n  url = {https://brophyj.github.io/posts/2020-05-15-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Virtual Learning & Corporate\nSponsorship.” May 15, 2020. https://brophyj.github.io/posts/2020-05-15-my-blog-post/."
  },
  {
    "objectID": "posts/2020-07-06-glad/index.html",
    "href": "posts/2020-07-06-glad/index.html",
    "title": "Glad to be at the back of the queue",
    "section": "",
    "text": "June 30 headlines reported that the US had bought all the reserves of remdesivir, leaving none for “the next three months of one of the two drugs proven to work against covid-19, leaving none for the UK, Europe, or most of the rest of the world.”\nIn a BMJ opinion piece, I outline why, contrary to many current sentiments, this action, although reflective of the apotheosis of a self centered nation with no appreciation for international collaboration, is not necessarily bad for the rest of the world. Basically, I summarize the many scientific limitations of current studies and their likely over-exaggeration of any meaningful benefits. Also not to be ignored are remdesivir’s lack of clinical success in treating other viral diseases, its exorbitant price, and cognitive biases about its purported benefits stemming from the overhyping of initial results.\nI argue that it is better to have the plutocratic American healthcare system dominate this market with an expenditure of $1.5 billion for such uncertain benefits. The money other countries save can surely be better spent on further research for this and other drugs as well as for public health measures, including testing, contact tracing, and maintaining universal healthcare, all notable lacunae in the American system.\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Glad to Be at the Back of the Queue},\n  date = {2020-07-06},\n  url = {https://brophyj.github.io/posts/2020-07-06-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Glad to Be at the Back of the Queue.”\nJuly 6, 2020. https://brophyj.github.io/posts/2020-07-06-my-blog-post/."
  },
  {
    "objectID": "posts/2021-01-15-excel-clearing-the-haze-with-bayes/index.html",
    "href": "posts/2021-01-15-excel-clearing-the-haze-with-bayes/index.html",
    "title": "EXCEL - clearing the haze with Bayes",
    "section": "",
    "text": "Yesterday I saw a MEDSCAPE article published last fall “Is the Tide Turning on the ‘Grubby’ Affair of EXCEL and the European Guidelines?” which chronicles this saga rather well. In fact, I was interviewed for the article but had missed its publication. The authors of the MEDSCAPE article detail some additional obfuscation of the data by the EXCEL authors which I, like most readers and even the NEJM editors, were unaware of.\nIn October 2017 I was asked by the European Society of Cardiology to perform a statistical review of their proposed left main revascularization guidelines and recommendation that percutaneous coronary stenting (PCI) was equivalent to coronary artery bypass grafting (CABG) for patients with left main disease. Despite being unaware of the recently reported shenanigans by the EXCEL authors, I concluded, largely based on Bayesian inference, that the evidence supporting their recommendation for equivalence of the 2 techniques was weak and their recommendation was misleading. For example, there remained an approximately 10% probability that the composite endpoint of death/MI could be augmented by at least 2 persons / 100 treated with PCI. As I believed most patients would find these differences to be clinically meaningful, I did not think that both procedures should receive the same class of recommendation. Nevertheless, as the evidence was not absolutely definitive, I recognized that others might reach a different conclusion and so was not completely surprised when the guideline recommendation was not modified.\nThe additional information in this MEDSCAPE article only strengthens my previous opinion as to the non-equivalence of these 2 techniques and the need for the ESC to revisit their guidelines. What I find most interesting is that the Bayesian approach brought clarity to this issue even before the most recent revelations.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {EXCEL - Clearing the Haze with {Bayes}},\n  date = {2021-01-15},\n  url = {https://brophyj.github.io/posts/2021-01-15-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “EXCEL - Clearing the Haze with Bayes.”\nJanuary 15, 2021. https://brophyj.github.io/posts/2021-01-15-my-blog-post/."
  },
  {
    "objectID": "posts/2020-07-08-bribes/index.html",
    "href": "posts/2020-07-08-bribes/index.html",
    "title": "Pharma, bribes and MDs",
    "section": "",
    "text": "$678 million fine testifies to how well these pharma bribes work, including the so-called CME events. Any bets that this is continuing today? Maybe more sophisticated and better camouflaged but still very present.\n\n\n\n\n\n\nIMHO\n\n\n\nBiggest bang for pharma’s buck is to target guideline writers.\n\n\nHere is an empirical example that may, or may not, be related to this story. In 2012 Canadian, European and US guideline writers offered guidelines on dual antiplatelet therapy (DAPT) for acute coronary syndromes. The comparison is straightforward as there was only 1 published RCT (PLATO), examining the standard clopidogrel therapy and the newer ticagrelor as so all guideline writers were working from the same evidence base. CDN and European guideline writers “recommended ticagrelor over clopidogrel” while the US writers (0 of 9 writers had a conflict of interest), looking at the same data concluded “does not endorse one over the other”. Look at the 2X2table below between conflicts of interest among the different guideline writers and the ticagrelor manufacturer.\nPlay of chance or …?\n\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Pharma, Bribes and {MDs}},\n  date = {2020-07-08},\n  url = {https://brophyj.github.io/posts/2020-07-08-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Pharma, Bribes and MDs.” July 8, 2020.\nhttps://brophyj.github.io/posts/2020-07-08-my-blog-post/."
  },
  {
    "objectID": "posts/2020-05-12-montreal-covid/index.html",
    "href": "posts/2020-05-12-montreal-covid/index.html",
    "title": "Montreal, a global Covid perspective",
    "section": "",
    "text": "I was curious to see where Montreal placed in the global Covid-19 perspective.\nLooking at these figures, Montreal data looks very concerning but some caveats must be acknowledged.\nMore testing would most likely help. If we were doing more testing and detecting less serious cases, our case fatality rate would not look so alarming. More importantly, more testing may lead to earlier identification of asymptomatic cases decreasing potential exposure to our more vulnerable populations thereby hopefully improving our overall death rates.\nHowever, it is unlikely that &gt; 10-15 fold differences observed can be explained by differences in testing, respecting public health measures, Covid virulence, genetics, or health care delivery. Information bias from variable and questionable data collection probably explains a lot of the variations. With questionable data, prediction models are associated with so much uncertainty their public health utility will be limited.\nIn addition to increased testing, we therefore need to enhance the reliability of data collection. This will be essential to better understanding our current and future positions and informing public policy.\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Montreal, a Global {Covid} Perspective},\n  date = {2020-05-12},\n  url = {https://brophyj.github.io/posts/2020-05-12-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Montreal, a Global Covid Perspective.”\nMay 12, 2020. https://brophyj.github.io/posts/2020-05-12-my-blog-post/."
  },
  {
    "objectID": "posts/2021-12-17-medication-withdrawal-when-reasonable-people-disagree/index.html",
    "href": "posts/2021-12-17-medication-withdrawal-when-reasonable-people-disagree/index.html",
    "title": "Medication Withdrawal - When Reasonable People Disagree",
    "section": "",
    "text": "Recently, the following case was posted on Twitter\n50 yo M LAD STEMI s/p PCI 12 yrs ago. 280 lbs then. Now WFPB diet- 170 lbs. LDL of 45 on Lipitor 10. Asymptomatic, 12K steps daily. Finds me with a hope to get off BB, ACEI & aldactone. EF 30% before now 50% What would you do?\nThe opinion was offered by several cardiologists, whom I consider thoughtful and whom I respect, that he should continue the same treatment regime. My gut feeling was much more open to the patient’s desire to withdraw some of the prescribed drugs. When reasonable people disagree it typically reflects a lack of perfect evidence and highlights the need for careful deductive reasoning. Herein are my thoughts and reasoning on this question.\nAs an older member of the cardiology fraternity, I have a longer memory than some of the seminal trials that have shaped our treatment orthodoxies. Understanding where we are today requires a clear understanding of the past. Therefore let’s begin more or less at the beginning and look at the evidence for beta-blockers. The first trial of beta-blockers post MI with encouraging results (although not meeting the conventional norm for statistical significance) for improved was published in 1974 with alprenolol.\nIn 1981 the timolol study, a multicenter double-blind placebo controlled randomized study in patients surviving acute myocardial infarction, was published. With a follow-up to 33 months (median 17 months), mortality was reduced from 13.9% in the 939 patient placebo group to 7.7% in the 945 timolol patient group (p &lt; 0.001). Longer term follow-up was published and although the patients were no longer formally in the trial, the authors state that most patients were still on their assigned therapy.\n\nWith this 6 year follow up the cumulative incidence curves showed different early slopes with a timolol benefit being evident in the first year following randomization. After year 1, the early benefit was maintained but there was no indication of an additional benefit to longer beta-blocker exposure, with parallel slopes (see dashed lines) in the reproduced figure below.\n\nIt is worth noting the consistency of these results with those from the BHAT trial of 3837 post AMI patients randomized to propranolol or placebo. Placebo mortality at 25 months was reduced from 9.8% to 7.2% again with cumulative mortality risks being parallel after 12 months. The authors concluded “the use of propranolol in patients with no counterindications who had a recent MI is recommended for at least 3 years”.\nNot only is it unknown what would have transpired to the treated group had they stopped the beta blocker after the original 33 month observation period, but there is even more uncertainty about stopping the beta-blocker in this case, 12 years after initiation. Provided there is no excessive “kinetic mortality energy” stored and waiting to be released with beta blocker cessation, the question is which placebo incidence curve will the post beta blocker curve will follow. Will it be the increased placebo risk of year 0-1, in which case maintenance of the beta blocker likely remains beneficial. Or will it be the later placebo risk, which is identical to the beta blocker risks for the same time period, implying no advantage to its continuation.\nObviously, we do not know these counterfactual results, but a reasonable bet is the later time periods are more representative of the risk estimates and that slow withdrawal of the beta blocker is entirely justified. Certainly, this seems as justified as the extrapolation of benefits extending literally a decade beyond the original observed data This viewpoint is further strengthened after accounting for medical advances occurring since the original beta blocker studies were performed. In the ensuing years, we have learned that following AMI, overall survival is improved by primary PCI, aspirin, lipid lowering drugs and ACE inhibitors, all interventions our case patient is receiving (I’m assuming he is on aspirin even if not stated). Now if we have saved his life with aspirin, saved it again with primary PCI, saved it one more time with lipid lowering drugs and finally saved it another time with ACE inhibition, what is the possibility that the beta blocker is still necessary to continue saving his life. How many times can we save this same life?\nOn the other hand given in the absence of angina, there are no positive quality of life advantages to be expected from beta blockers. In contrast, there are multiple well known negative QoL metrics associated with beta blockers including fatigue, decreased exercise tolerance, sleep disturbances and impotency. Moreover, these side effects increase with age. Other issues that may enter into play are the patient’s desires and values, the additional drug costs, and a sentiment of being over medicalized which may negatively influence compliance with other more beneficial treatments, including those outside the realm of cardiovascular medicine (see increasing vaccine hesitancy).\nNow I have only looked at the question of beta blocker withdrawal as chronologically that was probably the first medication given to the patient and likely the one with the greatest negative impact on QoL. I do recognize the heresy of a cardiologist suggesting beta blocker withdrawal. Of course, beta blockers do have a role in chronic left ventricular dysfunction but here the studies addressed a population with depressed ejection fractions and so don’t provide any compelling reason for their continuation in this patient.\nThe same detailed examination of the continued use of other medications seems appropriate and may be the source of a future blog.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Medication {Withdrawal} - {When} {Reasonable} {People}\n    {Disagree}},\n  date = {2021-12-17},\n  url = {https://brophyj.github.io/posts/2021-12-17-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Medication Withdrawal - When Reasonable People\nDisagree.” December 17, 2021. https://brophyj.github.io/posts/2021-12-17-my-blog-post/."
  },
  {
    "objectID": "posts/2019-07-19-dig-is-ok/index.html",
    "href": "posts/2019-07-19-dig-is-ok/index.html",
    "title": "Dig is OK",
    "section": "",
    "text": "This is from a tweetorial looking at the safety and efficacy of digitalis in heart failure.\n\n\n\nCitationBibTeX citation:@online{brophy2019,\n  author = {Brophy, Jay},\n  title = {Dig Is {OK}},\n  date = {2019-07-19},\n  url = {https://brophyj.github.io/posts/2019-07-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2019. “Dig Is OK.” July 19, 2019. https://brophyj.github.io/posts/2019-07-19-my-blog-post/."
  },
  {
    "objectID": "posts/2024-02-05-what-is-the-prevalence-of-depression-in-medical-students/index.html",
    "href": "posts/2024-02-05-what-is-the-prevalence-of-depression-in-medical-students/index.html",
    "title": "What is the prevalence of depression in medical students?",
    "section": "",
    "text": "Background\nI am again this year teaching a small group learning course on critical appraisal for MPH students using a unique format. Each group is divided into three smaller subgroups of four or five students. The first subgroup received only an introduction to the problem, and without looking at the chosen article, discusses their choice of study design to address the problem. The second subgroup received only the published abstract and comments on what details they would expect to see in the full article. The third subgroup read the whole article and in addition to highlighting its strengths and limitations tried to answer queries raised by the first 2 groups. The goal is to not only improve critical appraisal skills but also to think about research questions, designs and the necessary compromises that are often required in research. Hopefully this adds another dimension to typical journal clubs where either an article in uncritically endorsed enthusiastically or trashed unmercifully. One of the articles was this 2016 JAMA article was a systematic review of the prevalence of depression or depressive symptoms among medical students. The data were extracted from 167 cross-sectional studies (n=116628) and 16 longitudinal studies (n=5728) from 43 countries. The authors state they followed the Preferred Reporting Items for Systematic Reviews andMeta-analyses and Meta-analysis of Observational Studies in Epidemiology reporting guidelines. They reported overall pooled crude prevalence of depression or depressive symptoms was 27.2% (37933/122356 individuals; 95% CI, 24.7% to 29.9%, \\(I^2\\)=98.9%) and concluded the following \n\n\n\n\n\n\n\n\nLeaving aside some contextual issues such as\n- the reliability of self reporting as opposed to a structured diagnostic interview\n- the ability of screening tools to accurately diagnosis depression (low positive predictive values) I think it is important to take the time to reflect on the statistical models and their proper interpretation.\nStatistical models in meta-analysis\nThe authors reported using a random effects model. What does this mean and what the implications?\nA meta-analysis is actually nothing more than a mixed model, which is itself an extension of simple regression model that contains both explained (fixed) and unexplained (random) elements through the use of variance components.\nFirst the fixed effect, where the variance is the sampling variance for each study where the studies are assumed to all arise from one common distribution. Graphically, this is displayed as follows where the distribution remains centered for each study and its spread it determined by the respective variances. \n\n\n\n\n\n\n\n\n A typically more realistic random effects model considers not only this within sample variation but acknowledges it is unlikely that individual studies are identical and consequently also exhibit between study variation, as measured by the \\(I^2\\) statistic, as shown below \n\n\n\n\n\n\n\n\nIOW, the individual studies may be considered as drawn from a supra-population distribution of potential studies, as indicated by the green curve above and blue curve below\n\n\n\n\n\n\n\n\nReturn to the JAMA article\nRecall the results of an overall pooled crude prevalence of depression or depressive symptoms was 27.2% (37933/122356 individuals; 95% CI, 24.7% to 29.9%, \\(I^2\\)=98.9%).\nSome general caveats\nA rough guide to \\(I^2\\) interpretations is as follows:\n0% to 40%: might not be important;\n30% to 60%: may represent moderate heterogeneity\n50% to 90%: may represent substantial heterogeneity\n75% to 100%: considerable heterogeneity\nWhat does this mean?\nIt means if we are combining studies to compare apples to oranges and start including studies that are comparing apples to bananas, apples to grapes and oranges to pineapples, etc we are to end up with a fruit salad rather than an understanding of the original comparison being sought (leaving aside the possibility of performing a network meta-analysis). Even if only apple versus orange studies are included, if they are of poor quality where perhaps the “appleness” or “orangeness” has been poorly measured we are unlikely to get reliable results. Summarizing,- GIGO (garbage in - garbage out)\n- excessive unexplained heterogeneity = fruit salad\nAnalyses\nConsider their Figure 7 (chosen since the 183 studies have been grouped into 23 groups making data extraction more manageable). \n\n\n\n\n\n\n\n\nQuestion: Does this Figure represent a meta-analysis?Answer: Given that a meta-analysis is the statistical combination of the results of multiple studies to quantify the effect size across all of the studies, and given its absence (as well as any measure of the combined variation), it is hard to label this Figure a meta-analysis.\nHere is a Figure that does include the desired measures.\n\n\n\n\n\n\n\n\nThe combined effect shows a results that is comparable with the reported depression prevalence of 27.2% (95% CI, 24.7% to 29.9%) with the difference due to the original analysis not grouping the 183 studies.Question: What is the dotted line and what does it represent?Answer: This is known as the prediction interval. Remember that in a random effects model the studies come from an over-riding population distribution and the 95% CI of this distribution is the prediction interval. Therefore the next study would come from a draw from this distribution and NOT from a draw of distribution of the mean effect shown in the line above.\n\n\n\n\n\n\n\n\nQuestion: What is probability of the prevalence of depression in the next study (or in your medical school)?Answer: Although 95% CI are often interpreted as probability intervals, this is actually incorrect. To obtain probability statements one needs to switch to a Bayesian paradigm. This is beyond the scope of this presentation but fortunately, given the large amount of available data, little error is introduced by not doing the formal Bayesian analysis. This is shown by the figure below, produced by a Bayesian analysis with vague priors, which gives essential the same result as the frequentist analysis.\n\n\n\n\n\n\n\n\nThe blue areas represent the probability distributions and the thin and thick black lines the 80% and 95% credible intervals (CrI). The mean and 95% CrI are 0.314, 0.250 - 0.379, very close to the previous confidence intervals.\nSince we are interested in the next study we need the prediction interval distribution which follows\n\n\n\n\n\n\n\n\nConclusions\nWhat to think about the authors’ conclusion “the prevalence of depression among medical students is 27%”?\nIgnoring the non-trivial limitations of depression screening tools as diagnostic tools and the lack of validity for self administration, a proper understanding of the statistical model suggests that this meta-analysis of these diverse publications reveals the next study is likely to show at least some medical students have depressive symptoms but this is likely less than 75%!\nIOW, this study is not very informative and its implied precision is a misrepresentation.\nCaveat lector Figure on the top is often presented but figure on the bottom better reflects reality\n\n\n\n\n\n\n\nCaveat lector\n\n\n\n\n\n\n\n\nCaveat lector\n\n\n\n\n\n\nCaveat lector\nCaveat lector\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {What Is the Prevalence of Depression in Medical Students?},\n  date = {2024-02-05},\n  url = {https://brophyj.github.io/posts/2024-02-05-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “What Is the Prevalence of Depression in\nMedical Students?” February 5, 2024. https://brophyj.github.io/posts/2024-02-05-my-blog-post/."
  },
  {
    "objectID": "posts/2024-02-19-does-it-make-a-difference-sedentary-break/index.html",
    "href": "posts/2024-02-19-does-it-make-a-difference-sedentary-break/index.html",
    "title": "Does it make a difference - sedentary break",
    "section": "",
    "text": "Background\nRecall that the goal of the 605 course is to not only improve critical appraisal skills, but also to think about research questions, designs, and the necessary compromises that are often required in research. Hopefully this adds another dimension to typical journal clubs where either an article in uncritically endorsed enthusiastically or trashed unmercifully, although this is sometimes quite merited This week’s selected article was the 2023 publication Breaking Up Prolonged Sitting to Improve Cardiometabolic Risk: Dose–Response Analysis of a Randomized Crossover Trial concluding\n\nThe present study provides important information concerning efficacious sedentary break doses. Higher-frequency and longer-duration breaks (every 30 min for 5 min) should be considered when targeting glycemic responses, whereas lower doses may be sufficient for BP lowering.\n\nIn this paper, of 25 participants who attended a screening visit, 18 were randomized and 11 completed the randomized crossover designed study which investigated 5 different strategies to examine the acute effects of multiple doses of a light-intensity walking-based sedentary break intervention on cardiometabolic risk factors among middle- and older-age adults. The trial conditions consisted of one uninterrupted sedentary (control) condition and four acute (experimental) conditions that entailed different sedentary break frequency/duration combinations: (1) light-intensity walking every 30 min for 1min, (2) light-intensity walking every 30 min for 5 min, (3) light-intensity walking every 60 min for 1 min, and (4) light-intensity walking every 60 min for 5 min. As the largest response was for glucose differences, will restrict this commentary to that outcome.\nBefore applying any reanalyzing of their data, let’s just stop for a moment and ask ourselves the following question\n\nHow likely do we think a study with only 11 individuals can detect meaningful glucose difference in glucose measurements with these anti sedentary strategies? Would one expect the differences to be so large that they could be detected by this small a sample size?\n\nIn any case, their conclusion appears supported by their published Figure 1 as shown here\n\n\n\n\n\n\n\n\n This Figure is remarkable for 2 main points\n1. The early separation between the control and intervention groups which appears maximum for 5 minutes exercise every 30 minutes\n2. The outcome is not the glucose level from each randomized but the difference in level compared to the control (baseline) group. \nThe early outcome difference\nIn the supplemental material, the authors report summary glucose levels for each group at 15 minute intervals. Using these values, we may simulate the glucose measurements for each group. Let’s consider the values at 45 minutes after T0. Given the authors state that the first 20 minutes are assigned to a standardized meal and that no internvetion occurs before 30 minutes, any variation in the 45 minutes values can’t be due to the intervention. Yet, plotting an analyzing this data (ANOVA) reveals the following\n\n\n[1] \"Analysis of variance using outcome of group glucose - control glucose\"\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)\ngroup        3   1953     651      10 0.000046\nResiduals   40   2594      65                 \n\n\n\n\n\n\n\n\nFinding these large differences even before any of the interventions could take effect should be a red flag for reservations about the final conclusions.The outcome measure\nUsing the outcome measure as the difference in glucose levels between the active treatments strategies and the control (baseline) group is a potentially fatal flaw. The fatal flaw is using the difference from baseline as their outcome measure. Bland and Altman have published about why this “is biased and invalid, producing conclusions which are, potentially, highly misleading. The actual alpha level of this procedure can be as high as 0.50 for two groups and 0.75 for three”. In short, we need to remember what is being randomized, it is the assignment to a given treatment strategy. Individuals are not randomized to their baseline glucose levels, any more than they are randomized to their weights, heights, eye color, or any other characteristic. With small sample sizes we may well expect that even in randomized samples, there may be meaningful differences in these characteristics, therefore including them as a component of the outcome is not appropriate and may bias the results.Harrell lists all the many assumptions required to be met before analyzing change from baseline could (potentially) be used.\ni. the variable is not used as an inclusion/exclusion criterion for the study, otherwise regression to the mean will be strong\nii. if the variable is used to select patients for the study, a second post-enrollment baseline is measured and this baseline is the one used for all subsequent analysis\niii. the post value must be linearly related to the pre value\niv. the variable must be perfectly transformed so that subtraction “works” and the result is not baseline-dependent\nv. the variable must not have floor and ceiling effects\nvi. the variable must have a smooth distribution\nvii. the slope of the pre value vs. the follow-up measurement must be close to 1.0 when both variables are properly transformed (using the same transformation on both)\nWith this in mind, this study doesn’t meet these assumptions (according to their CONSORT Fig 1, glucose was an entrance criteria, therefore possible regression to the mean may be present, glucose has a definite “floor effect”, and no proof that model is linear).\nTo demonstrate the bias inherent in using change from baseline as opposed to final glucose reading alone, let’s again use the simulated data. We will analyze (ANOVA) and plot the outcome according to treatment where the outcome is i) the final glucose reading in each group or ii) ii) the change from baseline for each group.\n\n\n\n\n\n\n\n\nThis suggests the original analysis is quite wrong and there is likely no difference between the strategies being tested. This is no surprise as with only 11 (or 9) subjects), even with a crossover design, the differences would have to be to very large to reach statistical significance. See the common sense response to the opening question above.\nThere are some other additional discussion points to consider.\n1. While 18 subjects were randomized only 11 were analyzed and 2 of them had missing values. This raises the possibility of a non-quantifiable selection bias.\n2. The authors report a post-hoc power calculation, a statistically inappropriate and nonsensical technique. If a nonsignificant finding was obtained, power will always be low to detect the observed effect size, as observed power is directly related to the obtained P value, with the former providing no additional information than the latter.\n3. There is no discussion of whether these measured outcomes have any clinical relevance. Suppose against all reason, the true glucose incremental area under the curve differential for the best treatment strategy was indeed the reported -11.8 mg% over 8 hours. This translated to 1.4 mg% / hour for an 8 hour day or 0.47 mg% / hour over 24 hours. How likely would this small (i.e. trivial) a difference have any meaningful clinical effect? Important to recall the adage“Measure what is important and don’t make important what you can measure”\n\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {Does It Make a Difference - Sedentary Break},\n  date = {2024-03-05},\n  url = {https://brophyj.github.io/posts/2024-02-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “Does It Make a Difference - Sedentary\nBreak.” March 5, 2024. https://brophyj.github.io/posts/2024-02-19-my-blog-post/."
  },
  {
    "objectID": "posts/2021-10-12-heparin-in-hospitalized-covid-19-patients/index.html",
    "href": "posts/2021-10-12-heparin-in-hospitalized-covid-19-patients/index.html",
    "title": "Heparin in hospitalized COVID-19 patients",
    "section": "",
    "text": "Spyropoulos and colleagues published randomized clinical trial on therapeutic-dose heparin in hospitalized COVID-19 patients where they concluded that the primary efficacy outcome was reduced in non-intensive care unit (ICU) patients, but not in ICU patients. In the intention-to-treat population, the authors reported a relative risk (RR) between the therapeutic dose and standard dose was 0.46 (95% confidence interval [CI] 0.27 - 0.81; p = 0.004) in the non-ICU stratum and 0.92 (95% CI 0.62 - 1.39; p = 0.71) in ICU stratum. Due to this difference in statistical significance, the authors concluded that the effect was only present in the non-ICU stratum.\nAlong with colleagues Arthur Albuquerque and Carolina Santolia, we felt the statistical analysis supporting this conclusion was questionable and therefore submitted a letter to the editor which was not accepted for publication as the editors were “unable to assign your letter a sufficient priority for publication in JAMA Internal Medicine”. Consequently in addition to submitting the letter to PubPeer, I have reproduced the letter in this post.\nAs noted by Altman and Bland, statistical analysis should be targeted to the clinical question: is the effect of therapeutic-dose heparin different between ICU and non-ICU patients? To answer this question, one should directly compare the efficacy of therapeutic-dose heparin between non-ICU and ICU patients using am interaction test, which was not performed in this study. Thus, the authors cannot conclude that the treatment effect differed between these subgroups based on their simple comparison of statistically significant and statistically insignificant p values. We applied the appropriate interaction analysis and found a ratio of relative risks equal to 0.5 [95% CI 0.25, 0.99; p = 0.046] between non-ICU and ICU patients.\nBeyond applying the proper statistical test, it must also be properly interpreted. If complete equipoise existed in the null hypothesis of no therapeutic difference between the different hospital populations before this study, then after observing this data there remains a 13% probability that the null hypothesis is true. Of course, if there existed a stronger prior belief in the null hypothesis, then the posterior belief in the null hypothesis of no difference would be even greater than 13%, despite the interaction p value &lt; 0.05.\nHighly uncertain times, such as the COVID-19 pandemics, require rapid evidence to aid clinicians in making decisions at the bedside and reliable subgroup analyses are of obvious importance in enhancing personalized clinical decision-making. However, this requires both the appropriate statistical analysis and its proper interpretation.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Heparin in Hospitalized {COVID-19} Patients},\n  date = {2021-10-12},\n  url = {https://brophyj.github.io/posts/2021-10-12-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Heparin in Hospitalized COVID-19\nPatients.” October 12, 2021. https://brophyj.github.io/posts/2021-10-12-my-blog-post/."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html",
    "title": "Misinformed conclusions",
    "section": "",
    "text": "In three papers that I read this week, I believe that each suffers from misinterpretations in their conclusions[@RN6772][@RN6776][@RN6773]."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#likely-unintentional-but-misinformation-nevertheless",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#likely-unintentional-but-misinformation-nevertheless",
    "title": "Misinformed conclusions",
    "section": "",
    "text": "In three papers that I read this week, I believe that each suffers from misinterpretations in their conclusions[@RN6772][@RN6776][@RN6773]."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#example-1",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#example-1",
    "title": "Misinformed conclusions",
    "section": "Example 1",
    "text": "Example 1\n\n\n\n\n\n\n\n\nIn the first paper[@RN6772], the authors conclude\n\n“In Medicare patients with pure native AR, TAVR with the current commercially available transcatheter valves has comparable short-term outcomes. Although long-term outcomes were inferior to SAVR, the possibility of residual confounding, biasing long-term outcomes, given older and frailer TAVR patients, cannot be excluded.”\n\nWhile acknowledging that TAVR was associated with higher unadjusted (HR, 1.90; 95% CI, 1.59-2.26; P &lt; .001) and adjusted risk of all-cause mortality compared with SAVR (adjusted HR, 1.41; 95% CI, 1.03-1.93; P = .02) with long term follow-up (which may indeed be at least partially explained by residual confounding), the authors interpret the results as being comparable for the two techniques at one year. They report one year propensity score adjusted mortality as 5.7% and 6.9% mortality in the TAVR and SAVR patient, respectively (p=0.3).\nDrawing conclusions based on p values is known to be dangerous and a proposed, albeit perhaps minor, improvement is to use confidence intervals[@RN5420]. The following shows the risk difference with the 95% CI for the two mortality outcomes.\n\nCodeprop.test(c(round(c(.057*9880,.069*1147))),c(9980,1147))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(round(c(0.057 * 9880, 0.069 * 1147))) out of c(9980, 1147)\nX-squared = 2.7, df = 1, p-value = 0.1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.028287  0.003362\nsample estimates:\n prop 1  prop 2 \n0.05641 0.06888 \n\n\nThe implies that even ignoring any biases, concluding that the two approaches have similar short term outcomes is only reasonable if one believes that a possible 2.8% absolute mortality reduction with SAVR from sampling variation alone is not clinically important. Others might consider that the conclusion is another example of conflating absence of evidence with evidence of absence and that additional data is required before drawing firm conclusions. This underscores the problem with dichotomized p values and risk ratios (as opposed to risk differences) as decision making tools."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#example-2",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#example-2",
    "title": "Misinformed conclusions",
    "section": "Example 2",
    "text": "Example 2\nA NEJM article[@RN6776] reported P that\n\n“the prophylactic use of tranexamic acid during cesarean delivery did not lead to a significantly lower risk of a composite outcome of maternal death or blood transfusion than placebo” (RR 0.89; 95% CI 0.74 to 1.07; P=0.19 for the primary outcome).”\n\nThese results could also be expressed as risk differences and attributable risks.\n\nCodemat1 &lt;- matrix(c(233,201,5238,5328), nrow = 2, \n              dimnames = list(c(\"placebo\",\"tranexamic acid\"),c(\"Outcome +\",\"Outcome -\")))\nmat1\n\n                Outcome + Outcome -\nplacebo               233      5238\ntranexamic acid       201      5328\n\nCodeepiR::epi.2by2(mat1)\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          233         5238       5471        4.26 (3.74 to 4.83)\nExposed -          201         5328       5529        3.64 (3.16 to 4.16)\nTotal              434        10566      11000        3.95 (3.59 to 4.33)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 1.17 (0.97, 1.41)\nOdds ratio                                     1.18 (0.97, 1.43)\nAttrib risk in the exposed *                   0.62 (-0.10, 1.35)\nAttrib fraction in the exposed (%)            14.64 (-2.70, 29.05)\nAttrib risk in the population *                0.31 (-0.30, 0.92)\nAttrib fraction in the population (%)         7.86 (-1.76, 16.57)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 2.820 Pr&gt;chi2 = 0.093\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.096\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nThe risk difference is 6 fewer outcomes /1000 treated (95% CI-13.5, 1.0) and the attributable fraction is 14.6% (95% CI -2.70, 29.05) of outcomes being eliminated by treatment.  While these measures do not reach statistical significance, they may provide some additional insights into the trial’s interpretation as the associated sampling variations suggest an effect size as large as 13 fewer outcomes / 1000 treated and an attributable fraction as large as 29% have not been eliminated. If one believes these potential measures are of clinical importance, then continuing research with this agent may be indicated."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#example-3",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#example-3",
    "title": "Misinformed conclusions",
    "section": "Example 3",
    "text": "Example 3\nThe third paper is entitled ” Effect of a Run-In Period on Estimated Treatment Effects in Cardiovascular Randomized Clinical Trials: A Meta-Analytic Review”[@RN6773] and the authors concluded “The use of a run-in period was not associated with a difference in the magnitude of treatment effect among cardiovascular prevention trials”.\nI found this result to be a priori very surprising. However without the raw data it is difficult to completely reproduce and assess what the authors have done. Nevertheless working with the aggregate data from the last column in Figure 1, I have performed a Bayesian analysis, with a vaguely informative prior such that the results are completely dominated by the published data.\nThe posterior probability is displayed below.\n\n\n\n\n\n\n\n\nggplot version\n\n\n\n\n\n\n\n\nBased on this analysis, the probability that run-in trials has a larger treatment effect than non-run-in trials was 99 %. There was an 79 % probability that the effect size was at least 5% greater in the run-in trials. While this effect is not large, it is more in keeping with face validity that would suggest that run-in trials, by excluding non-compilers and those developing side effects, would be expected to yield a larger effect size than a comparable trial without a run-in period."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#references",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#references",
    "title": "Misinformed conclusions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/2022-12-05-scientific-authorship/index.html",
    "href": "posts/2022-12-05-scientific-authorship/index.html",
    "title": "What does scientific authorship mean?",
    "section": "",
    "text": "I recently reviewed a paper with 18 authors which reached a conclusion of no morality difference between 2 contrasts (RR 1.0, 95% CI 0.5-2.0). Now this is an obviously faulty interpretation that mixes “absence of evidence with evidence of absence”. This got me thinking about scientific authorship and raised several existential questions.\n1. Are all 18 authors statistically incompetent or is it only the 1 or 2 authors who probably did the majority of the work that are deficient? 2. If it is only 1-2 authors who are statistically incompetent, then what of the other 16-17 authors? Did they simply not bother to critically read a manuscript to which they signed names?\n3. Do these individuals, and by extension clinicians in general, know what constitutes authorship?\n4. How likely is it that these 18 individuals all meet authorship criteria?\n5. How seriously do journals view authorship criteria and do they take any measure towards verification? The International Committee of Medical Journal Editors (ICMJE) have produced an article that is helpful in defining the role of authors and contributors which can be found here and assists in answering some of the above questions. The documents underscores that“authorship confers credit and has important academic, social, and financial implications. Authorship also implies responsibility and accountability for published work.” The ICMJE document recommends that authorship be based on the following 4 criteria:\n1. Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work;AND\n2. Drafting the work or revising it critically for important intellectual content;AND\n3. Final approval of the version to be published;AND\n4. Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\nThe ICMJE document also mentions several non-authorship contributions including one that is perhaps especially germane for clinicians “provided and cared for study patients”\nSo returning to my existential questions;\n1. I don’t know the answer but prefer to believe that the problem of statistical incompetence involved all 18 authors, although about 75% of respondents to my twitter poll believed the opposite (see next point as to why I prefer this option)\n2. I find it preferable to believe in a general problem of statistical incompetence, which is correctable with simple educational initiatives, over the alternative hypothesis of a lack of integrity, which seems more difficult to readily address.\n3. I strongly doubt that most authors are aware of the ICMJE authorship criteria\n4. I think the probability of all 18 meeting the authorship criteria is about zero.\n5. My personal experience, admittedly perhaps somewhat selected and limited, suggests that authorship criteria is not of major concern for journal editors. Not sure where this little rant takes me, other than to state the obvious that medical publishing needs to pay more attention to authorship criteria to assure its overall scientific quality, credibility and accountability. \nAlternatively, the message about authorship and the quality of the associated science is perhaps better captured by a XKCD inspired cartoon that has been adapted for publications in epidemiology\n\n\n\n\nCitationBibTeX citation:@online{brophy2022,\n  author = {Brophy, Jay},\n  title = {What Does Scientific Authorship Mean?},\n  date = {2022-12-05},\n  url = {https://brophyj.github.io/posts/2022-12-05-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2022. “What Does Scientific Authorship Mean?”\nDecember 5, 2022. https://brophyj.github.io/posts/2022-12-05-my-blog-post/."
  },
  {
    "objectID": "posts/2024-02-12-endotoxin-exposure-in-asthmatic-children-and-matched-healthy-controls/index.html",
    "href": "posts/2024-02-12-endotoxin-exposure-in-asthmatic-children-and-matched-healthy-controls/index.html",
    "title": "Endotoxin exposure in asthmatic children and matched healthy controls",
    "section": "",
    "text": "Background\nRecall that the goal of the course is to not only improve critical appraisal skills but also to think about research questions, designs, and the necessary compromises that are often required in research. Hopefully this adds another dimension to typical journal clubs where either an article in uncritically endorsed enthusiastically or trashed unmercifully, although this is sometimes quite merited For reasons I am somewhat unclear about, this week’s selected article was the 2005 publication Endotoxin exposure in asthmatic children and matched healthy controls: results of IPEADAM study. Although not described as such, this was apparently a nested case-control study from a 1999 crosssectional study by the same authors, of over 1500 completed questionnaires. Cases were children between the ages of 4 and 17 who had a study diagnosis of probable asthma. These asthmatic children were matched for sex, age and sib-ship size with children living in asthma free households and the following main conclusions reported. \n\n\n\n\n\n\n\n\nA critical analysis should consider bias and statistical issues\nBiases\nIt is helpful to consider biases in observational studies according to the following grouping\n- selection\n- misinformation\n- confounding\nSelection bias\nSelection bias is a distortion in the measure of association due to a sample selection such the measured association is not representative of what exists in the target population.\nIf the selection of cases and controls is not done independently of the exposure, then when you compare exposure distributions any observed differences are a combination of a true effect and an artifact of the way the data was collected.\nThis figure may help explain. Where would you put the reinforcements?\n\n\n\n\n\n\n\n\n\nAfter identification of the research question, the next step is to distinguish among the various populations, including the\n- target population (all children in the UK)\n- study population (all those with a chance of being in the study, ie the 1500 children who completed the original questionnaire)\n- sample population (the 200 children in Figure 1)\n- analyzed sample population (the 90 matched pairs of children)\nIn their previous work, the authors report an asthma prevalence of approximately 20%, or about 300 children based on their study population of 1500. The following questions need answering.\n- Does the study population (?1500) reflect the target population?\n- Do the 200 sample children reflect the total study population ?\n- Do the 90 matched pairs, the analytical sample population reflect the overall sample population? What about the missing cases?Many reasons to be concerned about possible selection bias in this study.\nLet’s begin by examining their Figure 1 which is reproduced below. \n\n\n\n\n\n\n\n\n\nHow many asthma cases (and controls) did they identify in this Figure?\nThe figure shows 105 cases and 93 controls, a total of 198 children. Yet the authors claim this figure shows the age distribution of 200 children. While the difference is small, it does raise further concerns about the reliability of their data analysis when there is a discordance in a simple counting of individuals.\nWhy do they only use 90 cases? Presumably to meet their matching criteria, age (+/- 1 year). Below is the best matching I could create from their data.\n\n\n\n\n\n\n\n\n\nThe question then becomes why match?\nMost people think matching is to control for confounding but this is only partly true (and indeed may be more than offset by potential selection bias) and the best reason is to improve precision in situations of sparse data.\nMoreover, in this case a better alternative would have been not to match but to control for age in regression analysis as this would have allowed an estimation of the effect of age on the detection of asthma, while using the whole study sample of 198 children, including all 105 cases.\nPerforming logistic regression glm(stat~ages, data = data_long_uncount, family = \"binomial\") shows that each year of additional age is associated with a odds ration decrease in asthma diagnosis = OR = 0.92, 95%CI 0.89 - 0.96\nMisclassification\nThe authors don’t report the raw data for the endotoxin exposure but assuming the following distribution among cases and controls gives an OR that approximates their results\n\n\n--Observed data-- \n         Outcome: Exposed+ \n       Comparing: Cases vs. Controls \n\n         Cases Controls\nExposed+    40       50\nExposed-    27       63\n\n                            2.5% 97.5%\nObserved Relative Risk: 1.3  1.0   1.8\n   Observed Odds Ratio: 1.9  1.0   3.4\n---\n                                                   \nMisclassification Bias Corrected Relative Risk: 1.3\n   Misclassification Bias Corrected Odds Ratio: 1.9\n\n\nAs the authors report from their previous work that their asthma detection questionnaire has only a 70% sensitivity and 91% specificity misclassification is present.\nThe effect of misclassification can be quantified using quantitative bias analysis (QBA) via the episensr package and misclassification function. This seems an improvement over purely qualitative heuristics, such as “non-differential misclassification biases toward the null”.\n\nCodelibrary(episensr)\nmisclassification(matrix(c(40, 50, 27, 63),\n                          dimnames = list(c(\"Exposed+\", \"Exposed-\"), c(\"Cases\", \"Controls\")),\n                         nrow = 2, byrow = TRUE),\n                  type = \"outcome\",\n                  bias_parms = c(.70,.70,.91,.91))\n\n--Observed data-- \n         Outcome: Exposed+ \n       Comparing: Cases vs. Controls \n\n         Cases Controls\nExposed+    40       50\nExposed-    27       63\n\n                            2.5% 97.5%\nObserved Relative Risk: 1.3  1.0   1.8\n   Observed Odds Ratio: 1.9  1.0   3.4\n---\n                                                   \nMisclassification Bias Corrected Relative Risk: 1.4\n   Misclassification Bias Corrected Odds Ratio: 3.6\n\n\nConceptually, this misclassification of cases and controls suggest any association between indoor air pollutants and asthma will likely be underestimated and this analysis permits an estimate of the order of magnitude.\nConfounding\nHow much residual or unmeasured confounding would be required to wipe out the observed effect? This can be determined using the EValue package as described in this paper.\n\n\n         point lower upper\nRR         1.4   1.1   1.8\nE-values   2.1   1.3    NA\n\n\nThis suggests that a moderate confounder with a risk of 2.1 fold to exposure and a 2.1 fold increase in the outcome would be required to eliminate the observed risk. Of course if the true OR is greater due to misclassification bias the size of unmeasured confounder needed to eradicate the observed association would be even larger.\nIn conclusion, there are multiple potential biases (selection, misclassification and confounding) associated with this study. Unmeasured confounding is unlikely to explain the observed association and misclassification suggest the association may be underestimated, but the inability to evaluate the magnitude and direction of the selection bias limits definitive determination of the overall direction of the bias assessment.\nAnalyses\nThe authors they performed a logistic regression where presumably probable asthma is the dependent variable. In this case understanding their table 2 is difficult.\n\n\n\n\n\n\n\n\n From Table 2 the authors make causal statements not only about endotoxins and asthma but also about other variables, including for example, dampness and single parent home.\nMaking assertions based on regression coefficients from a multivariable analysis is subject to Table 2 Fallacy. These covariate effect estimates may also be confounded even though the effect estimate for the main exposure is not confounded and their proper interpretation far from obvious. The authors state “There was no difference in the levels of exposure to house dust major allergen Der p 1, expressed in quartiles of exposure between asthmatic and non-asthmatic matched controls (Table 3)”\n\n\n\n\n\n\n\n\n Looking at the confidence intervals we see they are very wide and, for example, don’t exclude that for quartile 2 there is a 180% increase or 64% decrease in asthma prevalence compared to quartile 1. The authors have conflated “an absence of evidence with evidence of absence”. Aslo see this previous post.\nThe authors make the same error in Table 4\n\n\n\n\n\n\n\n\n when they conclude there are no interactions. The more reasonable conclusion being even if one ignores the potential biases, they lack sufficient power to draw conclusions about interactions.\nConclusion\nA critical appraisal of this publication has shown several interesting avenues for discussion, perhaps explaining why it was suggested for review. Based on the critical appraisal it would seem there is little to support their original conclusions.\n\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {Endotoxin Exposure in Asthmatic Children and Matched Healthy\n    Controls},\n  date = {2024-02-19},\n  url = {https://brophyj.github.io/posts/2024-02-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “Endotoxin Exposure in Asthmatic Children and\nMatched Healthy Controls.” February 19, 2024. https://brophyj.github.io/posts/2024-02-19-my-blog-post/."
  },
  {
    "objectID": "posts/2023-03-22-something-old-and-something-older/index.html",
    "href": "posts/2023-03-22-something-old-and-something-older/index.html",
    "title": "Something old and something older",
    "section": "",
    "text": "I’m co-teaching a small group learning course on critical appraisal for our MPH students. We tried to avoid the common theme of many critical appraisal courses and journal clubs that simply try to trash whatever article is under review. Rather we divide the groups into three sections and the first one receives only an introduction to the problem, and without looking at the chosen article, discusses what sort of study design, with the associated strengths and limitations, they would consider to address the problem. The second group receives only the abstract and comments on what details they would expect to see in the article. The third group reads the whole article and in addition to highlighting its strengths and limitations tries to answer queries raised by the first 2 groups. The basic idea is not only to improve critical appraisal skills but also to think about possible research designs and the necessary compromises that are often required in doing research.\nThis week one of the other co-instructors (for reasons that will become evident later) chose an older NEJM article from 2007 on the effectiveness of influenza vaccine in the community-dwelling elderly to evaluate. Despite several strengths including a prospective cohort design with a large sample size and controling for several important confounders, the reported 50% reduction in all cause mortality for the vaccinated group is an obvious red flag that a substantial bias exists in the execution of this observational study.\nLetters to the editor immediately following the article’s publication noted that the mortality benefit exceeded all reasonable estimates, as influenza mortality during the winter months typically accounts for 5% or less of total mortality. In these letters, the perceived bias was attributed to residual and unmeasured confounding. While there is undoubtedly some degree of residual confounding, it seems unlikely that it could explain this magnitude of bias, from a theoretical ceiling of an approximate 5% mortality decrease to the reported 50% decrease. The clue to resolving this bias is found by a careful reading of their methods\nAll noninstitutionalized members of the plans were included in that season’s cohort if they were 65 years of age or older as of October 1, had been continuously enrolled in the plan for the preceding 12 months, were alive on the first day of the influenza season, and were either continuously enrolled or died during the outcome period.\nSince it is virtually impossible that the vaccinated individuals all received their vaccination on Oct 1 of each flu season, it appears that the researchers looked in the future to determine their vaccinated and unvaccinated groups. This leads to an obvious misclassification of exposure time. This neglect of the period of exposure to risk was well described by Bradford Hill (of the causal checklist fame) in a 1937 Lancet publication where he even provides a numerical example involving inoculation! The data for his example is presented below with both the true and fallacious comparisons between inoculated and non-inoculated subjects where he assumed the true attack rate was identical in both groups. By neglecting the time of inoculation, the true risk ratio of 1 gives a biased result of a 50% reduction.  Inspired by Hill’s example, I re-examined the 2007 data. Now the authors didn’t give us the raw data, not even the total number of deaths in either exposure group nor the timing of vaccinations so some assumptions must be made. They did provide the total number of vaccinated (415,249), unvaccinated (298,623) and total number of deaths (8796). It seems safe to assume that the vaccinated didn’t all get it on Oct 1. I assumed that on Oct 1 nobody was vaccinated and that subsequently on Nov 1, Dec 1 and Jan 1, 1/3 got vaccinated at each date with 1% waiting until Feb 1. Following Hill’s example, I assumed there was no difference in death rates between the 2 groups and that the death rate per month was 0.002058. Again following Hill’s work I produced the following table. Now if one analyses the data in the above Table, as per the NEJM (see Total line), the crude RR = 0.45, this is slightly different than the reported RR but to be expected as i had to guess what the actually monthly mortality rates were.\nHowever the essential point is that the unadjusted RR in my Table is derived when the data simulation has assumed a constant monthly mortality rate that is NOT different between the exposed and unexposed groups.\nIn other words, almost the entire effect can be explained by inaccurate classification of exposure times. Using the proper method of person time, as shown in Hill’s example, gives the correct RR = 1. It is quite amazing that 70 years after Hill’s publication, authors were still getting it wrong. It’s not only the original authors, reviewers and editors but also the whole medical commons. For example, in the 2007 letters to the editor that queried this unbelievable mortality result, nobody brought up this potential source of error, rather everyone tried to uniquely blame residual confounding. Moreover, as of March 23 2023, despite these initial confounding concerns, this publication has been cited 233 times, including 13 times in 2022. In only 5 of these 233 citations was the term “bias” also found and in those papers the discussion was again limited to confounding. Two potential confounding sources mentioned were i) increased vaccinations in the worried well ii) decreased vaccination in those deemed to have too many competing comorbidities for vaccination to make a difference. The majority of articles citing this 2007 publication therefore seem to have taken the results at face value. As an example, in 2023 we can find the following uncritically writing “prevent influenza-related hospitalizations and deaths among older adults” citing this 2007 publication as the reference.\nIn more modern times, this inaccurate accounting of exposure time and analysis has been termed immortal time bias by Suissa, one of the other co-instructors for our critical appraisal course and who suggested the article for review. Suissa has shown that immortal time bias and can lead to large discrepancies from the truth by either misclassification or selection bias.\nHernan and colleagues have proposed a paradigm of emulating a randomized trial to minimize the biases associated with observational studies, including immortal time. The key is align i) Elgibity ii) Assignment and iii) follow-up \\(T_o\\) times. Failure to align, as shown in the following figure may lead to i) prevalence bias ii) selection bias iii) immortal time (due to selection bias) and iv) immortal time bias (due to misclassification).\n\nSo, my general and specific take home messages are\nGeneral\n1) teaching is a great way to keep learning about the past and present\n2) answers to current problems can often be found in past work\n3) although science may be self correcting, the path to self correction can be exceedingly long\nSpecific\n1) when searching for biases, don’t stop with confounding as misclassification and selection bias may be hugely important\n2) in the presence of a large unexpected effect size, think IMMORTAL TIME BIAS (especially present in observational pharmacoepidemiology studies) as the magnitue of this bias may dwarf any confounding bias\n\n\n\nCitationBibTeX citation:@online{brophy2023,\n  author = {Brophy, Jay},\n  title = {Something Old and Something Older},\n  date = {2023-03-22},\n  url = {https://brophyj.github.io/posts/2023-03-22-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2023. “Something Old and Something Older.”\nMarch 22, 2023. https://brophyj.github.io/posts/2023-03-22-my-blog-post/."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "2025",
    "section": "",
    "text": "Presentations \n\n2025\n\n\nDoctoral seminar - Epidemiology 706\nMcGill University, Department of Epidemiology, Winter 2025Summary - 706 is a PhD-level seminar aimed at providing space for students to engage with over-arching concepts critical to the theory and practice of epidemiology, as well to explore recent controversies and debates in the field. The purpose of this course to reinforce your formal methodological coursework by making space to develop and sharpen your critical thinking skills.\n\n\n\n Syllabus (opens in new tab) \n\n\n\nStatistics, uncertainty and the physician\nMcGill University Health Centre, Department of Medicine Grand Rounds, Jan 7 2025Summary - Some not so random reflections on statistical concepts tailored for general internists and cardiologists\n\n\n\n View Presentation (opens in new tab) \n\n2024\n\n\nHierarchical models\nMcGill University, Department of Epidemiology, Nov 2024Summary - Lecture to PhD students (EPIB704) on the benefits of and procedures for performing hierarchical modelling\n\n\n\n View Presentation (opens in new tab) \n\n\n\nAn Introduction to Statistics for Cardiologists\nMcGill University Health Centre, Division of Cardiology Grand Rounds, Nov 2024Summary - Some not so random reflections on statistical concepts tailored for cardiologists\n\n\n\n View Presentation (opens in new tab) \n\n2023\n\n\n\nAn introduction to systematic reviews and (sometimes) meta-analysis\nMcGill University Medical School Lecture, Montreal QC, Nov 30 2023Summary - Review of basic systematic review and meta-analysis techniques for 2nd year medical students\n\n\n\n  Download slides \n\n\n\n\n\n\n\nA Bayesian view of contemporary randomized clinical trials – New reflections from an old lens\nMontreal Jewish General Hospital, Clinical Epidemiology Rounds, Montreal QC, Nov 15 2023Summary - Review of Bayesian principles and their advantages highlighted by re-analyses of several randomized trials recently published in the NEJM\n\n\n\n  Download slides \n\n\n\n\n\n\n\nDual antiplatelet therapy (DAPT) in 2023 - The McGill Experience\nMcGill University Health Centre, Department of Medicine Grand Rounds, Montreal QC, Oct 30 2023Summary - Review of three research studies performed at McGill - i) Bayesian network meta-analysis ii) pharmacoepidemiology study iii) cluster randomized registry trial\n\nYouTube presentation\n\n\n  Download slides \n\n\n\n\n\n\n\nL’experience de l’unité d’évaluation des technologies (TAU) de CUSM - Regarder dans le rétroviseur pour mieux voir l’avenir (The Experience of McGill’s Technology Assessment Unit - Looking back to better see the future)\nSymposium en évaluation des technologies et modes d’intervention en santé, Centre hospitalier de l’Université de Montréal (CHUM), Montreal QC, Oct 19 2023Summary - Review of TAU’s lessons learned, the first hospital based technology assessment unit\n\n\n\n  Download slides (english)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James (Jay) Brophy",
    "section": "",
    "text": "James (Jay) Brophy is a tenured (full) professor with a joint appointment in the Departments of Medicine and Epidemiology and Biostatistics at McGill University. He is a cardiologist and does research in cardiovascular epidemiology. His research interests are eclectic and include outcomes research, pharmacoepidemiology, Bayesian statistics, health technology assessment, economic analyses and clinical research. He held a FRQS chair in health technology assessment and evidence-based medicine (2008-2023).\n He is a fellow of the Canadian Academy of Health Sciences and the Canadian Cardiovascular Society.\nHave a look at my   \ncurriculum vitae - pdf version"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "James (Jay) Brophy",
    "section": "Education",
    "text": "Education\n\nPhD Epidemiology & biostatistics - McGill University\n\nCardiology / Internal medicine - Université de Montreal\n\nMD - McMaster University\n\nM.Eng - McMaster University\n\nB.Eng - McGill University"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "James (Jay) Brophy",
    "section": "Interests",
    "text": "Interests\n\nCardiovascular medicine & epidemiology\nBayesian statistics\nEvidence based medicine\nData analysis & visualization\nKnowledge sharing\nMedical decision-making"
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Art",
    "section": "",
    "text": "(Click on image to enlarge)"
  },
  {
    "objectID": "example.html#barney-watercolours",
    "href": "example.html#barney-watercolours",
    "title": "Art",
    "section": "Barney watercolours",
    "text": "Barney watercolours\nBarney (2011-) my great friend, running partner and favorite muse\n\n\n\n\n\n\n\n\n\nResting after a busy day\n\n\n\n\n\n\n\nPround profile\n\n\n\n\n\n\n\nEnjoying the sunshine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith a friend\n\n\n\n\n\n\n\nSnoozing\n\n\n\n\n\n\n\nSnoozing with a friend"
  },
  {
    "objectID": "example.html#barney-ipad-drawings",
    "href": "example.html#barney-ipad-drawings",
    "title": "Art",
    "section": "Barney iPad drawings",
    "text": "Barney iPad drawings\n\n\n\n\n\n\n\n\n\nStately\n\n\n\n\n\n\n\nWhere is everyone?\n\n\n\n\n\n\n\nWaiting for …"
  },
  {
    "objectID": "example.html#barney-photos",
    "href": "example.html#barney-photos",
    "title": "Art",
    "section": "Barney photos",
    "text": "Barney photos\n\n\n\nIn action"
  },
  {
    "objectID": "example.html#other-paintings",
    "href": "example.html#other-paintings",
    "title": "Art",
    "section": "Other paintings",
    "text": "Other paintings\n\n\n\n\n\n\n\n\n\nFlowers\n\n\n\n\n\n\n\nPoppies\n\n\n\n\n\n\n\nSheep\n\n\n\n\n\n\n\n\n\nFlowers1\n\n\n\n\n\n\n\nBunny\n\n\n\n\n\n\n\nFrom Sargent\n\n\n\n\n\n\n\n\n\nWinter\n\n\n\n\n\n\n\nWinter1\n\n\n\n\n\n\n\nZuna\n\n\n\n\n\n\n\n\n\nVenice\n\n\n\n\n\n\n\nSintra Portugal\n\n\n\n\n\n\n\nWestmount library\n\n\n\n\n\n\n\n\nResting after a busy day\nPround profile\nEnjoying the sunshine\nWith a friend\nSnoozing\nSnoozing with a friend\nStately\nWhere is everyone?\nWaiting for …\nIn action\nFlowers\nPoppies\nSheep\nFlowers1\nBunny\nFrom Sargent\nWinter\nWinter1\nZuna\nVenice\nSintra Portugal\nWestmount library"
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "",
    "text": "The Notion-2 trial(Jorgensen et al. 2024) was recently fast tracked for publication in the European Heart Journal. This study randomized low risk patients (≤75 years of age and median Society of Thoracic Surgeons (STS) risk score of 1.1%) with severe aortic stenosis (AS) to Transcatheter Aortic Valve Implantation (TAVI) or to conventional aortic valve surgery. The study population included both tricuspid and bicuspid AS.\nThe primary endpoint was a composite of all-cause mortality, stroke, or rehospitalization (related to the procedure, valve, or heart failure) at 12 months.\nA total of 370 patients were enrolled and 1-year incidence of the primary endpoint was 10.2% in the TAVI group and 7.1% in the surgery group [absolute risk difference 3.1%; 95% confidence interval (CI), −2.7% to 8.8%; hazard ratio (HR) 1.4; 95% CI, 0.7–2.9; P = .3]. The authors concluded\n&gt; “Among low-risk patients aged ≤75 years with severe symptomatic AS, the rate of the composite of death, stroke, or rehospitalization at 1 year was similar between TAVI and surgery.”"
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html#background",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html#background",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "",
    "text": "The Notion-2 trial(Jorgensen et al. 2024) was recently fast tracked for publication in the European Heart Journal. This study randomized low risk patients (≤75 years of age and median Society of Thoracic Surgeons (STS) risk score of 1.1%) with severe aortic stenosis (AS) to Transcatheter Aortic Valve Implantation (TAVI) or to conventional aortic valve surgery. The study population included both tricuspid and bicuspid AS.\nThe primary endpoint was a composite of all-cause mortality, stroke, or rehospitalization (related to the procedure, valve, or heart failure) at 12 months.\nA total of 370 patients were enrolled and 1-year incidence of the primary endpoint was 10.2% in the TAVI group and 7.1% in the surgery group [absolute risk difference 3.1%; 95% confidence interval (CI), −2.7% to 8.8%; hazard ratio (HR) 1.4; 95% CI, 0.7–2.9; P = .3]. The authors concluded\n&gt; “Among low-risk patients aged ≤75 years with severe symptomatic AS, the rate of the composite of death, stroke, or rehospitalization at 1 year was similar between TAVI and surgery.”"
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html#how-was-the-study-designed",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html#how-was-the-study-designed",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "How was the study designed?",
    "text": "How was the study designed?\nThe study was designed assuming a sample of 372 patients would provide the trial 90% power to show the non-inferiority of TAVI to surgery with regard to the primary endpoint at 1 year, assuming a Kaplan–Meier estimate of the primary endpoint of 10% in the TAVI group and 15% in the surgery group. The authors stated “To test for non-inferiority, we determined whether the upper boundary of the 95% confidence interval (CI) for the difference in the rate of the primary endpoint between the TAVI and surgery group was less than the pre-specified non-inferiority margin of 5% points.”\nIn other words, the null hypothesis (\\(H_O\\)) is \\(\\theta_{TAVI} - \\theta_{surgery} &gt; 5%\\) where \\(\\theta\\) is the proportion of outcomes in the respective treatment arms. One hopes to reject this null hypothesis and accept the alternative hypothesis (\\(H_A\\)) that the difference is &lt; 5% and therefore claim non-inferiority. Although the authors did claim non-inferiority, their data does not support this conclusions as the null hypothesis can’t be rejected as the upper limit of the 95% confidence interval for the difference in outcomes between TAVR and surgery is 8.8% exceeding the prespecified non-inferiority margin of 5% points."
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html#how-then-did-the-authors-reach-their-conclusion",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html#how-then-did-the-authors-reach-their-conclusion",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "How then did the authors reach their conclusion?",
    "text": "How then did the authors reach their conclusion?\nThe authors apparently ignored their non-inferiority design and analysed their study with conventional null hypothesis significance testing (NHST) and a null hypothesis \\(\\theta_{TAVI} - \\theta_{surgery} = 0\\) which they were unable to reject it since the p value (0.3) exceeded the conventional \\(\\alpha\\) level (0.05)\nNHST limitations and its tendency to cause cognitive errors has been described countless times in the medical literature[Wasserstein and Lazar (2016)](Wasserstein, Schirm, and Lazar 2019). This particular cognitive has even been described with the pithy aphorism absence of evidence is not evidence of absence(Altman and Bland 1995)."
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html#what-is-the-strength-of-this-conclusion",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html#what-is-the-strength-of-this-conclusion",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "What is the strength of this conclusion?",
    "text": "What is the strength of this conclusion?\nWith the NHST paradigm, it is impossible to quantify the evidence in favor of the null hypothesis. A non-significant finding can occur due to low power or a truly absent effect and the reporting of a p value simply can’t disentangled these two possibilities. An alternative to NHST is Null hypothesis Bayesian testing (NHBT) which allows the strength of evidence for (or against) \\(H_O\\) and \\(H_A\\) to be directly compared. This is most commonly achieved with Bayes factors, which quantifies the relative probabilities of the data under \\(H_O\\) and \\(H_A\\).\nBFs\nThe relationship between BFs and the relative support for for (or against) \\(H_O\\) and \\(H_A\\) is reflected in the following graphic.\n\nCode# Load necessary libraries\nlibrary(ggplot2)\nlibrary(grid)\n\ng &lt;- rasterGrob(c(\"lightgreen\", \"yellow\", \"orange\", \"red\"), \n                width=unit(1,\"npc\"), height = unit(1,\"npc\"), \n                interpolate = TRUE) \n# Create a continuous range for the y-axis\ny_values &lt;- seq(0.01, 1000, length.out = 10000)\n\n# Create data frame for plotting\ndata &lt;- data.frame(\n  BayesFactor = y_values\n)\n\n# Define a continuous color gradient from a deeper yellow to green to blue\ncolor_gradient &lt;- c(\"#FFD700\", \"#00B300\", \"#0000FF\")\n\n# Plot the data\nggplot(data, aes(y = BayesFactor, fill = BayesFactor)) +\n  geom_tile(aes(x = 0.5), width = 0.2) +  # Greatly reduce the x-direction plot area\n  scale_y_continuous(trans = \"log10\", \n                     breaks = c(0.01, 1/3, 1, 3, 10, 30, 100, 1000), \n                     labels = c(\"0\", \"1/3\", \"1\", \"3\", \"10\", \"30\", \"100\", \"∞\"),\n                     expand = c(0, 0)) +\n  #scale_fill_gradientn(colors = color_gradient, name = NULL) +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 1/3, yend = 1/3), linetype = \"dashed\", color = \"black\") +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 3, yend = 3), linetype = \"dashed\", color = \"black\") +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 10, yend = 10), linetype = \"dashed\", color = \"black\") +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 30, yend = 30), linetype = \"dashed\", color = \"black\") +\n  annotate(\"text\", x = 0.5, y = sqrt(0.01 * 1/3), label = \"Evidence against\\ntreatment effect\", size = 4.5, hjust = 0.5, vjust = 0.5) +\n  annotate(\"text\", x = 0.5, y = 2, label = \"Not enough data to\\nknow if the drug works\", size = 4.5, hjust = 0.5, vjust = 0.5) +\n  annotate(\"text\", x = 0.5, y = 20, label = \"Evidence for\\ntreatment effect\", size = 4.5, hjust = 0.5, vjust = 0.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(30 * 500), label = \"{very strong pro-alternative}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(10 * 30), label = \"{strong pro-alternative}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(3 * 10), label = \"{moderate pro-alternative}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(1/3 * 3), label = \"{ambiguous}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(0.01 * 1/3), label = \"{pro-null}\", hjust = 0, size = 4.5) +\n  coord_cartesian(xlim = c(0.4, 1), ylim = c(0.01, 1000), clip = \"off\") +  # Limit x-axis and ensure no clipping of text\n  theme_minimal() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")  # Reduced overall plot area\n  ) +\n  labs(y = \"Bayes Factor\") +\n  annotation_custom(g, xmin=0.4, xmax=0.6, ymin=-2, ymax=Inf) +\n  ggtitle(\"Bayes factors and evidential strength\")\n\nggsave(\"output/BF.pdf\", dpi = 600, device = \"pdf\")\nggsave(\"output/BF.png\", dpi = 600, device = \"png\")\n\n\n\nWhat is the Bayes Factor for the Notion-2 survival data?\nAn approach to calculating BFs within Cox proportional hazards survival models has been developed by Linde(Linde, Tendeiro, and Ravenzwaaij 2022) and operationalized with the baymedr package(Linde, van Ravenzwaaij, and Tendeiro 2022).\n\nCode# simulate Notion-2 dataset\nsim_data &lt;- coxph_data_sim( # See ?coxph_data_sim for details\n  n_data = 100, # Number of data sets to be simulated\n  ns_c = 183, # Sample size (control condition)\n  ns_e = 187, # Sample size (experimental condition)\n  ne_c = 13, # Number of events (control condition)\n  ne_e = 19, # Number of events (experimental\n  # condition)\n  cox_hr = c(1.4, 0.7, 2.9), # HR, lower bound CI, upper bound CI\n  cox_hr_ci_level = 0.95, # Confidence level CI\n  maxit = 300, # Max number of PSO iterations (for\n  # psoptim())\n  maxit.stagnate = ceiling(300 / 5), # Max number of PSO iterations without\n  # reduction in loss (for psoptim())\n  cores = 5 # Number of cores to be used\n)\nsave(sim_data, file = \"output/sim_data.RData\")\n\n\n\nCodeload(\"~/Desktop/current/notion2/output/sim_data.RData\")\n\nsim_bf &lt;- coxph_bf( # See ?coxph_bf for details\n  data = sim_data, # Object containing the data\n  null_value = 0, # H0 value\n  alternative = \"two.sided\", # H1 type (one- or two-sided)\n  direction = NULL, # H1 direction (low or high)\n  prior_mean = 0, # Beta prior mean\n  prior_sd = 1 # Beta prior SD\n)\n\nsim_bf\n\n******************************\nCox proportional hazards analysis\n---------------------------------\nH0:              beta == 0\nH1:              beta != 0\nNormal prior:    Mean = 0.000\n                 SD = 1.000\n\n    Median BF10 = 0.511\n    MAD SD BF10 = 1.439e-04\n******************************\n\n\nThe median BF is 0.511 falls in the “ambiguous” range revealing that there is little evidence to support the null hypothesis of no difference between the two treatments.\nThis result can be checked (approximately) by ignoring any time dependency and simply considering the outcome data in the form of a 2X2 contingency table. Here is the Notion-2 data in tabular form for the primary outcome.\n\nCode#deaths\n#TAVR 187, 19\n#SAVR 183,\ndat &lt;- matrix(c(19,168,13,170), nrow = 2, byrow = TRUE,\n              dimnames = list(c(\"TAVR\", \"SAVR\"),\n                              c(\"E+\", \"E-\")))\nkable(dat)\n\n\n\n\n\nE+\nE-\n\n\n\nTAVR\n19\n168\n\n\nSAVR\n13\n170\n\n\n\n\n\n\nThe BF for this data structure is available with the contingencyTableBF function from the BayesFactor package(Morey and Rouder 2024)\n\nCodelibrary(BayesFactor)\nbf = contingencyTableBF(dat, sampleType = \"indepMulti\", fixedMargin = \"cols\")\nbf\n\nBayes factor analysis\n--------------\n[1] Non-indep. (a=1) : 0.39 ±0%\n\nAgainst denominator:\n  Null, independence, a = 1 \n---\nBayes factor type: BFcontingencyTable, independent multinomial\n\n\nThe result is, as expected, consistent with the earlier result again underlining the lack of any strong evidence to support the null hypothesis.\nOf course, a close and proper interpretation of a standard statistical analysis provides the same inferences as shown by the binomial risk ratios and their 95% confidence intervals as shown below and published in the original manuscript.\n\nCodelibrary(epiR)\nepi.2by2(dat, method = \"cohort.count\", digits = 2, conf.level = 0.95, \n         units = 100, interpret = FALSE, outcome = \"as.columns\")\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +           19          168        187      10.16 (6.23 to 15.41)\nExposed -           13          170        183       7.10 (3.84 to 11.84)\nTotal               32          338        370       8.65 (5.99 to 11.99)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 1.43 (0.73, 2.81)\nInc odds ratio                                 1.48 (0.71, 3.09)\nAttrib risk in the exposed *                   3.06 (-2.65, 8.77)\nAttrib fraction in the exposed (%)            30.08 (-37.37, 64.42)\nAttrib risk in the population *                1.54 (-3.15, 6.24)\nAttrib fraction in the population (%)         17.86 (-22.71, 45.02)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 1.094 Pr&gt;chi2 = 0.296\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.356\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units"
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html#full-bayesian-analysis",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html#full-bayesian-analysis",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "Full Bayesian analysis",
    "text": "Full Bayesian analysis\nThe BF approach has the advantage of not requiring a prior belief on the the relative risk of two interventions. However this comes at the expense of not being able to calculate the posterior probability distribution for the risk difference or ratio.\nAs an initial approach one can assume a non-informative prior with a Beta(1,1) distribution so that the posterior distribution is completely determined by the observed Notion2 data.\n\nCodepacman::p_load(brms, tidyverse, tidybayes, ggdist)\ndata_bin &lt;- data.frame(N = c(183,187), y = c(13,19), grp2 = as.factor(c(\"Surgery\",\"TAVR\"))) \nf = bf(y | trials(N) ~ 0 + grp2)\n\n#get_prior(formula = f,data = data_bin,family = binomial(link = \"identity\"))\n\nm &lt;- brm(\n  formula = f,\n  data = data_bin,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(1, 1), class = b, lb = 0, ub = 1)), # gets rid of a bunch of unhelpful warnings\n  chains = 4, warmup = 1000, iter = 2000, seed = 123,\n  refresh = 0\n)\n\nsave(m, file = \"output/m_brms\")\n\n\nThis produces the following results for the risk differences\n\nCodeload(\"output/m_brms\")\nsummary(m)\n\n Family: binomial \n  Links: mu = identity \nFormula: y | trials(N) ~ 0 + grp2 \n   Data: data_bin (Number of observations: 2) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngrp2Surgery     0.08      0.02     0.04     0.12 1.00     3036     2467\ngrp2TAVR        0.11      0.02     0.07     0.15 1.00     3176     2066\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThese results can also be shown graphically\n\nCodedraws &lt;- brms::as_draws_df(m)\ndraws &lt;- draws %&gt;% \n  # rename and drop the unneeded columns\n  transmute(p0 = b_grp2Surgery,\n            p1 = b_grp2TAVR) %&gt;% \n  # compute the OR\n  mutate(rr = p1 / p0, diff = p1-p0 )\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n\n# function that approximates the density at the provided values\napproxdens &lt;- function(x) {\n  dens &lt;- density(x)\n  f &lt;- with(dens, approxfun(x, y))\n  f(x)\n}\n\nprobs &lt;- c(0.145, 1) # sum(draws$diff&gt;0)/4000\n\ndraws1 &lt;- draws %&gt;%\n  mutate(dy = approxdens(diff),                         # calculate density\n         p = percent_rank(diff),                        # percentile rank \n         pcat = as.factor(cut(p, breaks = probs,         # percentile category based on probs\n                              include.lowest = TRUE)))\n\nggplot(draws1, aes(diff, dy) ) +\n  geom_ribbon(aes(ymin = 0, ymax = dy, fill = pcat), alpha=.2) +\n  geom_line() +\n  scale_fill_brewer(guide = \"none\", palette = \"Set2\") +\n  labs(x = \"Risk difference (TAVR - surgery))\",\n       y = NULL) +\n  theme_classic() +\n  labs(title = \"Notion2 trial results with vague non-informative prior\", subtitle = \"Shaded area = probability increased TAVR risk (85.5%)\") \n\n\n\n\n\n\n\nThis Bayesian analysis has provided a quantitative answer to the risk differences between TAVR and surgery and highlights the uncertainty that was missing from the original published conclusion. The BF approach to hypothesis testing provides an improvement over the traditional NHST by calculating the strength of the evidence, very weak support of the null hypothesis of no difference in this case.\nOf course, rather than fixating on the p value, an examination of the 95% confidence interval (0.73, 2.81) shows that while this contains the null effect of 1, it is also compatible with a possible 27% reduction or 181% increase in risk with TAVR compared to SAVR. As these are meaningful differences, if becomes obvious that a claim of similarity in outcomes between the two procedures is not supported by the data, both from a frequentist and Bayesian viewpoints."
  },
  {
    "objectID": "posts/2024-08-15-NHST-misleads-(again)/index.html#references",
    "href": "posts/2024-08-15-NHST-misleads-(again)/index.html#references",
    "title": "TAVR vs. surgery - NHST gets it wrong (again)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/2024-08-27-NHST-misleads-(again)/index.html",
    "href": "posts/2024-08-27-NHST-misleads-(again)/index.html",
    "title": "Mortality following post operative atrial fibrillation",
    "section": "",
    "text": "A recent publication concluded“In this retrospective cohort study of 21 568 patients who underwent cardiac surgery, women were less likely to develop poAF than men when controlling for other relevant characteristics; however, women who did develop poAF had a higher risk of long-term mortality than men who developed poAF. This observed elevated risk calls for a tailored approach to perioperative care in women undergoing cardiac surgery.”(Karamnov et al. 2024) The results were as follows:“Compared with same-sex individuals without poAF, men with poAF had a 17% higher mortality hazard (hazard ratio [HR], 1.17; 95% CI, 1.11-1.25; P &lt; .001), and women with poAF had a 31% higher mortality hazard (HR, 1.31; 95% CI, 1.21-1.42; P &lt; .001).].” \nDo these results support the authors conclusions calling for “a tailored approach to perioperative care in women undergoing cardiac surgery”?"
  },
  {
    "objectID": "posts/2024-08-27-NHST-misleads-(again)/index.html#does-sex-matter",
    "href": "posts/2024-08-27-NHST-misleads-(again)/index.html#does-sex-matter",
    "title": "Mortality following post operative atrial fibrillation",
    "section": "",
    "text": "A recent publication concluded“In this retrospective cohort study of 21 568 patients who underwent cardiac surgery, women were less likely to develop poAF than men when controlling for other relevant characteristics; however, women who did develop poAF had a higher risk of long-term mortality than men who developed poAF. This observed elevated risk calls for a tailored approach to perioperative care in women undergoing cardiac surgery.”(Karamnov et al. 2024) The results were as follows:“Compared with same-sex individuals without poAF, men with poAF had a 17% higher mortality hazard (hazard ratio [HR], 1.17; 95% CI, 1.11-1.25; P &lt; .001), and women with poAF had a 31% higher mortality hazard (HR, 1.31; 95% CI, 1.21-1.42; P &lt; .001).].” \nDo these results support the authors conclusions calling for “a tailored approach to perioperative care in women undergoing cardiac surgery”?"
  },
  {
    "objectID": "posts/2024-08-27-NHST-misleads-(again)/index.html#why-might-one-be-concerned-about-this-conclusion",
    "href": "posts/2024-08-27-NHST-misleads-(again)/index.html#why-might-one-be-concerned-about-this-conclusion",
    "title": "Mortality following post operative atrial fibrillation",
    "section": "Why might one be concerned about this conclusion?",
    "text": "Why might one be concerned about this conclusion?\nA quick look at the results shows some overlap in the confidence intervals for mortality in men and women and so it is difficult to ascertain without a formal statistical interaction test whether the differences in sex specific hazard rates are beyond the play of chance. Such a test doesn’t appear to have been reported in the publication.\nAltman and Bland(Altman and Bland 2003) discuss this issue of interaction and provide a simple method of calculation.\n\nCodeinter_action &lt;- function(rr1, l1, u1, rr2, l2, u2){\n  log_rr1 &lt;- log(rr1)\n  log_l1 &lt;- log(l1)\n  log_u1 &lt;- log(u1)\n  log_rr2 &lt;- log(rr2)\n  log_l2 &lt;- log(l2)\n  log_u2 &lt;- log(u2)\n  se1 &lt;- (log_u1-log_l1)/3.92\n  se2 &lt;- (log_u2-log_l2)/3.92\n  diff_rr &lt;- log_rr1 - log_rr2\n  diff_se &lt;- sqrt(se1^2 + se2^2)\n  z &lt;- abs(diff_rr) / diff_se\n  p &lt;- 2*(1-pnorm(z))\n  ldiff &lt;- diff_rr - 1.96*diff_se\n  udiff &lt;- diff_rr +1.96*diff_se\n  RRR &lt;- exp(diff_rr)\n  lCI_RRR &lt;- exp(ldiff)\n  uCI_RRR &lt;- exp(udiff)\n  ans &lt;- paste0(\"The ratio of the relative risks is \", round(RRR,2), \", 95% CI \",\n               round(lCI_RRR,2), \" - \", round(uCI_RRR,2), \" p = \" , round(p,2))\n  list(ans)\n}\n\ninter_action(1.31,1.21,1.42,1.17,1.11,1.25)\n\n[[1]]\n[1] \"The ratio of the relative risks is 1.12, 95% CI 1.01 - 1.24 p = 0.03\"\n\n\nThe interaction test is indeed statistically positive but with the lower limit of the confience interval approaching 1, the statistical significance of this association is likely not terribly robust. For example, a very small amount of residual or unmeasured confounding could make the statistical significance vanish as shown in the following calculation.\n\nCodelibrary(EValue)\nevalues.HR(1.12,1.01,1.24,rare = 0)\n\n         point lower upper\nRR        1.08  1.01  1.16\nE-values  1.38  1.09    NA\n\n\nAlso statistical significance and p values provide no information about the strength of the evidence as the same p value can arise from a large effect size in a moderate sample or a small effect size in a very large sample."
  },
  {
    "objectID": "posts/2024-08-27-NHST-misleads-(again)/index.html#a-bayesian-approach",
    "href": "posts/2024-08-27-NHST-misleads-(again)/index.html#a-bayesian-approach",
    "title": "Mortality following post operative atrial fibrillation",
    "section": "A Bayesian approach",
    "text": "A Bayesian approach\nRather than concentrating on directional hypothesis testing, i.e. drawing inferences based on the (in)compatability of the data with a null hypothesis upon repeated sampling, a richer deeper understanding of the data can be obtained with quantitative hypothesis testing. This involves estimating the probabilities of women and men dying and then making direct comparisons with probability statements and represents a Bayesian approach to data analysis.\nLet y represent the observations and let the corresponding data generating mechanism be described by a distribution (likelihood) function \\(f(y|p)\\), where p is a parameter of interest. In Bayesian statistics, the unknown parameter of interest p is not considered fixed but instead has a distribution. Before the experiment, the prior distribution f(p) is obtained from any previous knowledge about the parameter. The more knowledge is available, the more informative will this prior distribution be.\nThe object of Bayesian inference is the posterior distribution f(p|y) after observing the data and is evaluated with Bayes’ theorem as follows:\\[f(p|y) = \\frac{f(y|p)f(p)}{\\int_{p}f(y|p)f(p)dp}\\]\nThe response variable y, death in this case, has a binomial distribution with the total number of binomial trials, n, and the probability of the outcome, p: \\[y \\sim Bin(n,p)\\] In the absence of any other information, a uniform or non-informative prior Beta(𝑎=1,𝑏=1) distribution is often assumed. When the prior is a Beta distribution and the data in the form of a binomial, the posterior distribution follows the same form as the prior, a Beta distribution, known as a congugate analysis which is expressed as\\[ p|y\\sim Beta(a+y,b+n-y)\\]\nwith the posterior mean \\[E(p|y) =\n\\frac{a+y}{a+y+b+n-y} = \\frac{a+y}{a+b+n} =\n\\frac{a+n\\bar{y}}{a+b+n}\\]\nIn this specific example, one can calaculate the probability of mortality for both men and women by combining the data from the publication (the likelihood) with our prior information (here assumed to be non-informative). Once the posterior probability functions for women and men become available, summary statistics and plots comparing them are easily obtained and interpreted.\n\nCode# deaths for men and women\nd_w &lt;- 1294           # women deaths\nn_w &lt;- 2567           # women at risk\np_w &lt;-d_w/n_w         # probability women dying\nsd_w &lt;- sqrt(n_w*p_w*(1-p_w)) # sd women dying\nd_m &lt;- 2376           # men deaths\nn_m &lt;- 4859           #men at risk\np_m &lt;- d_m/n_m       # probability men dying\nsd_m &lt;- sqrt(n_m*p_m*(1-p_m))  # sd men dying\n\n# create a data frame of 10,000 men and 10,0000 women each with their respective probabilities \ndf &lt;- data.frame(sex = factor(rep(c(\"F\", \"M\"), each=10000)),\n                   death_prop = c(rbinom(n=10000, size=n_w, prob=p_w)/n_w,\n                   rbinom(n=10000, size=n_m, prob=p_m)/n_m))\n\nq_f &lt;- quantile(df[df$sex==\"F\",]$death_prop, c(.025,.5,0.975))\nq_m &lt;- quantile(df[df$sex==\"M\",]$death_prop, c(.025,.5,0.975))\n\nggplot(df, aes(x = death_prop)) + \n   geom_density(aes(fill = sex), alpha = 0.4) +\n  labs(title = \"Mean probability of (wo)men with post-operative atrial fibrillation dying\", subtitle = \"Based on 10,000 simulations\", caption = \"Horizontal lines = 95% confidence intervals \\nDashed vertical lines = mean\") +\n  xlab(\"Mean probability dying\") +\n  geom_vline(aes(xintercept = mean(death_prop), color = sex), data = df[df$sex==\"F\",], linetype = \"dashed\") +\n  geom_vline(aes(xintercept = mean(death_prop), color = sex), data = df[df$sex==\"M\",], linetype = \"dashed\") +\n  theme_classic() +\n  geom_segment(aes(x=.485,xend=.523,y=0,yend=0), color=\"#F8766D\",linewidth = 2) +\n  geom_segment(aes(x=.475,xend=.503,y=1,yend=1), color=\"#00BFC4\", linewidth = 2)\n\n\n\n\n\n\n\nTherefore mean probability of a woman dying is 0.504 with 95% CI 0.485 to 0.524.\nWhile the mean probability of a man dying is 0.489 with 95% CI 0.475 to 0.503.\nWhat then is the probability that the mean mortality for women is greater than the mean mortality for men?\n\nCodemean_prob &lt;- sum(df[df$sex==\"F\",]$death_prop &gt; df[df$sex==\"M\",]$death_prop) / 10000\n\n\nThe probability that the mean mortailty for women is greater than the mean probability for men is 0.892.\nThis analysis suggests that while the mean mortality for women is likely greater than for men, there remains a 1 in 10 chance that the reverse is true.\nWhat is the right research (clinical) question?\nMoreover, rather than comparing means what is arguably the more important research question is whether the probability that the mortality for the next women with POAF will be greater than for the next man with POAF. This involves calculating and comparing the posterior predictive distributions for each sex .\n\nCode#To obtain the posterior predictive distribution for next woman\np_w_next &lt;- rbeta(10^4,d_w+1,n_w-d_w+1)\ny_w_next &lt;- rbinom(length(p_w_next),1000,p_w_next) / 1000\nob_f &lt;- quantile(y_w_next, c(0.025,.5,0.975))\n\np_m_next &lt;- rbeta(10^4,d_m+1,n_m-d_m+1)\ny_m_next &lt;- rbinom(length(p_m_next),1000,p_m_next) / 1000\nob_m &lt;- quantile(y_m_next, c(0.025,.5,0.975))\n\nnext_0 &lt;- sum(y_w_next &gt; y_m_next) /10000\n\nnext_01 &lt;- sum(y_w_next - y_m_next &gt; 0.01) /10000\n\n\nIn contrast to comparisons of the sex specific means, there will be increased uncertainty about the probability about the next observed woman or man dying.\nThe probability of the next observed woman dying is 0.504 with 95% CI 0.469 to 0.54.\nThe probability of the next observed man dying is 0.489 with 95% CI 0.455 to 0.523.\nThis is displayed graphically below.\n\nCodedf1 &lt;- data.frame(sex = factor(rep(c(\"F\", \"M\"), each=10000)),\n                 death_prop = c(rbinom(n=10000, size=n_w, prob=y_w_next)/n_w,\n                                rbinom(n=10000, size=n_m, prob=y_m_next)/n_m))\nggplot(df1, aes(x = death_prop)) + \n  geom_density(aes(fill = sex), alpha = 0.4) +\n  labs(title = \"Probability of the next (wo)man with post-operative atrial fibrillation dying\", \n       subtitle = \"Based on 10,000 simulations from a binomial model\", \n       caption = \"Horizontal lines = 95% confidence intervals \\nDashed vertical lines = mean\") +\n  xlab(\"Probability dying\") +\n  geom_vline(aes(xintercept = mean(death_prop), color = sex), data = df[df$sex==\"F\",], linetype = \"dashed\") +\n  geom_vline(aes(xintercept = mean(death_prop), color = sex), data = df[df$sex==\"M\",], linetype = \"dashed\") +\n  theme_classic() +\n  geom_segment(aes(x=.467,xend=.540,y=0,yend=0), color=\"#F8766D\",linewidth = 2) +\n  geom_segment(aes(x=.456,xend=.523,y=.5,yend=.5), color=\"#00BFC4\", linewidth = 2)\n\n\n\n\n\n\n\nTherefore the probability that the mortality of the next observed woman exceeds that of the next observed man is 0.722.\nFinally with these posterior predictive distributions, one can the probability that the the mortality for next observed women exceeds the male mortality by any specified amount. For example, the probability that the mortality for the next observed woman exceeds that of a man by at least 1% is barely beyond that of a coin toss, 0.59.In conclusion, this analysis suggests the strength of the evidence in favor of an increased mortality for the next woman with POAF compared to a man is only moderate at best (71% probability) and that an approach that targets both sexes seems more reasonable than targeting only women."
  },
  {
    "objectID": "posts/2024-089-15-Something old with spin (FAME-2)/index.html",
    "href": "posts/2024-089-15-Something old with spin (FAME-2)/index.html",
    "title": "Something old with spin (FAME-2)",
    "section": "",
    "text": "I was reminded this week of an old trial from 2012, FAME-2 that concluded\n&gt; “In the FAME 2 trial, we compared the treatment strategy of percutaneous coronary interventions (PCI), performed according to current quality standards, plus the best available medical therapy with the best available medical therapy alone in patients with stable coronary artery disease and hemodynamically significant stenoses. FFR-guided PCI with drug-eluting stents plus the best available medical therapy, as compared with the best available medical therapy alone, resulted in significantly improved clinical outcomes.”\nI was also reminded how annoyed I was at this very misleading conclusion. Not that it matters much but I wish I had bothered to blog about the study at the moment of its publication. OTOH, waiting has demonstrated the enormous impact an article in a high impact journal can have, even when the conclusions are quite erronous as this study has now been cited &gt; 3600 times. The study has also been very influential in pushing guidelines to endorse PCI revascularizations as a preferred treatment option for stable chronic coronary disease"
  },
  {
    "objectID": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#background",
    "href": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#background",
    "title": "Something old with spin (FAME-2)",
    "section": "",
    "text": "I was reminded this week of an old trial from 2012, FAME-2 that concluded\n&gt; “In the FAME 2 trial, we compared the treatment strategy of percutaneous coronary interventions (PCI), performed according to current quality standards, plus the best available medical therapy with the best available medical therapy alone in patients with stable coronary artery disease and hemodynamically significant stenoses. FFR-guided PCI with drug-eluting stents plus the best available medical therapy, as compared with the best available medical therapy alone, resulted in significantly improved clinical outcomes.”\nI was also reminded how annoyed I was at this very misleading conclusion. Not that it matters much but I wish I had bothered to blog about the study at the moment of its publication. OTOH, waiting has demonstrated the enormous impact an article in a high impact journal can have, even when the conclusions are quite erronous as this study has now been cited &gt; 3600 times. The study has also been very influential in pushing guidelines to endorse PCI revascularizations as a preferred treatment option for stable chronic coronary disease"
  },
  {
    "objectID": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#summary-study-design-and-results",
    "href": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#summary-study-design-and-results",
    "title": "Something old with spin (FAME-2)",
    "section": "Summary study design and results",
    "text": "Summary study design and results\nIn the FAME 2 trial, the treatment strategy of PCI, performed according to current quality standards (FFR-guided PCI with drug-eluting stents), plus the best available medical therapy was compared with the best available medical therapy alone in patients with stable coronary artery disease and hemodynamically significant stenoses.\nA total of 888 patients had at least one hemodynamically significant stenosis with an FFR of 0.80 or less in a large epicardial artery: 447 patients were randomly assigned to FFR-guided PCI plus the best available medical therapy, and 441 patients to the best available medical therapy alone. During an average 7 month followup, the primary end point, a composite of death from any cause, nonfatal myocardial infarction, or urgent revascularizations were compared between the intervention and control arms.\nThe results are as"
  },
  {
    "objectID": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#whats-my-beef",
    "href": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#whats-my-beef",
    "title": "Something old with spin (FAME-2)",
    "section": "What’s my beef?",
    "text": "What’s my beef?\nThe core issue is whether PCI can simultaneously serve as an intervention in the intervention arm and as an outcome in the control arm. Now the authors would probably argue that the PCIs in the control arm are “urgent” and therefore different. However, it is exactly the same technique and “urgent” is merely a subjective adjective that is of questionable value in an unblinded study. Any complications from the revascularization whether urgent or not is captured by the other components of the composite primary outcome.\nSo forgetting this ersatz distinction between “urgent” and routine PCI, the above Table 2 which treats all PCIs equally is modified as follows\nThe code to produce these numbers is as follows;\n\nCodemat &lt;- matrix(c(447,.5,93,348), byrow = TRUE, nrow=2,\n              dimnames = list(c(\"PCI\", \"medical\"),\n                              c(\"outcome+\", \"outcome-\")))\n\nepiR::epi.2by2(mat, method=\"cohort.count\")\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          447          0.5      447.5    99.89 (98.96 to 100.00)\nExposed -           93          348        441     21.09 (17.37 to 25.20)\nTotal              540        348.5      888.5     60.78 (57.48 to 64.00)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 4.74 (3.95, 5.67)\nInc odds ratio                                 3345.29 (206.96, 54072.84)\nAttrib risk in the exposed *                   78.80 (74.98, 82.62)\nAttrib fraction in the exposed (%)            78.89 (74.71, 82.38)\nAttrib risk in the population *                39.69 (34.71, 44.67)\nAttrib fraction in the population (%)         65.30 (59.47, 70.30)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 578.553 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\nCodemat1 &lt;- matrix(c(447,.5,86,355), byrow = TRUE, nrow=2,\n              dimnames = list(c(\"PCI\", \"medical\"),\n                              c(\"outcome+\", \"outcome-\")))\n\nepiR::epi.2by2(mat1, method=\"cohort.count\")\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          447          0.5      447.5    99.89 (98.96 to 100.00)\nExposed -           86          355        441     19.50 (15.91 to 23.51)\nTotal              533        355.5      888.5     59.99 (56.68 to 63.23)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 5.12 (4.24, 6.19)\nInc odds ratio                                 3690.35 (228.18, 59684.03)\nAttrib risk in the exposed *                   80.39 (76.68, 84.10)\nAttrib fraction in the exposed (%)            80.48 (76.40, 83.85)\nAttrib risk in the population *                40.49 (35.58, 45.39)\nAttrib fraction in the population (%)         67.49 (61.66, 72.44)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 597.992 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units"
  },
  {
    "objectID": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#conclusion",
    "href": "posts/2024-089-15-Something old with spin (FAME-2)/index.html#conclusion",
    "title": "Something old with spin (FAME-2)",
    "section": "Conclusion",
    "text": "Conclusion\nSo if one treats a revascularization as a revascularization as a revascularization regardless of group assignment, one reaches the conclusion that the PCI intervention arm is associated with 5 times more interventions with no reduction in mortality or myocardial infarction. This is in stark contrast with how this trial has been interpreted. It is very alarming that the use of one subjective adjective can lead to such diametrically opposed conclusions.\nMy final take home mesaage isIf you can spin FAME-2 results, then you can probably successfully spin any results."
  },
  {
    "objectID": "posts/2024-09-15-Something old with spin (FAME-2)/index.html",
    "href": "posts/2024-09-15-Something old with spin (FAME-2)/index.html",
    "title": "Something old with spin (FAME-2)",
    "section": "",
    "text": "I was reminded this week of an old trial from 2012, FAME-2 that concluded\n&gt; “In the FAME 2 trial, we compared the treatment strategy of percutaneous coronary interventions (PCI), performed according to current quality standards, plus the best available medical therapy with the best available medical therapy alone in patients with stable coronary artery disease and hemodynamically significant stenoses. FFR-guided PCI with drug-eluting stents plus the best available medical therapy, as compared with the best available medical therapy alone, resulted in significantly improved clinical outcomes.”\nI was also reminded how annoyed I was at this very misleading conclusion. Not that it matters much but I wish I had bothered to blog about the study at the moment of its publication. OTOH, waiting has demonstrated the enormous impact an article in a high impact journal can have, even when the conclusions are quite erronous as this study has now been cited &gt; 3600 times. The study has also been very influential in pushing guidelines to endorse PCI revascularizations as a preferred treatment option for stable chronic coronary disease"
  },
  {
    "objectID": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#background",
    "href": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#background",
    "title": "Something old with spin (FAME-2)",
    "section": "",
    "text": "I was reminded this week of an old trial from 2012, FAME-2 that concluded\n&gt; “In the FAME 2 trial, we compared the treatment strategy of percutaneous coronary interventions (PCI), performed according to current quality standards, plus the best available medical therapy with the best available medical therapy alone in patients with stable coronary artery disease and hemodynamically significant stenoses. FFR-guided PCI with drug-eluting stents plus the best available medical therapy, as compared with the best available medical therapy alone, resulted in significantly improved clinical outcomes.”\nI was also reminded how annoyed I was at this very misleading conclusion. Not that it matters much but I wish I had bothered to blog about the study at the moment of its publication. OTOH, waiting has demonstrated the enormous impact an article in a high impact journal can have, even when the conclusions are quite erronous as this study has now been cited &gt; 3600 times. The study has also been very influential in pushing guidelines to endorse PCI revascularizations as a preferred treatment option for stable chronic coronary disease"
  },
  {
    "objectID": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#summary-study-design-and-results",
    "href": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#summary-study-design-and-results",
    "title": "Something old with spin (FAME-2)",
    "section": "Summary study design and results",
    "text": "Summary study design and results\nIn the FAME 2 trial, the treatment strategy of PCI, performed according to current quality standards (FFR-guided PCI with drug-eluting stents), plus the best available medical therapy was compared with the best available medical therapy alone in patients with stable coronary artery disease and hemodynamically significant stenoses.\nA total of 888 patients had at least one hemodynamically significant stenosis with an FFR of 0.80 or less in a large epicardial artery: 447 patients were randomly assigned to FFR-guided PCI plus the best available medical therapy, and 441 patients to the best available medical therapy alone. During an average 7 month followup, the primary end point, a composite of death from any cause, nonfatal myocardial infarction, or urgent revascularizations were compared between the intervention and control arms.\nThe results are as"
  },
  {
    "objectID": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#whats-my-beef",
    "href": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#whats-my-beef",
    "title": "Something old with spin (FAME-2)",
    "section": "What’s my beef?",
    "text": "What’s my beef?\nThe core issue is whether PCI can simultaneously serve as an intervention in the intervention arm and as an outcome in the control arm. Now the authors would probably argue that the PCIs in the control arm are “urgent” and therefore different. However, it is exactly the same technique and “urgent” is merely a subjective adjective that is of questionable value in an unblinded study. Any complications from the revascularization whether urgent or not is captured by the other components of the composite primary outcome.\nSo forgetting this ersatz distinction between “urgent” and routine PCI, the above Table 2 which treats all PCIs equally is modified as follows\nThe code to produce these numbers is as follows;\n\nCodemat &lt;- matrix(c(447,.5,93,348), byrow = TRUE, nrow=2,\n              dimnames = list(c(\"PCI\", \"medical\"),\n                              c(\"outcome+\", \"outcome-\")))\n\nepiR::epi.2by2(mat, method=\"cohort.count\")\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          447          0.5      447.5    99.89 (98.96 to 100.00)\nExposed -           93          348        441     21.09 (17.37 to 25.20)\nTotal              540        348.5      888.5     60.78 (57.48 to 64.00)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 4.74 (3.95, 5.67)\nInc odds ratio                                 3345.29 (206.96, 54072.84)\nAttrib risk in the exposed *                   78.80 (74.98, 82.62)\nAttrib fraction in the exposed (%)            78.89 (74.71, 82.38)\nAttrib risk in the population *                39.69 (34.71, 44.67)\nAttrib fraction in the population (%)         65.30 (59.47, 70.30)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 578.553 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\nCodemat1 &lt;- matrix(c(447,.5,86,355), byrow = TRUE, nrow=2,\n              dimnames = list(c(\"PCI\", \"medical\"),\n                              c(\"outcome+\", \"outcome-\")))\n\nepiR::epi.2by2(mat1, method=\"cohort.count\")\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          447          0.5      447.5    99.89 (98.96 to 100.00)\nExposed -           86          355        441     19.50 (15.91 to 23.51)\nTotal              533        355.5      888.5     59.99 (56.68 to 63.23)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 5.12 (4.24, 6.19)\nInc odds ratio                                 3690.35 (228.18, 59684.03)\nAttrib risk in the exposed *                   80.39 (76.68, 84.10)\nAttrib fraction in the exposed (%)            80.48 (76.40, 83.85)\nAttrib risk in the population *                40.49 (35.58, 45.39)\nAttrib fraction in the population (%)         67.49 (61.66, 72.44)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 597.992 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units"
  },
  {
    "objectID": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#conclusion",
    "href": "posts/2024-09-15-Something old with spin (FAME-2)/index.html#conclusion",
    "title": "Something old with spin (FAME-2)",
    "section": "Conclusion",
    "text": "Conclusion\nSo if one treats a revascularization as a revascularization as a revascularization regardless of group assignment, one reaches the conclusion that the PCI intervention arm is associated with 5 times more interventions with no reduction in mortality or myocardial infarction. This is in stark contrast with how this trial has been interpreted. It is very alarming that the use of one subjective adjective can lead to such diametrically opposed conclusions.\nMy final take home mesaage isIf you can spin FAME-2 results, then you can probably successfully spin any results."
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html",
    "title": "Estimating Uncertainty",
    "section": "",
    "text": "I recently read an article entitled “One-Year Outcomes of Transseptal Mitral Valvein- Valve in Intermediate Surgical Risk Patients”(Malaisrie et al. 2024) paper. After examining 50 patients from 12 sites that underwent mitral valve-in-valve from 2018 to 2021 the authors concluded\n\n“Mitral valve-in-valve with a balloon-expandable valve via transseptal approach in intermediate-risk patients was associated with improved symptoms and quality of life, adequate transcatheter valve performance, and no mortality or stroke at 1-year follow-up.”\n\nThis prompted me to wonder how probable would it be to observe 0 deaths in the 50 patients at 1 year if the annual transcather mitral valve mortality was actually the same as their expected value with a surgical redo, 4%. I also wondered how well my colleagues would estimate this uncertainty (probability) and therefore created and circulated the following short questionnaire."
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html#background",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html#background",
    "title": "Estimating Uncertainty",
    "section": "",
    "text": "I recently read an article entitled “One-Year Outcomes of Transseptal Mitral Valvein- Valve in Intermediate Surgical Risk Patients”(Malaisrie et al. 2024) paper. After examining 50 patients from 12 sites that underwent mitral valve-in-valve from 2018 to 2021 the authors concluded\n\n“Mitral valve-in-valve with a balloon-expandable valve via transseptal approach in intermediate-risk patients was associated with improved symptoms and quality of life, adequate transcatheter valve performance, and no mortality or stroke at 1-year follow-up.”\n\nThis prompted me to wonder how probable would it be to observe 0 deaths in the 50 patients at 1 year if the annual transcather mitral valve mortality was actually the same as their expected value with a surgical redo, 4%. I also wondered how well my colleagues would estimate this uncertainty (probability) and therefore created and circulated the following short questionnaire."
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html#quiz",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html#quiz",
    "title": "Estimating Uncertainty",
    "section": "Quiz",
    "text": "Quiz\nA recent study reported no mortality in 50 patients undergoing transcatheter mitral valve-in-valve replacement (MVIV) for patients with bioprosthetic valve failure. The STS mortality score for these patients undergoing a standard redo operation is an estimated 4%.\n\nQ1. If mortality rates with transcatheter MVIV are assumed the same as with a standard redo, what is the probability of observing 0 deaths as these authors did?\n\n\n&lt;1%\n\n1 - 4.9%\n\n5 -9.9%\n\n10 - 14.9%\n\n&gt; 15%\n\n\nQ2. If mortality rates with transcatheter MVIV were actually 40% higher those with a standard redo, what is the probability of observing 0 deaths as these authors did?\n\n\n&lt;1%\n\n1 - 4.9%\n\n5 -9.9%\n\n10 - 14.9%\n\n&gt; 15%\n\n\nQ3. Given the above information (and that there is probably not a lot of other of good quality available evidence), what is your probability (belief) that MVIV is as safe or safer than a standard redo operation in this population?\n\n\n&lt; 25 %\n\n25 - 50%\n\n51 - 75%\n\n76 - 95%\n\n&gt; 95%"
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html#basic-probability-distributions",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html#basic-probability-distributions",
    "title": "Estimating Uncertainty",
    "section": "Basic Probability Distributions",
    "text": "Basic Probability Distributions\nAnswering these questions, requires making some assumptions about possible models, in other words underlying probability distributions, that could represent the data. Here are some of the more common probability distributions and their utility according to the available data.\n\n\n\n\nNormal distribution\nNormal distributions are ubiquitous and generally reasonably well understood. They are important in medicine, at least partly due to the central limit theorem that states under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal. Mathematically the normal distribution is expressed as \\[f(x) =  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\\]\nwhere\\({\\mu}\\) is the mean or expectation of the distribution (and also its median and mode)\n\\({\\sigma^{2}}\\) is the variance and\n\\({\\sigma }\\) is the standard deviation of the distribution\nGraphically this may be plotted as shown\n\nCoden_dis = tibble(dist = c(dist_normal(0, 1)))\n\nn_dis %&gt;%\n  ggplot(aes(y = format(dist), xdist = dist)) +\n  stat_halfeye(aes(fill = after_stat(abs(x) &lt; 1.96))) +\n  labs(ggtitle=\"Normal (0,1) distribution\", subtitle=\"Blue = 95% area\", caption = \"thick horizontal black line represents +/- 1 sd (68%) \\nthin horizontal black line represents +/- 2 sd (95%)\") +\n  scale_fill_manual(values = c(\"gray85\", \"skyblue\")) + \n  scale_y_discrete(expand = c(0, 0, 0, 1)) +\n  ylab(\"\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nStudent’s t distribution\nThe Student’s t distribution is another continuous probability distribution that generalizes the standard normal distribution, remaining symmetric around zero and bell-shaped but with heavier tails. The amount of probability mass in the tails is controlled by the parameter \\({\\nu}\\). For \\({\\nu = 1}\\) the Student’s t distribution becomes the standard Cauchy distribution, which has very “fat” tails; whereas for \\({\\nu = \\infty}\\) it becomes the standard normal distribution\n\nCoden_t_dis = tibble(\n  dist = c(dist_normal(0, 1), dist_student_t(3, 0, 1))\n)\n\nn_t_dis %&gt;%\n  ggplot(aes(y = format(dist), xdist = dist)) +\n  stat_halfeye(aes(fill = after_stat(abs(x) &lt; 1.96))) +\n  labs(title=\"Normal and t distribution\", subtitle = \"t distribution with 3 degrees of freedon\", caption = \"thick horizontal black line represents +/- 1 sd (68%) \\nthin horizontal black line represents +/- 2 sd (95%)\") +\n  scale_fill_manual(values = c(\"gray85\", \"skyblue\")) + \n  xlim(-5,5) +\n  scale_y_discrete(expand = c(0, 0, 0, 1)) +\n  ylab(\"\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nUniform distribution\nThe uniform distribution assumes that all continuous outcomes between boundaries have an equal probability of occurring.\nGraphically,\n\nCode# Create a density plot using ggplot2\nggplot(data.frame(Values = runif(10000, min = 0, max = 1)), aes(x = Values)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +  \n  labs(title = \"Density Plot of Uniform Distribution (0,1)\", x = \"Values\", y = \"Density\") +\n  theme_minimal()  \n\n\n\n\n\n\n\nBinomial Distribution\nHowever for this quiz, the data are not continuous but discrete with counts and therefore the above distributions would be inappropriate models (cf negative counts being impossible). A binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, where each trial has the same probability of success, denoted by p. The distribution is characterized by two parameters: n the number of trials, and p, the probability of success in each trial.\nMathematically, this can be expressed as\\[P(X = k) = {n \\choose x} * p^k * (1 - p)^{n-k}\\] • P(X=k) is the probability of having k successes in n trials\n• (nCk) is the binomial coefficient, also known as “n choose k”, which represents the number of ways to choose k successes from n trials\n• p is the probability of success in each trial\n• 1-p is the probability of failure in each trial\n• n is the total number of trials\n• k is the number of successes\nBinomial distributions are commonly used to model situations such as coin flips, where there are two possible outcomes (success or failure) and each trial is independent of the others. They have obviously applications in various fields, including medicine.\nPoisson Distribution\nThe Poisson distribution is a discrete probability distribution that represents the number of events occurring in a fixed interval of time or space, given that these events occur with a constant rate and independently of the time since the last event. It is named after the French mathematician Siméon Denis Poisson.\nMathematically this can be expressed as\n\\[P(X = k) = \\frac{\\lambda^k * e^{-\\lambda}}{k!}\\] • P(X=k) is the probability of observing k events in a given period of time\n• \\(\\lambda\\) is the expectation of the events over the same period of time\nFor a Poisson random variable, the variance = mean = E(Y) = $ The Poisson distribution is a special case of the binomial, with trials n -&gt; \\(\\infty\\) and p (any trial success) -&gt; 0\nIf p is small, binomial P(k successes) \\(\\approx\\) poisson P(k with \\(\\lambda\\) = np)\nThe Poisson distribution is perhaps better appreciated by plotting the above equation\n\nCode# Build Poisson distributions\noptions(digits = 3)\np_dat &lt;- map_df(1:4, ~ tibble(\n  l = paste(.),\n  x = 0:10,\n  y = dpois(0:10, .)\n))\n# Use ggplot2 to plot\nggplot(p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  geom_line() +\n  geom_point(data = p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  labs(color = \"Lambda:\") +\n  ggtitle(\"Poisson distribution for varying rates (Lambda)\") +\n  scale_x_continuous(\"no. of events\",breaks=seq(0,10, 1)) +\n  ylab(\"probability\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html#quiz-results",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html#quiz-results",
    "title": "Estimating Uncertainty",
    "section": "Quiz Results",
    "text": "Quiz Results\nI received replies from 15 (42%, n= 36) MUHC cardiology staff and 4 (18%, n = 22) fellows with the following results\n\nCodedat1 &lt;- read.csv(\"results1.csv\", header = T) |&gt; \n  pivot_longer(cols=c('q1', 'q2'),\n               names_to='question',\n               values_to='value') |&gt; \n  mutate(id=factor(id))\n\npd &lt;- position_dodge(0.2)\ndat1 %&gt;% \n  mutate(value = fct_relevel(value, \n                            \"&lt;1%\", \"1-4.9%\", \"5-9.9%\", \n                            \"10-14.9%\", \"&gt;15%\")) |&gt; \n  ggplot(aes(x = question, y = value, group = id, color=id)) +\n  geom_point(position = pd) +\n  geom_line(position = pd, linewidth = 1/3, alpha = 1/2) +\n  guides(color = \"none\") +\n  facet_wrap(~ q4, labeller = label_both, axes = \"all_y\") +\n  xlab(\"Questions 1 & 2\") +\n  ylab(\"Prabability intervals\") +\n  ggtitle(\"Responses for Questions 1 & 2\") +\n  theme_ggdist()\n\n\n\n\n\n\n\n\nCodedat2 &lt;- read.csv(\"results1.csv\", header = T) |&gt; \n  mutate(id=factor(id), q3 = factor(q3))\ndat2 %&gt;% \n  mutate(value = fct_relevel(q3, \n                            \"&lt; 25%\", \"25-50%\", \"51-75%\", \n                            \"76-95%\")) |&gt; \n  ggplot(aes(x = q3, group = id, color=id)) +\n  geom_bar(stat=\"count\", width=0.7, fill=\"lightblue\")+\n  guides(color = FALSE) +\n  facet_wrap(~ q4, labeller = label_both, axes = \"all_y\") +\n  xlab(\"Questions 3\") +\n  ylab(\"Counts\") +\n  labs(title = \"Responses for Questions 3\", subtitle = \"Probability MVIV is as safe or safer than standard redo OR\") +\n  theme_ggdist()\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4."
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html#quiz-answers",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html#quiz-answers",
    "title": "Estimating Uncertainty",
    "section": "Quiz Answers",
    "text": "Quiz Answers\nTo appreciate the uncertainty associated with observing 0 deaths in 50 patients if the expected rate was 4%, one can use either a Poisson distribution (counts) or a binomial distribution (independent Bernouilli trials) where each of the 50 subjects is considered as alive or dead with a probability of 4%. These calculations can be done with the above equations or more easily with any software that includes Poisson or Binomial distributions.\nQ1. If mortality rates with transcatheter MVIV are assumed the same as with a standard redo, what is the probability of observing 0 deaths asthese authors did?\n\nCodeoptions(digits=1)\n# calculate the probability of 0 to 6 deaths among 50 patients with an expected rate of 4%\n# binomial distribution\ndb &lt;- 100*dbinom(c(0:8),50, .04) # expected 4% probability of death\n\n# Poisson distribution\ndp &lt;- 100*dpois(c(0:8),2) # number of expected deaths among the 50 subjects over 1 year\n# cat(\"Assuming a binomial distribution with an event (death) \n# probability of 4%, the probability for 0,1,2,3,4,5,6 events is  \", # db[1],\"%,\", db[2],\"%,\", db[3],\"%,\", db[4],\"%,\", db[5],\"%,\", # db[6],\"%, respectively\")\n#cat(\"\\nAssuming a Poisson distribution with an event (death) rate of # 2 (2 deaths / 50 individual in 1 year, 4% expected mortality), the # probability for 0,1,2,3,4,5,6 events is \", dp[1],\"%,\", dp[2],\"%,\", # dp[3],\"%,\", dp[4],\"%,\", dp[5],\"%,\", dp[6],\"%, respectively\")\n\n\nAssuming a binomial distribution with an event (death) probability of 4%, the probability for 0, 1, 2, 3, 4, 5, 6 events is 13%, 27.1%, 27.6%, 18.4%, 9%, 3.5%, respectively. Assuming a Poisson distribution with an event (death) rate of 2 (# deaths in 50 individual in 1 year with 4% expected mortality), the probability for 0, 1, 2, 3, 4, 5, 6 events is 13.5%, 27.1%, 27.1%, 18%, 9%, 3.6%, respectively.\nThis data can also be visualized as shown here\n\nCode# Build Poisson distributions\noptions(digits = 3)\np_dat &lt;- map_df(2, ~ tibble(\n  l = paste(.),\n  x = 0:10,\n  y = dpois(0:10, .)\n))\n# Use ggplot2 to plot\nggplot(p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  geom_line() +\n  geom_point(data = p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  labs(color = \"Lambda:\") +\n  ggtitle(\"Poisson distribution for death \\nrate of 2 / 50 subjects per year\") +\n  scale_x_continuous(\"no. of events\",breaks=seq(0,10, 1)) +\n  ylab(\"probability\") +\n  theme_minimal() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nThe similarity between the Poisson and Binomial distributions is shown graphically below\n\nCodeoptions(digits=2)\ndf &lt;- data.frame(y=round(rbinom(50000,50,.04)))\n\ng = ggplot(df, aes(x=y)) \n# g + geom_histogram(binwidth=1, fill=\"grey60\", color=\"red\") # counts\n# g + geom_histogram(aes(y=after_stat(density)), binwidth=1, fill=\"grey60\", color=\"red\") # density\n\ng + geom_histogram(aes(y=after_stat(density)), binwidth=1, # density\n                   fill=\"grey60\", color=\"red\") +\n  geom_density(adjust=5, color=\"red\", size=1.1) +\n  xlab(\"counts\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1)) +\n  ggtitle(\"Histogram (and kernal density) of count from a \n  \\nBinomial (50, 0.04) probability distribution\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nCode# same code for Poisson\n# df1 &lt;- data.frame(y=round(rpois(50000,2)))\n\n\nTherefore the answer to Q1 is 10 - 14.9%\n\nQ2. If mortality rates with transcatheter MVIV were actually 40% higher those with a standard redo, what is the probability of observing 0 deaths as these authors did?\n\nIn other words what is the probability of observing 0 deaths if the expected rate was 5.6% (4*1.4).\n\nCodeoptions(digits=1)\n# calculate the probability of 0 to 6 deaths among 50 patients with an expected rate of 4%\n# binomial distribution\ndb1 &lt;- 100*dbinom(c(0:8),50, .056) # expected 4% probability of death\n\n# Poisson distribution\ndp1 &lt;- 100*dpois(c(0:8),2.8) # number of expected deaths among the 50 subjects over 1 year\n\n\nAssuming a binomial distribution with an event (death) probability of 5.6%, the probability for 0, 1, 2, 3, 4, 5, 6 events is 5.6%, 16.6%, 24.2%, 22.9%, 16%, 8.7%, respectively. Assuming a Poisson distribution with an event (death) rate of 2.8 (# deaths in 50 individual in 1 year with 5.6% expected mortality), the probability for 0, 1, 2, 3, 4, 5, 6 events is 6.1%, 17%, 23.8%, 22.2%, 15.6%, 8.7%, respectively.\nThis may be easier to appreciate visually\n\nCodep_dat &lt;- map_df(c(2,2.8), ~ tibble(\n  l = paste(.),\n  x = 0:10,\n  y = dpois(0:10, .)\n))\n# Use ggplot2 to plot\nggplot(p_dat, aes(x, y, color = factor(l, levels = c(2,2.8)))) +\n  geom_line() +\n  geom_point(data = p_dat, aes(x, y, color = factor(l, levels = c(2,2.8)))) +\n  labs(color = \"Rates\") +\n  ggtitle(\"Poisson distribution for varying rates\") +\n  scale_x_continuous(breaks = c(1:10), limits = c(0, 10)) +\n  theme_minimal()\n\n\n\n\n\n\n\nClearly, if the expected death rate is higher the curves shift left meaning the probability of observing 0 deaths will fall with increasing mortality rates, soThe answer to Q2 is 5 - 9.9%\n\nQ3. Given the above information (and that there is probably not a lot of other of good quality available evidence), what is your probability (belief) that MVIV is as safe or safer than a standard redo operation in this population?\n\nIideally one would like to know the outcome with both interventions in the same patient but such counterfactuals unfortunately don’t exist. As an alternative we perform randomized trials, where we hope that subjects in the two treatment arms are exchangeable. In this case, we lack a proper comparative arm but can assume that if the STS model is accurate and that the MVIV patients would have had a 4% mortality if instead of MVIV they had had a surgical redo. Therefore our data are the 50 observed MVIV results and 50 results under a redo operation with a 4% mortality rate.\n\nCode# create data frame\ndat1 &lt;- tibble(Trial = c(\"study\", \"study\"), Tx = c(\"redo\", \"MVIV\"),\n               fail = c(2, 0),\n               success = c(48,50)) %&gt;% \n  mutate(total = fail + success,\n         prop_success = success / total) \n\nfit4 &lt;- brm(success | trials(total) ~ Tx, data = dat1, iter = 10000,\n            family = binomial(link=\"identity\"), refresh=0,\n            control = list(adapt_delta = .999),\n            backend = \"cmdstanr\", seed = 123,\n            file = \"fit4.RDS\") #aggregate data\n\n\n\nCodefit4 &lt;- readRDS(\"fit4.RDS\")\nsummary(fit4)\n\nWarning: There were 15936 divergent transitions after warmup. Increasing\nadapt_delta above 0.999 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: binomial \n  Links: mu = identity \nFormula: success | trials(total) ~ Tx \n   Data: dat1 (Number of observations: 2) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.98      0.02     0.93     1.00 1.00     1229     2480\nTxredo       -0.04      0.04    -0.13     0.03 1.01      703      521\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs expected the mean mortality difference (MVIV - redo) is - 4% with 95% credible intervals -13% -to 3% This probability density for this mortality difference is shown graphically below\n\nCodedraws &lt;- as_draws_df(fit4)\np_gt_0 &lt;- sum(draws$b_Txredo &gt;0) / 20000\ndraws %&gt;% \n  ggplot(aes(x = b_Txredo, fill = stat(x &gt; 0))) +\n  stat_halfeye(point_interval = mean_qi, .width = .95) +\n  scale_fill_manual(values = c(\"gray85\", \"skyblue\")) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"Mortality differece (Redo - MVIV)\") +\n  labs(title = \"Mortality difference (surgical redo - catheter MVIV)\",\n       subtitle = \"Blue area = probability increased mortality with MVIV (23.5%)\") +\n  theme_ggdist() +\n  theme(legend.position = \"none\")\n\nWarning: `stat(x &gt; 0)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x &gt; 0)` instead.\n\n\n\n\n\n\n\n\nThis figure shows that there is a 11.7% probability that MVIV patients in this study would have a worse outcome with a surgical redo, provided their underlying mortality has been well predicted with the STS model. Alternatively, the probability that MVIV is as safe or safer than a redo is 88.3%.Therefore the answer to Q3 is 76-95%"
  },
  {
    "objectID": "posts/2024-09-30-Reflections on uncertainties/index.html#discussion",
    "href": "posts/2024-09-30-Reflections on uncertainties/index.html#discussion",
    "title": "Estimating Uncertainty",
    "section": "Discussion",
    "text": "Discussion\nFor Q1, only 3 people (16%) correctly estimated that there was a 10-14.9% probability of observing 0 deaths in 50 patients, if the true underlying mortality rate was 4%. Of the 16 respondents who incorrectly estimated the probability, most (13 of 16) underestimated the probability of observing 0 deaths. For Q2, most respondents (13/19, 68%) correctly reasoned that if the true underlying mortality increased, it would be more unlikely to observe 0 deaths. However 6 individuals didn’t appreciate this and incorrectly predicted no change or an increase in the probability of observing 0 deaths if the true underlying rate was higher. For Q3, there are many assumptions required and estimates are likely fairly unreliable. Nevertheless, the question asked to ignore the potential biases and to consider only the data in this one study. Only 2 (10.5%) of individuals correctly estimated the probability of MVIV being as safe or safer than a redo being in the 76-95% interval. 12 of 19 (63%) of respondents estimated this probability at being under 50%.\nDrawing meaningful conclusions is obviously limited by the limited number of respondents, especially among the fellows. It is unknown if respondents have better, worse or same quantitative skills as the non-respondents. Notwithstanding this limitation, it would appear from the wide range of responses and the limited number of “correct” answers that additional quantitative training may be helpful."
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html",
    "title": "Estimating Uncertainties",
    "section": "",
    "text": "I recently read an article entitled “One-Year Outcomes of Transseptal Mitral Valve in Valve in Intermediate Surgical Risk Patients”(Malaisrie et al. 2024) paper. After examining 50 patients from 12 sites that underwent mitral valve-in-valve from 2018 to 2021 the authors concluded\n\n“Mitral valve-in-valve with a balloon-expandable valve via transseptal approach in intermediate-risk patients was associated with improved symptoms and quality of life, adequate transcatheter valve performance, and no mortality or stroke at 1-year follow-up.”\n\nThis prompted me to wonder how probable would it be to observe 0 deaths in the 50 patients at 1 year if the annual transcather mitral valve in valve (MVIV) mortality was actually the same as their expected value with a surgical redo, 4%. I also wondered how well my colleagues would estimate this uncertainty (probability) and therefore created and circulated the following short questionnaire."
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html#background",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html#background",
    "title": "Estimating Uncertainties",
    "section": "",
    "text": "I recently read an article entitled “One-Year Outcomes of Transseptal Mitral Valve in Valve in Intermediate Surgical Risk Patients”(Malaisrie et al. 2024) paper. After examining 50 patients from 12 sites that underwent mitral valve-in-valve from 2018 to 2021 the authors concluded\n\n“Mitral valve-in-valve with a balloon-expandable valve via transseptal approach in intermediate-risk patients was associated with improved symptoms and quality of life, adequate transcatheter valve performance, and no mortality or stroke at 1-year follow-up.”\n\nThis prompted me to wonder how probable would it be to observe 0 deaths in the 50 patients at 1 year if the annual transcather mitral valve in valve (MVIV) mortality was actually the same as their expected value with a surgical redo, 4%. I also wondered how well my colleagues would estimate this uncertainty (probability) and therefore created and circulated the following short questionnaire."
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html#quiz",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html#quiz",
    "title": "Estimating Uncertainties",
    "section": "Quiz",
    "text": "Quiz\nA recent study reported no mortality in 50 patients undergoing transcatheter mitral valve-in-valve replacement (MVIV) for patients with bioprosthetic valve failure. The STS mortality score for these patients undergoing a standard redo operation is an estimated 4%.\n\nQ1. If mortality rates with transcatheter MVIV are assumed the same as with a standard redo, what is the probability of observing 0 deaths as these authors did?\n\n\n&lt;1%\n\n1 - 4.9%\n\n5 -9.9%\n\n10 - 14.9%\n\n&gt; 15%\n\n\nQ2. If mortality rates with transcatheter MVIV were actually 40% higher those with a standard redo, what is the probability of observing 0 deaths as these authors did?\n\n\n&lt;1%\n\n1 - 4.9%\n\n5 -9.9%\n\n10 - 14.9%\n\n&gt; 15%\n\n\nQ3. Given the above information (and that there is probably not a lot of other of good quality available evidence), what is your probability (belief) that MVIV is as safe or safer than a standard redo operation in this population?\n\n\n&lt; 25 %\n\n25 - 50%\n\n51 - 75%\n\n76 - 95%\n\n&gt; 95%"
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html#basic-probability-distributions",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html#basic-probability-distributions",
    "title": "Estimating Uncertainties",
    "section": "Basic Probability Distributions",
    "text": "Basic Probability Distributions\nAnswering these questions, requires making some assumptions about possible models, in other words underlying probability distributions, that could represent the data. Here are some of the more common probability distributions and their utility according to the available data.\n\n\n\n\nNormal distribution\nNormal distributions are ubiquitous and generally reasonably well understood. They are important in medicine, at least partly due to the central limit theorem that states under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal. Mathematically the normal distribution is expressed as \\[f(x) =  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\\]\nwhere\\({\\mu}\\) is the mean or expectation of the distribution (and also its median and mode)\n\\({\\sigma^{2}}\\) is the variance and\n\\({\\sigma }\\) is the standard deviation of the distribution\nGraphically this may be plotted as shown\n\nCoden_dis = tibble(dist = c(dist_normal(0, 1)))\n\nn_dis %&gt;%\n  ggplot(aes(y = format(dist), xdist = dist)) +\n  stat_halfeye(aes(fill = after_stat(abs(x) &lt; 1.96))) +\n  labs(ggtitle=\"Normal (0,1) distribution\", subtitle=\"Blue = 95% area\", caption = \"thick horizontal black line represents +/- 1 sd (68%) \\nthin horizontal black line represents +/- 2 sd (95%)\") +\n  scale_fill_manual(values = c(\"gray85\", \"skyblue\")) + \n  scale_y_discrete(expand = c(0, 0, 0, 1)) +\n  ylab(\"\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nStudent’s t distribution\nThe Student’s t distribution is another continuous probability distribution that generalizes the standard normal distribution, remaining symmetric around zero and bell-shaped but with heavier tails. The amount of probability mass in the tails is controlled by the parameter \\({\\nu}\\). For \\({\\nu = 1}\\) the Student’s t distribution becomes the standard Cauchy distribution, which has very “fat” tails; whereas for \\({\\nu = \\infty}\\) it becomes the standard normal distribution\n\nCoden_t_dis = tibble(\n  dist = c(dist_normal(0, 1), dist_student_t(3, 0, 1))\n)\n\nn_t_dis %&gt;%\n  ggplot(aes(y = format(dist), xdist = dist)) +\n  stat_halfeye(aes(fill = after_stat(abs(x) &lt; 1.96))) +\n  labs(title=\"Normal and t distribution\", subtitle = \"t distribution with 3 degrees of freedon\", caption = \"thick horizontal black line represents +/- 1 sd (68%) \\nthin horizontal black line represents +/- 2 sd (95%)\") +\n  scale_fill_manual(values = c(\"gray85\", \"skyblue\")) + \n  xlim(-5,5) +\n  scale_y_discrete(expand = c(0, 0, 0, 1)) +\n  ylab(\"\") +\n  theme_bw() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nUniform distribution\nThe uniform distribution assumes that all continuous outcomes between boundaries have an equal probability of occurring.\nGraphically,\n\nCode# Create a density plot using ggplot2\nggplot(data.frame(Values = runif(10000, min = 0, max = 1)), aes(x = Values)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +  \n  labs(title = \"Density Plot of Uniform Distribution (0,1)\", x = \"Values\", y = \"Density\") +\n  theme_minimal()  \n\n\n\n\n\n\n\nBinomial Distribution\nHowever for this quiz, the data are not continuous but discrete with counts and therefore the above distributions would be inappropriate models (cf negative counts being impossible). A binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials, where each trial has the same probability of success, denoted by p. The distribution is characterized by two parameters: n the number of trials, and p, the probability of success in each trial.\nMathematically, this can be expressed as\\[P(X = k) = {n \\choose x} * p^k * (1 - p)^{n-k}\\] • P(X=k) is the probability of having k successes in n trials\n• (nCk) is the binomial coefficient, also known as “n choose k”, which represents the number of ways to choose k successes from n trials\n• p is the probability of success in each trial\n• 1-p is the probability of failure in each trial\n• n is the total number of trials\n• k is the number of successes\nBinomial distributions are commonly used to model situations such as coin flips, where there are two possible outcomes (success or failure) and each trial is independent of the others. They have obviously applications in various fields, including medicine.\nPoisson Distribution\nThe Poisson distribution is a discrete probability distribution that represents the number of events occurring in a fixed interval of time or space, given that these events occur with a constant rate and independently of the time since the last event. It is named after the French mathematician Siméon Denis Poisson.\nMathematically this can be expressed as\n\\[P(X = k) = \\frac{\\lambda^k * e^{-\\lambda}}{k!}\\] • P(X=k) is the probability of observing k events in a given period of time\n• \\(\\lambda\\) is the expectation of the events over the same period of time\nFor a Poisson random variable, the variance = mean = E(Y) = $ The Poisson distribution is a special case of the binomial, with trials n -&gt; \\(\\infty\\) and p (any trial success) -&gt; 0\nIf p is small, binomial P(k successes) \\(\\approx\\) poisson P(k with \\(\\lambda\\) = np)\nThe Poisson distribution is perhaps better appreciated by plotting the above equation\n\nCode# Build Poisson distributions\noptions(digits = 3)\np_dat &lt;- map_df(1:4, ~ tibble(\n  l = paste(.),\n  x = 0:10,\n  y = dpois(0:10, .)\n))\n# Use ggplot2 to plot\nggplot(p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  geom_line() +\n  geom_point(data = p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  labs(color = \"Lambda:\") +\n  ggtitle(\"Poisson distribution for varying rates (Lambda)\") +\n  scale_x_continuous(\"no. of events\",breaks=seq(0,10, 1)) +\n  ylab(\"probability\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html#quiz-results",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html#quiz-results",
    "title": "Estimating Uncertainties",
    "section": "Quiz Results",
    "text": "Quiz Results\nI received replies from 15 (42%, n= 36) MUHC cardiology staff and 4 (18%, n = 22) fellows with the following results\n\nCodedat1 &lt;- read.csv(\"results1.csv\", header = T) |&gt; \n  pivot_longer(cols=c('q1', 'q2'),\n               names_to='question',\n               values_to='value') |&gt; \n  mutate(id=factor(id))\n\npd &lt;- position_dodge(0.2)\ndat1 %&gt;% \n  mutate(value = fct_relevel(value, \n                            \"&lt;1%\", \"1-4.9%\", \"5-9.9%\", \n                            \"10-14.9%\", \"&gt;15%\")) |&gt; \n  ggplot(aes(x = question, y = value, group = id, color=id)) +\n  geom_point(position = pd) +\n  geom_line(position = pd, linewidth = 1/3, alpha = 1/2) +\n  guides(color = \"none\") +\n  facet_wrap(~ q4, labeller = label_both, axes = \"all_y\") +\n  xlab(\"Questions 1 & 2\") +\n  ylab(\"Prabability intervals\") +\n  ggtitle(\"Responses for Questions 1 & 2\") +\n  theme_ggdist()\n\n\n\n\n\n\n\n\nCodedat2 &lt;- read.csv(\"results1.csv\", header = T) |&gt; \n  mutate(id=factor(id), q3 = factor(q3))\ndat2 %&gt;% \n  mutate(value = fct_relevel(q3, \n                            \"&lt; 25%\", \"25-50%\", \"51-75%\", \n                            \"76-95%\")) |&gt; \n  ggplot(aes(x = q3, group = id, color=id)) +\n  geom_bar(stat=\"count\", width=0.7, fill=\"lightblue\")+\n  guides(color = \"none\") +\n  facet_wrap(~ q4, labeller = label_both, axes = \"all_y\") +\n  xlab(\"Questions 3\") +\n  ylab(\"Counts\") +\n  labs(title = \"Responses for Questions 3\", subtitle = \"Probability MVIV is as safe or safer than standard redo OR\") +\n  theme_ggdist()"
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html#quiz-answers",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html#quiz-answers",
    "title": "Estimating Uncertainties",
    "section": "Quiz Answers",
    "text": "Quiz Answers\nTo appreciate the uncertainty associated with observing 0 deaths in 50 patients if the expected rate was 4%, one can use either a Poisson distribution (counts) or a binomial distribution (independent Bernouilli trials) where each of the 50 subjects is considered as alive or dead with a probability of 4%. These calculations can be done with the above equations or more easily with any software that includes Poisson or Binomial distributions.\nQ1. If mortality rates with transcatheter MVIV are assumed the same as with a standard redo, what is the probability of observing 0 deaths asthese authors did?\n\nCodeoptions(digits=1)\n# calculate the probability of 0 to 6 deaths among 50 patients with an expected rate of 4%\n# binomial distribution\ndb &lt;- 100*dbinom(c(0:8),50, .04) # expected 4% probability of death\n\n# Poisson distribution\ndp &lt;- 100*dpois(c(0:8),2) # number of expected deaths among the 50 subjects over 1 year\n# cat(\"Assuming a binomial distribution with an event (death) \n# probability of 4%, the probability for 0,1,2,3,4,5,6 events is  \", # db[1],\"%,\", db[2],\"%,\", db[3],\"%,\", db[4],\"%,\", db[5],\"%,\", # db[6],\"%, respectively\")\n#cat(\"\\nAssuming a Poisson distribution with an event (death) rate of # 2 (2 deaths / 50 individual in 1 year, 4% expected mortality), the # probability for 0,1,2,3,4,5,6 events is \", dp[1],\"%,\", dp[2],\"%,\", # dp[3],\"%,\", dp[4],\"%,\", dp[5],\"%,\", dp[6],\"%, respectively\")\n\n\nAssuming a binomial distribution with an event (death) probability of 4%, the probability for 0, 1, 2, 3, 4, 5, 6 events is 13%, 27.1%, 27.6%, 18.4%, 9%, 3.5%, respectively. Assuming a Poisson distribution with an event (death) rate of 2 (# deaths in 50 individual in 1 year with 4% expected mortality), the probability for 0, 1, 2, 3, 4, 5, 6 events is 13.5%, 27.1%, 27.1%, 18%, 9%, 3.6%, respectively.\nThis data can also be visualized as shown here\n\nCode# Build Poisson distributions\noptions(digits = 3)\np_dat &lt;- map_df(2, ~ tibble(\n  l = paste(.),\n  x = 0:10,\n  y = dpois(0:10, .)\n))\n# Use ggplot2 to plot\nggplot(p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  geom_line() +\n  geom_point(data = p_dat, aes(x, y, color = factor(l, levels = 1:5))) +\n  labs(color = \"Lambda:\") +\n  ggtitle(\"Poisson distribution for death \\nrate of 2 / 50 subjects per year\") +\n  scale_x_continuous(\"no. of events\",breaks=seq(0,10, 1)) +\n  ylab(\"probability\") +\n  theme_minimal() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nThe similarity between the Poisson and Binomial distributions is shown graphically below\n\nCodeoptions(digits=2)\ndf &lt;- data.frame(y=round(rbinom(50000,50,.04)))\n\ng = ggplot(df, aes(x=y)) \n# g + geom_histogram(binwidth=1, fill=\"grey60\", color=\"red\") # counts\n# g + geom_histogram(aes(y=after_stat(density)), binwidth=1, fill=\"grey60\", color=\"red\") # density\n\ng + geom_histogram(aes(y=after_stat(density)), binwidth=1, # density\n                   fill=\"grey60\", color=\"red\") +\n  geom_density(adjust=5, color=\"red\", linewidth=1.1) +\n  xlab(\"counts\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1)) +\n  ggtitle(\"Histogram (and kernal density) of count from a \n  \\nBinomial (50, 0.04) probability distribution\")\n\n\n\n\n\n\nCode# same code for Poisson\n# df1 &lt;- data.frame(y=round(rpois(50000,2)))\n\n\nTherefore the answer to Q1 is 10 - 14.9%\n\nQ2. If mortality rates with transcatheter MVIV were actually 40% higher those with a standard redo, what is the probability of observing 0 deaths as these authors did?\n\nIn other words what is the probability of observing 0 deaths if the expected rate was 5.6% (4*1.4).\n\nCodeoptions(digits=1)\n# calculate the probability of 0 to 6 deaths among 50 patients with an expected rate of 4%\n# binomial distribution\ndb1 &lt;- 100*dbinom(c(0:8),50, .056) # expected 4% probability of death\n\n# Poisson distribution\ndp1 &lt;- 100*dpois(c(0:8),2.8) # number of expected deaths among the 50 subjects over 1 year\n\n\nAssuming a binomial distribution with an event (death) probability of 5.6%, the probability for 0, 1, 2, 3, 4, 5, 6 events is 5.6%, 16.6%, 24.2%, 22.9%, 16%, 8.7%, respectively. Assuming a Poisson distribution with an event (death) rate of 2.8 (# deaths in 50 individual in 1 year with 5.6% expected mortality), the probability for 0, 1, 2, 3, 4, 5, 6 events is 6.1%, 17%, 23.8%, 22.2%, 15.6%, 8.7%, respectively.\nThis may be easier to appreciate visually\n\nCodep_dat &lt;- map_df(c(2,2.8), ~ tibble(\n  l = paste(.),\n  x = 0:10,\n  y = dpois(0:10, .)\n))\n# Use ggplot2 to plot\nggplot(p_dat, aes(x, y, color = factor(l, levels = c(2,2.8)))) +\n  geom_line() +\n  geom_point(data = p_dat, aes(x, y, color = factor(l, levels = c(2,2.8)))) +\n  labs(color = \"Rates\") +\n  ggtitle(\"Poisson distribution for varying rates\") +\n  scale_x_continuous(breaks = c(1:10), limits = c(0, 10)) +\n  theme_minimal()\n\n\n\n\n\n\n\nClearly, if the expected death rate is higher the curves shift right meaning the probability of observing 0 deaths will fall with increasing mortality rates, soThe answer to Q2 is 5 - 9.9%\n\nQ3. Given the above information (and that there is probably not a lot of other of good quality available evidence), what is your probability (belief) that MVIV is as safe or safer than a standard redo operation in this population?\n\nIideally one would like to know the outcome with both interventions in the same patient but such counterfactuals unfortunately don’t exist. As an alternative we perform randomized trials, where we hope that subjects in the two treatment arms are exchangeable. In this case, we lack a proper comparative arm but can assume that if the STS model is accurate and that the MVIV patients would have had a 4% mortality if instead of MVIV they had had a surgical redo. Therefore our data are the 50 observed MVIV results and 50 results under a redo operation with a 4% mortality rate.\n\nCode# create data frame\ndat1 &lt;- tibble(Trial = c(\"study\", \"study\"), Tx = c(\"redo\", \"MVIV\"),\n               fail = c(2, 0),\n               success = c(48,50)) %&gt;% \n  mutate(total = fail + success,\n         prop_success = success / total) \n\nfit4 &lt;- brm(success | trials(total) ~ Tx, data = dat1, iter = 10000,\n            family = binomial(link=\"identity\"), refresh=0,\n            control = list(adapt_delta = .999),\n            backend = \"cmdstanr\", seed = 123,\n            file = \"fit4.RDS\") #aggregate data\n\n\n\nCodefit4 &lt;- readRDS(\"fit4.RDS\")\nsummary(fit4)\n\nWarning: There were 15936 divergent transitions after warmup. Increasing\nadapt_delta above 0.999 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: binomial \n  Links: mu = identity \nFormula: success | trials(total) ~ Tx \n   Data: dat1 (Number of observations: 2) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.98      0.02     0.93     1.00 1.00     1229     2480\nTxredo       -0.04      0.04    -0.13     0.03 1.01      703      521\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs expected the mean mortality difference (MVIV - redo) is - 4% with 95% credible intervals -13% -to 3% This probability density for this mortality difference is shown graphically below\n\nCodedraws &lt;- as_draws_df(fit4)\np_gt_0 &lt;- sum(draws$b_Txredo &gt;0) / 20000\ndraws %&gt;% \n  ggplot(aes(x = b_Txredo, fill = after_stat(x &gt; 0))) +\n  stat_halfeye(point_interval = mean_qi, .width = .95) +\n  scale_fill_manual(values = c(\"gray85\", \"skyblue\")) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"Mortality difference (MVIV - Redo)\") +\n  labs(title = \"Mortality difference (catheter MVIV - surgical redo)\",\n       subtitle = \"Blue area = probability increased mortality with MVIV (23.5%)\") +\n  theme_ggdist() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nThis figure shows that there is a 11.7% probability that MVIV patients in this study would have a worse outcome with a surgical redo, provided their underlying mortality has been well predicted with the STS model. Alternatively, the probability that MVIV is as safe or safer than a redo is 88.3%.Therefore the answer to Q3 is 76-95%"
  },
  {
    "objectID": "posts/2024-09-30-Reflections-on-uncertainties/index.html#discussion",
    "href": "posts/2024-09-30-Reflections-on-uncertainties/index.html#discussion",
    "title": "Estimating Uncertainties",
    "section": "Discussion",
    "text": "Discussion\nFor Q1, only 3 people (16%) correctly estimated that there was a 10-14.9% probability of observing 0 deaths in 50 patients, if the true underlying mortality rate was 4%. Of the 16 respondents who incorrectly estimated the probability, most (13 of 16) underestimated the probability of observing 0 deaths. For Q2, most respondents (13/19, 68%) correctly reasoned that if the true underlying mortality increased, it would be more unlikely to observe 0 deaths. However 6 individuals didn’t appreciate this and incorrectly predicted no change or an increase in the probability of observing 0 deaths if the true underlying rate was higher. For Q3, there are many assumptions required and estimates are likely fairly unreliable. Nevertheless, the question asked to ignore the potential biases and to consider only the data in this one study. Only 2 (10.5%) of individuals correctly estimated the probability of MVIV being as safe or safer than a redo being in the 76-95% interval. 12 of 19 (63%) of respondents estimated this probability at being under 50%.\nDrawing meaningful conclusions is obviously limited by the limited number of respondents, especially among the fellows. It is unknown if respondents have better, worse or same quantitative skills as the non-respondents. Notwithstanding this limitation, it would appear from the wide range of responses and the limited number of “correct” answers that additional quantitative training may be helpful."
  },
  {
    "objectID": "posts/2024-10-30-Reflections-on-peer-review/index.html",
    "href": "posts/2024-10-30-Reflections-on-peer-review/index.html",
    "title": "Reflections on peer review",
    "section": "",
    "text": "I suspect that you would be hard pressed to find a researcher who doesn’t believe that there are problems with peer review, at least with the unfavorable reviews of grant submissions and manuscripts that they have personally received, even if they are uncertain about the systematic nature of the problem.\nTypical peer review model\n\nEditors decide based on unknown study characteristics (likely some combination of novelty, controversy and potential hits but not methodological quality) to desk reject or send for peer review\nIf sent for external review, based on reviews received and editor’s prior beliefs, decision made to reject or accept, generally a conditional acceptance depending on requested revisions.\nIf the outcome is rejection, the authors generally receive either 1) a desk rejection notification with no reviewer comments or 2) a rejection letter with anonymized reviewer comments. However, in both cases, there is typically no information about the editorial decision-making process, other than perhaps a vanilla, non-informative “it doesn’t meet our priorities”.\n\nThe whole process lacks accountability from both reviewers and editors. Except for the BMJ (and perhaps others that I’m not aware of), journals still rely on anonymous reviews. It is difficult to believe that reviewer diligence would not be enhanced with signed reviews. The absence of any clarity on editorial decision making is not only unhelpful for authors seeking to improve the quality of future resubmissions but also enlarges the potential space for arbitrary decisions that are more influenced by novelty, controversy, and click bait rather than based on the study’s research question and methodological merits independent of results.\nAre there other possible solution besides signed review reviews to increase the quality of the peer review system. Two other solutions that have suggested are\n\nRequest peer review be performed in the absence of the results) and discussion to encourage rewarding study design and analytical quality\nPut more effort into post-publication review. Essentially this has been put forward as an efficiency argument whereby efforts are concentrated on papers that people care about.\n\nWhy these thoughts of peer review this Saturday morning. One answer is the role that inadequate peer review plays in allowing the publication of studies with faulty design, data (although in fairness hard to see how peer review can deal with this) and analytics that contribute to the reproducibility crisis and form the basis for many of my previous blog posts. Of course this is only one side of the peer review problem, a kind of type I error. What about the other side, the type II error where a paper or grant is refused by a questionable peer review process.\nYesterday I received a review from a respected Journal, at least as measured by their impact factor of 30.2 and according to their website 3.3 million annual article views and downloads. This is what has stimulated this post and I’m including a redacted version of a letter I sent to the editor in response to this review.\nDear Editor-in-chief of XXXX\nI would like to provide you with some feedback regarding your peer review of my recently submitted manuscript ( XX-XXXX ). To be clear, my complaints have nothing to do with the final decision but are related to the process. As my manuscript involves a well performed pragmatic randomized clinical trial (RCT), I have no doubt it will eventually find a good home. Therefore, I am NOT seeking to appeal your decision. Nor am I expecting or desiring a reply to this letter, especially one containing “alternative” facts.\n\nIssue #1 Unacceptable Delay:\n\nThe manuscript was submitted on July 8th and my rejection letter with two enclosed reviews received on September 27, a total of 12 weeks or 83 days to be exact. As I have experience as journal associate editor, I am aware of the difficulty in securing peer reviewers and receiving their timely reports. Nevertheless, tardy acceptance replies, and final reports are now successfully minimized by automated computerized managerial systems so that an associate editor with a minimally responsible attitude can typically, especially for a high impact journal like yours, secure reviewers within one week. Reviewers are now expected to return their reviews within 2 weeks and even allowing for an additional week that required another automated reminder, the review process is now at 4 weeks. So in my case, the journal editors then required a further 8 weeks to reach a decision? Seriously? This borders on total disrespect for the authors. No, it doesn’t border on, it is disrespectful!\n\nIssue # 2 Quality of Review # 1:\n\nThey begin by mentioning a typo that was in the abstract. Is this really the most important element for this reviewer? Their second point is to flatly disagree with the study premise that controversy regarding different treatment options exists. They supply nothing to support their opinion and completely ignore the manuscript’s introductory paragraphs which explain in detail, with references, the existence and origins of the uncertainty. While I find these two points rather disappointing as an opening critique of our study, their third point is so egregious that it rendered me almost apoplectic. And here it is - “for my knowledge what is a “pragmatic” RCT?”. Bear in mind that the title of my manuscript includes the wording “a pragmatic cluster randomized controlled trial”. This raises several questions. First, while there is no shame in not knowing what a pragmatic RCT is, it does make one wonder about the appropriateness of selecting this reviewer. Second, if the reviewer doesn’t know about pragmatic trials, then they would seem to have two distinct courses of action. One would be to decline the review, and the second would be to spend about 15 minutes Googling and reading about the subject. Simply stating in your review “for my knowledge what is a “pragmatic” RCT?” is NOT a reasonable third option. At this point, I would have thought the editorial board might consider excluding this review from further consideration and replacing them with a third reviewer, if necessary. I won’t bother with the rest of this mundane review which includes questions that were well answered in the manuscript had it been read with the slightest attentiveness and otherwise irrelevant comments.\n\nIssue #3 The editorial decision process.\n\nI will first comment on the report from Reviewer 2. Lest you feel I am over valuing my own work, here is opening paragraph from Reviewer #2\n“This is an elegant and innovative trial that provides valuable new evidence to the field. Its pragmatic design that leverages provincial electronic health records and time-clustered randomization strengthen the value of this work while minimizing logistical challenges and reducing costs. The use of a Bayesian analytical approach is commendable, as it allows incorporation of different prior beliefs (vague, skeptical, enthusiastic and summary) highlighting how these could impact the interpretation of the results with respect to clinically meaningful outcomes. The authors have done an exceptional job in making all these complex methodological aspects accessible to the reader through a clear and concise manuscript.”\nReviewer 2 goes on to make several suggestions that would improve the overall clarity of the writing that I am thankful for, and which have been incorporated in a revised version. Returning to the editorial process, I have already commented on the excessive delays, but I have concerns that extend beyond that issue. How, was a reviewer who has no knowledge of pragmatic trials selected to review one? How is a flat rejection arrived at when one review is obviously of ridiculously low quality and the other largely favorable? Is the editorial board unable to distinguish between the quality of the two reviews? The rejection letter makes no comment in that regard, not even the usual generic “cover-all your bases” reply “it doesn’t meet our current priorities” which conveniently absolves the editors from making any meaningful comments about their decision making.\nI repeat I am not appealing your decision, and the manuscript is currently under review elsewhere. I offer these comments in the rather naïve hope of improving your peer review process.\nRegards\n\nNow that I have vented, it is time to get back to more enjoyable weekend activities.\n\n\n\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {Reflections on Peer Review},\n  date = {2024-10-05},\n  url = {https://brophyj.com/posts/2024-09-14-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “Reflections on Peer Review.” October 5,\n2024. https://brophyj.com/posts/2024-09-14-my-blog-post/."
  },
  {
    "objectID": "posts/2024-10-30-Reflections-on-peer-review/index.html#background",
    "href": "posts/2024-10-30-Reflections-on-peer-review/index.html#background",
    "title": "Reflections on peer review",
    "section": "",
    "text": "I suspect that you would be hard pressed to find a researcher who doesn’t believe that there are problems with peer review, at least with the unfavorable reviews of grant submissions and manuscripts that they have personally received, even if they are uncertain about the systematic nature of the problem."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction?/index.html",
    "href": "posts/2024-10-31-How_bad_before_retraction?/index.html",
    "title": "Reflections on peer review - part 2",
    "section": "",
    "text": "CitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {Reflections on Peer Review - Part 2},\n  date = {2024-10-31},\n  url = {https://brophyj.com/posts/2024-10-31-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “Reflections on Peer Review - Part 2.”\nOctober 31, 2024. https://brophyj.com/posts/2024-10-31-my-blog-post/."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction?/index.html#first-the-journal",
    "href": "posts/2024-10-31-How_bad_before_retraction?/index.html#first-the-journal",
    "title": "Reflections on peer review - part 2",
    "section": "First the journal",
    "text": "First the journal\nI was had heard of the journal, principally through its Nature branding but was quite unaware of much else, including its overall quality. Their website claims “Scientific Reports is the 5th most-cited journal in the world, with more than 734,000 citations in 2023”. Impressive! I also found their impact factor (IF) was 3.8. Now we are all aware of the quality limitations of IF but these numbers do raise some questions. \\[Impact\\,Factor = \\frac{\n  Number\\,of\\,Citations\\,in\\,2023\\,to\\,articles\\,from\\,2021\\,and\\,2022}{\n    Number\\,of\\,Articles\\,published\\,in\\,2021\\,and\\,2022}\\] Thus while the number of reported citations does seem impressive, the IF suggests either many of these citations refer to reports prior to 2021, the citations don’t count in the IF or that the journal publishes an extraordinary high number of articles each year that dilutes the IF.\n## The authors\nAuthors and Affiliations\nXiaolong Qu Department of Cardiovascular Medicine, Renji Hospital, Shanghai Jiaotong University School of Medicine, 160 Pujian Road, Pudong New Area, Shanghai, 200120, China\nLei Huang & Jiacheng Rong\nDepartment of Cardiology, Renji Hospital Ningbo Branch, Shanghai Jiao Tong University School of Medicine, 1155 Binhai 2nd Road, Hangzhou Bay New District, Ningbo, 315336, China\nAccording to Google Scholar they have previously published 8 articles together\n\nNow that I have vented, it is time to get back to more enjoyable weekend activities."
  },
  {
    "objectID": "indexTRIAL.html",
    "href": "indexTRIAL.html",
    "title": "James (Jay) Brophy",
    "section": "",
    "text": "James (Jay) Brophy is a tenured (full) professor with a joint appointment in the Departments of Medicine and Epidemiology and Biostatistics at McGill University. He is a cardiologist and does research in cardiovascular epidemiology. His research interests are eclectic and include outcomes research, pharmacoepidemiology, Bayesian statistics, health technology assessment, economic analyses and clinical research. He held a FRQS chair in health technology assessment and evidence-based medicine (2008-2023).\n He is a fellow of the Canadian Academy of Health Sciences and the Canadian Cardiovascular Society.\nHave a look at my  curriculum vitae"
  },
  {
    "objectID": "indexTRIAL.html#education",
    "href": "indexTRIAL.html#education",
    "title": "James (Jay) Brophy",
    "section": "Education",
    "text": "Education\n\nPhD Epidemiology & biostatistics - McGill University\n\nCardiology / Internal medicine - Université de Montreal\n\nMD - McMaster University\n\nM.Eng - McMaster University\n\nB.Eng - McGill University"
  },
  {
    "objectID": "indexTRIAL.html#interests",
    "href": "indexTRIAL.html#interests",
    "title": "James (Jay) Brophy",
    "section": "Interests",
    "text": "Interests\n\nCardiovascular medicine & epidemiology\nBayesian statistics\nEvidence based medicine\nData analysis & visualization\nKnowledge sharing\nMedical decision-making"
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html",
    "title": "Reflections on peer review - part 2",
    "section": "",
    "text": "One of the news feeds I subscribe to (ACC CV news) reported the headline “Ketogenic Diet Could Reduce The Risk For All-Cause Mortality By Up To 24%” with a reference to an article published in Nature Scientific Reports(Qu, Huang, and Rong 2024). Moreover it was reported that the results “suggest the keto diet did not increase the risk for cardiovascular disease-related death despite its content possibly contributing to a higher risk for cardiovascular diseases.” Sounds too good to be true so I had a look."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#first-the-journal",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#first-the-journal",
    "title": "Reflections on peer review - part 2",
    "section": "First the journal",
    "text": "First the journal\nI was had heard of the journal, principally through its Nature branding but was quite unaware of much else, including its overall quality. Their website claims “Scientific Reports is the 5th most-cited journal in the world, with more than 734,000 citations in 2023”. Impressive! I also found their impact factor (IF) was 3.8. Now we are all aware of the quality limitations of IF but these numbers do raise some questions. \\[Impact\\,Factor = \\frac{\n  Number\\,of\\,Citations\\,in\\,2023\\,to\\,articles\\,from\\,2021\\,and\\,2022}{\n    Number\\,of\\,Articles\\,published\\,in\\,2021\\,and\\,2022}\\] Thus while the number of reported citations does seem impressive, the IF suggests either many of these citations refer to reports prior to 2021, the citations don’t count in the IF or that the journal publishes an extraordinary high number of articles each year that dilutes the IF.\n## The authors\nAuthors and Affiliations\nXiaolong Qu Department of Cardiovascular Medicine, Renji Hospital, Shanghai Jiaotong University School of Medicine, 160 Pujian Road, Pudong New Area, Shanghai, 200120, China\nLei Huang & Jiacheng Rong\nDepartment of Cardiology, Renji Hospital Ningbo Branch, Shanghai Jiao Tong University School of Medicine, 1155 Binhai 2nd Road, Hangzhou Bay New District, Ningbo, 315336, China\nAccording to Google Scholar they have previously published 8 articles together\n\nNow that I have vented, it is time to get back to more enjoyable weekend activities."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#background",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#background",
    "title": "Reflections on peer review - part 2",
    "section": "",
    "text": "One of the news feeds I subscribe to (ACC CV news) reported the headline “Ketogenic Diet Could Reduce The Risk For All-Cause Mortality By Up To 24%” with a reference to an article published in Nature Scientific Reports(Qu, Huang, and Rong 2024). Moreover it was reported that the results “suggest the keto diet did not increase the risk for cardiovascular disease-related death despite its content possibly contributing to a higher risk for cardiovascular diseases.” Sounds too good to be true so I had a look."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#the-journal",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#the-journal",
    "title": "Reflections on peer review - part 2",
    "section": "The journal",
    "text": "The journal\nI had previously heard of the journal, principally through its Nature branding but was quite unaware of much else, including its overall quality. Their website claims “Scientific Reports is the 5th most-cited journal in the world, with more than 734,000 citations in 2023”. Impressive! I also found their impact factor (IF) was 3.8. Now we are all aware of the quality limitations of IF but these numbers do raise some questions. \\[Impact\\,Factor = \\frac{\n  Number\\,of\\,Citations\\,in\\,2023\\,to\\,articles\\,from\\,2021\\,and\\,2022}{\n    Number\\,of\\,Articles\\,published\\,in\\,2021\\,and\\,2022}\\] Thus while the number of reported citations does seem impressive, the IF suggests either many of these citations refer to reports prior to 2021, these citations wouldn’t count in the IF, or that the journal publishes an extraordinary high number of articles each year that dilutes the IF."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#the-authors",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#the-authors",
    "title": "Reflections on peer review - part 2",
    "section": "The authors",
    "text": "The authors\nAuthors and Affiliations\nXiaolong Qu Department of Cardiovascular Medicine, Renji Hospital, Shanghai Jiaotong University School of Medicine, 160 Pujian Road, Pudong New Area, Shanghai, 200120, China\nLei Huang & Jiacheng Rong\nDepartment of Cardiology, Renji Hospital Ningbo Branch, Shanghai Jiao Tong University School of Medicine, 1155 Binhai 2nd Road, Hangzhou Bay New District, Ningbo, 315336, China\nAccording to Google Scholar they have previously published 8 articles together."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#the-data",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#the-data",
    "title": "Reflections on peer review - part 2",
    "section": "The data",
    "text": "The data\nThis study used 43,776 adults from the National Health and Nutrition Examination Survey (NHANES) conducted between 2001 and 2018 to investigate the potential association between dietary ketogenic ratio (DKR) and both all-cause mortality and cardiovascular disease (CVD) mortality."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#their-results",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#their-results",
    "title": "Reflections on peer review - part 2",
    "section": "Their results",
    "text": "Their results\nUsing an adjusted Cox proportional hazards regression analysis the authors report a significant inverse association was observed between the dietary ketogenic ratio (DKR) and all-cause mortality (HR = 0.76, 95% CI = 0.63–0.9, P = 0.003). However, no significant association with cardiovascular mortality was found (HR = 1.13; CI = 0.79–1.6; P = 0.504)."
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#their-conclusions",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#their-conclusions",
    "title": "Reflections on peer review - part 2",
    "section": "Their conclusions",
    "text": "Their conclusions\n“In the adult population of the United States, adherence to a KD exhibits potential in reducing all-cause mortality risk while not posing an increased threat of CVD-related fatalities.”"
  },
  {
    "objectID": "posts/2024-10-31-How_bad_before_retraction/index.html#my-issues-in-no-particular-order",
    "href": "posts/2024-10-31-How_bad_before_retraction/index.html#my-issues-in-no-particular-order",
    "title": "Reflections on peer review - part 2",
    "section": "My issues (in no particular order)",
    "text": "My issues (in no particular order)\n\nMeasurement error: The exposure variable DKR is never measured. The authors report DKR is calculated based on an equation proposed in a 1980 publication entitled “The ketogenic diet: mechanism of anticonvulsant action”(Withrow 1980). The exact equation and the population is was derived from and relationship to the study population is unknown. This is in addition to the usual measurement errors associated with dietary 24 hour recall studies.\nSelection bias: The study population of eligible adults was 50,201 and 43,776 were analysed. It is unknown if this missing 13% resulted in selection bias but the possibility certainly exists.\nInferential errors: i) The authors report “no significant association with cardiovascular mortality was found (HR = 1.13; CI = 0.79–1.6; P = 0.504).” Classic error of “absence of evidence is not evidence of absence”[Altman and Bland (1995)](Gelman and Stern 2006). The data is compatible upon repeated sampling with a potential 60% increase in cardiovascular mortality. ii) Questionable causal inferences “Additional investigations are warranted to comprehensively comprehend the mechanisms underlying these observed associations and ascertain the feasibility and long-term sustainability of implementing a ketogenic diet for population-wide health benefits.”\nPoor reporting: i) The actual DKR values are not provided. Instead we are only given quartile distributions among the deceased and survivors.ii) no discussion of the impact of researcher degrees of freedom in the choice of the 2 adjusted models iii) no discussion of model mis-specification iv) no verification of the Cox model assumptions.\nQuestionable data analysis: Here is a summary of the key elements of the baseline data according to survival status.\n\n\nCodelibrary(gt)\n\n# Create the data frame\ndata &lt;- data.frame(\n  Characteristic = c(\"Gender\", \"Gender\", \"Age, y\", \"Age, y\", \"Age, y\", \"DKR\", \"DKR\", \"DKR\", \"DKR\"),\n  Category = c(\"Male\", \"Female\", \"&lt; 40\", \"40–60\", \"≥ 60\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n  Deceased = c(\"3,388 (56.0%)\", \"2,666 (44.0%)\", \"251 (4.1%)\", \"947 (15.6%)\", \"4,856 (80.2%)\", \n               \"1,563 (25.8%)\", \"1,540 (25.4%)\", \"1,498 (24.7%)\", \"1,453 (24.0%)\"),\n  Survival = c(\"17,758 (47.1%)\", \"19,964 (52.9%)\", \"14,908 (39.5%)\", \"13,121 (34.8%)\", \n               \"9,693 (25.7%)\", \"9,374 (24.9%)\", \"9,414 (25.0%)\", \"9,443 (25.0%)\", \"9,491 (25.2%)\"),\n  p_value = c(\"&lt; 0.001\", \"\", \"&lt; 0.001\", \"\", \"\", \"0.045\", \"\", \"\", \"\")\n)\n\n# Create the gt table\ngt_table &lt;- gt(data) %&gt;%\n  tab_header(\n    title = \"Study Characteristics and Outcomes\"\n  ) %&gt;%\n  cols_label(\n    Characteristic = \"Characteristic\",\n    Category = \"Category\",\n    Deceased = \"Deceased (N = 6,054)\",\n    Survival = \"Survival (N = 37,722)\",\n    p_value = \"p-value\"\n  ) %&gt;%\n  fmt_markdown(columns = c(Deceased, Survival)) %&gt;%\n  tab_options(\n    column_labels.border.bottom.width = px(1),\n    column_labels.border.bottom.style = \"solid\",\n    column_labels.border.bottom.color = \"#D3D3D3\",\n    table.width = pct(100),\n    data_row.padding = px(10)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"gray95\"),\n      cell_borders(sides = \"bottom\", color = \"gray90\", style = \"solid\", weight = px(1))\n    ),\n    locations = cells_body(\n      columns = everything(),\n      rows = TRUE\n    )\n  ) %&gt;%\n  tab_style(\n    style = cell_text(align = \"left\"),\n    locations = cells_body(columns = vars(Characteristic, Category))\n  ) %&gt;%\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body(columns = vars(Deceased, Survival, p_value))\n  )\n\ngt_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudy Characteristics and Outcomes\n\n\nCharacteristic\nCategory\nDeceased (N = 6,054)\nSurvival (N = 37,722)\np-value\n\n\n\n\nGender\nMale\n3,388 (56.0%)\n17,758 (47.1%)\n&lt; 0.001\n\n\nGender\nFemale\n2,666 (44.0%)\n19,964 (52.9%)\n\n\n\nAge, y\n&lt; 40\n251 (4.1%)\n14,908 (39.5%)\n&lt; 0.001\n\n\nAge, y\n40–60\n947 (15.6%)\n13,121 (34.8%)\n\n\n\nAge, y\n≥ 60\n4,856 (80.2%)\n9,693 (25.7%)\n\n\n\nDKR\nQ1\n1,563 (25.8%)\n9,374 (24.9%)\n0.045\n\n\nDKR\nQ2\n1,540 (25.4%)\n9,414 (25.0%)\n\n\n\nDKR\nQ3\n1,498 (24.7%)\n9,443 (25.0%)\n\n\n\nDKR\nQ4\n1,453 (24.0%)\n9,491 (25.2%)\n\n\n\n\n\n\n\n\nIt is not clear where the p = 0.045 for DKR variable comes from.\n\nCodedkr_table &lt;- matrix(c(1563, 1540, 1498, 1453, 9374, 9414, 9443, 9491), ncol = 2, byrow = TRUE)\ncolnames(dkr_table) &lt;- c(\"Deceased\", \"Survival\")\nrownames(dkr_table) &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\nchisq.test(dkr_table)\n\n\n    Pearson's Chi-squared test\n\ndata:  dkr_table\nX-squared = 1, df = 3, p-value = 0.8\n\n\nThis is worrisome and provides stimulus to look at the outcomes. Obviously without the raw data, we are limited to looking at the unadjusted models. The authors report the following for the unadjusted model\n\n\n\n\n\n\n\n\n\nUnadjusted model\nQ1\nQ2\nQ3\nQ4\n\n\n\n1\n1.03(0.96, 1.10)\n1.05(0.97, 1.12)\n1.05(0.98, 1.13)\n\n\nNow this can be verified using the baseline data provided Here’s the calculation for each quartile:\nQ1: Risk = 1563 / (1563 + 9374)\nQ2: Risk = 1540 / (1540 + 9414)\nQ3: Risk = 1498 / (1498 + 9443)\nQ4: Risk = 1453 / (1453 + 9491)\n\\[Risk\\,ratio_{Q2,Q1} = \\frac{Q2\\,risk}{Q1\\,risk} = \\frac{1540 / (1540 + 9414)}{1563/(1563 + 9374) } = 0.984\\] Results for the other RR and 95%CI are easily calculated\n\nCodelibrary(epiR)\n\n# Define the counts of deceased and survival for each quartile\ndeceased &lt;- c(1563, 1540, 1498, 1453)\nsurvival &lt;- c(9374, 9414, 9443, 9491)\n\n# Calculating the risk ratio and 95% CI using Q1 as the exposed control\nresults &lt;- lapply(2:4, function(i) {\n  # Create the 2x2 table for each comparison with correct exposure designation\n  # Exposed (Q1) first, then Unexposed (Q2, Q3, Q4)\n  table &lt;- matrix(c(deceased[i], survival[i], deceased[1], survival[1]), nrow = 2, byrow = TRUE)\n  colnames(table) &lt;- c(\"Deceased\", \"Survival\")\n  rownames(table) &lt;- c(sprintf(\"Q%d (Unexposed)\", i), \"Q1 (Exposed)\")\n  \n  # Calculate the risk ratios using epi.2by2\n  epiR::epi.2by2(table, method = \"cohort.count\", conf.level = 0.95)\n})\nresults\n\n[[1]]\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +         1540         9414      10954     14.06 (13.41 to 14.72)\nExposed -         1563         9374      10937     14.29 (13.64 to 14.96)\nTotal             3103        18788      21891     14.17 (13.72 to 14.64)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.98 (0.92, 1.05)\nInc odds ratio                                 0.98 (0.91, 1.06)\nAttrib risk in the exposed *                   -0.23 (-1.16, 0.69)\nAttrib fraction in the exposed (%)            -1.65 (-8.50, 4.76)\nAttrib risk in the population *                -0.12 (-0.92, 0.69)\nAttrib fraction in the population (%)         -0.82 (-4.13, 2.39)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 0.242 Pr&gt;chi2 = 0.622\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.628\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n[[2]]\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +         1498         9443      10941     13.69 (13.05 to 14.35)\nExposed -         1563         9374      10937     14.29 (13.64 to 14.96)\nTotal             3061        18817      21878     13.99 (13.53 to 14.46)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.96 (0.90, 1.02)\nInc odds ratio                                 0.95 (0.88, 1.03)\nAttrib risk in the exposed *                   -0.60 (-1.52, 0.32)\nAttrib fraction in the exposed (%)            -4.38 (-11.47, 2.26)\nAttrib risk in the population *                -0.30 (-1.10, 0.50)\nAttrib fraction in the population (%)         -2.14 (-5.48, 1.09)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 1.633 Pr&gt;chi2 = 0.201\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.205\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n[[3]]\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +         1453         9491      10944     13.28 (12.65 to 13.93)\nExposed -         1563         9374      10937     14.29 (13.64 to 14.96)\nTotal             3016        18865      21881     13.78 (13.33 to 14.25)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.93 (0.87, 0.99)\nInc odds ratio                                 0.92 (0.85, 0.99)\nAttrib risk in the exposed *                   -1.01 (-1.93, -0.10)\nAttrib fraction in the exposed (%)            -7.64 (-15.02, -0.73)\nAttrib risk in the population *                -0.51 (-1.31, 0.29)\nAttrib fraction in the population (%)         -3.68 (-7.05, -0.42)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 4.735 Pr&gt;chi2 = 0.030\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.031\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nSo if the unadjusted results are totally different from what has been reported, how could anyone have the slightest confidence in a magically derived adjusted model, especially when considered in the context of the reservations noted above?\nShould the authors maintain their confidence in their findings they should share the dataset and the statistical code that generated these findings.\nThe problems herein are aggravated by the imprimatur provided by publication in a Nature journal and of using a highly recognizable dataset which has historically been analyzed with much rigour and care.\nAccording to Altmetric, this article is in the 98th percentile (ranked 3,956th) of the 209,240 tracked articles of a similar age in all journals!\n\nNow that I have vented, it is time to get back to other activities."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html",
    "href": "posts/2024-11-04-Personal_beliefs/index.html",
    "title": "Personal beliefs",
    "section": "",
    "text": "In the CLEAR OASIS 9 trial acute MI patients were randomized to colchicine (n = 3,528) or placebo (n = 3,534) right after percutaneous coronary intervention (PCI).\nThe primary outcome of major adverse CV events (MACE), composite of CV death, MI, stroke, or ischemia-driven revascularization, for colchicine vs. placebo at 5 years, was: 9.1% vs. 9.3%, hazard ratio (HR) 0.99 (95% confidence interval [CI] 0.85-1.16), p = 0.93.\nSecondary outcomes for colchicine vs. placebo at 5 years:\nCV death: 3.3% vs. 3.2%, HR 1.03 (95% CI 0.80-1.34)\nAll-cause death: 4.6% vs. 5.1%, HR 0.90 (95% CI 0.73-1.12)\nMI: 2.9% vs. 3.1%, HR 0.88 (95% CI 0.66-1.17)\nIschemia-driven revascularization: 4.6% vs. 4.7%, HR 1.01 (95% CI 0.81-1.17)\nLSM C-reactive protein (CRP) levels at 3 months: 3.0 vs. 4.3 mg/dL, p &lt; 0.001\n\nCodelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(lme4)\n\n\nThere was an interesting quote from the principal investigator (PI) who said that before this study ““I was a believer in colchicine” but not a believer after the study. This dichotomization of beliefs is very common among physicians, perhaps influenced by the null hypothesis significance testing paradigm and the almighty p value that remain the cornerstone of most medical research.\nLet’s take a look at this from a probabilistic viewpoint."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html#most-recent-trial",
    "href": "posts/2024-11-04-Personal_beliefs/index.html#most-recent-trial",
    "title": "Personal beliefs",
    "section": "",
    "text": "In the CLEAR OASIS 9 trial acute MI patients were randomized to colchicine (n = 3,528) or placebo (n = 3,534) right after percutaneous coronary intervention (PCI).\nThe primary outcome of major adverse CV events (MACE), composite of CV death, MI, stroke, or ischemia-driven revascularization, for colchicine vs. placebo at 5 years, was: 9.1% vs. 9.3%, hazard ratio (HR) 0.99 (95% confidence interval [CI] 0.85-1.16), p = 0.93.\nSecondary outcomes for colchicine vs. placebo at 5 years:\nCV death: 3.3% vs. 3.2%, HR 1.03 (95% CI 0.80-1.34)\nAll-cause death: 4.6% vs. 5.1%, HR 0.90 (95% CI 0.73-1.12)\nMI: 2.9% vs. 3.1%, HR 0.88 (95% CI 0.66-1.17)\nIschemia-driven revascularization: 4.6% vs. 4.7%, HR 1.01 (95% CI 0.81-1.17)\nLSM C-reactive protein (CRP) levels at 3 months: 3.0 vs. 4.3 mg/dL, p &lt; 0.001\n\nCodelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(lme4)\n\n\nThere was an interesting quote from the principal investigator (PI) who said that before this study ““I was a believer in colchicine” but not a believer after the study. This dichotomization of beliefs is very common among physicians, perhaps influenced by the null hypothesis significance testing paradigm and the almighty p value that remain the cornerstone of most medical research.\nLet’s take a look at this from a probabilistic viewpoint."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html#what-is-the-prior-evidence",
    "href": "posts/2024-11-04-Personal_beliefs/index.html#what-is-the-prior-evidence",
    "title": "Personal beliefs",
    "section": "What is the prior evidence?",
    "text": "What is the prior evidence?\nThe PI stated before his trial was completed he was a “believer”, then what was his evidence to support this belief. Perhaps his belief was founded on the on the results from the COLCOT study where patients were randomized to the same dose of colchicine (n=2366) or placebo (n=2379) a mean of 13.5 days after their acute MI. The primary efficacy end point was a composite of death from cardiovascular causes, resuscitated cardiac arrest, myocardial infarction, stroke, or urgent hospitalization for angina leading to coronary revascularization. The primary end point occurred in 5.5% of the patients in the colchicine group, as compared with 7.1% of those in the placebo group (hazard ratio, 0.77; 95% confidence interval [CI], 0.61 to 0.96; P=0.02).\nPresumably others didn’t quite share his belief or the necessary equipoise would not have been present to proceed with the CLEAR OASIS 9 trial. A presentation of these results from a probabilistic viewpoint may help understand these discrepancies."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html#bayesian-view-of-the-prior-evidence",
    "href": "posts/2024-11-04-Personal_beliefs/index.html#bayesian-view-of-the-prior-evidence",
    "title": "Personal beliefs",
    "section": "Bayesian view of the prior evidence",
    "text": "Bayesian view of the prior evidence\nLets’ first write some Stan code for a two arm binomial trial with vague priors\n\nCode# stan code for 2 arm binomial\nstancode &lt;- '\ndata {\n  int&lt;lower=0&gt; n1;  // Total number of trials in the placebo arm\n  int&lt;lower=0&gt; y1;  // Number of successes in the placebo arm\n  int&lt;lower=0&gt; n2;  // Total number of trials in the intervention arm\n  int&lt;lower=0&gt; y2;  // Number of successes in the intervention arm\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p1; // Probability of success in the placebo arm\n  real&lt;lower=0, upper=1&gt; p2; // Probability of success in the intervention arm\n}\n\nmodel {\n  p1 ~ beta(1, 1); // Default prior for p1\n  p2 ~ beta(1, 1); // Default prior for p2\n\n  y1 ~ binomial(n1, p1); // Likelihood for the placebo arm\n  y2 ~ binomial(n2, p2); // Likelihood for the intervention arm\n}\n\ngenerated quantities {\n  real rr = p2 / p1;  // Relative Risk of intervention arm over placebo arm\n}\n'\nwriteLines(stancode, \"binom_2.stan\")\n\n# check file exists\n# file.exists(\"binom_2.stan\")\n# cat(readLines(\"binom_2.stan\"), sep = \"\\n\")\n\n\nNow will execute this program with the COLCOT data in order to fully appreciate the entire posterior probability distribution as opposed to be a limted fixed viewpoint of an uninteresting null hypothesis as is the conventional statistical approach.\n\nCode# Data for the COLCOT model\ndata_list &lt;- list(\n  n2 = 2366,\n  y2 = as.integer(2366 * 0.055),\n  n1 = 2379,\n  y1 = as.integer(2379 * 0.071)\n)\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nCode# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\n# mcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1117.      -1.12e+3 1.06    0.756   -1.12e+3 -1.12e+3  1.00    1725.\n2 p1           0.0712   7.10e-2 0.00539 0.00544  6.25e-2  8.05e-2  1.00    3654.\n3 p2           0.0553   5.52e-2 0.00480 0.00465  4.73e-2  6.34e-2  1.00    2972.\n4 rr           0.782    7.79e-1 0.0923  0.0894   6.35e-1  9.40e-1  1.00    3200.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nCode# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\nprob_rr_less_08 &lt;- mean(rr_vector &lt; 0.8)\nprob_rr_less_10 &lt;- mean(rr_vector &lt; 1.0)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 1.0 (P (statistical signifiance)): \", prob_rr_less_10, \n    \"\\nProbability RR &lt; 0.8: \", prob_rr_less_08, \n    \"\\nProbability RR &lt; 0.9: \", prob_rr_less_09,\n    \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \n    \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 1.0 (P (statistical signifiance)):  0.98 \nProbability RR &lt; 0.8:  0.6 \nProbability RR &lt; 0.9:  0.89 \nProbability RR 0.9 to 1.1:  0.1 \nProbability RR &gt; 1.1:  0.001 \n\n\nThis may also be appreciated graphically\n\nCode# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on COLCOT\",\n       subtitle = \"with a vague prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.64, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.79, y = max(density_df$y, na.rm = TRUE) * 0.73, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.9, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.05, y = max(density_df$y, na.rm = TRUE) * 0.53, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.15, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.10, y = max(density_df$y, na.rm = TRUE) * 0.35, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThis figure explains why there may be different beliefs about the colchicine effect with some being enthusiastic about a 90% probability of a clinical benefit (assuming a 10% reduction in RR is an appropriate clinical threshold). Conversely others may be more conservative and want a larger reduction in RR, given the inconvenience and costs of taking another medication. The probability of an at least 20% reduction is only 60% underscoring the need for some to await further studies to better define any clinical benefits."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html#bayesian-approach-with-stan",
    "href": "posts/2024-11-04-Personal_beliefs/index.html#bayesian-approach-with-stan",
    "title": "Personal beliefs",
    "section": "Bayesian approach with Stan",
    "text": "Bayesian approach with Stan\nFor the current CLEAR OASIS results, initially let’s assume a vague priors so that the posterior probability distribution is completely dominated by the current data. We will estimate the probabilities for each arm and their relative risk (RR). From the RR posterior probability density one can further calculate probabilities for various cutpoints. For example, one might assume that we are particularly interested in probabilities that exceed a clinically meaningful benefit or harm. These cutpoints can be individually chosen but let’s again assume a benefit threshold of RR &lt; 0.9 and harm threshold of RR &gt; 1.1.\n\nCode# Data for the model\ndata_list &lt;- list(\n  n2 = 3528,\n  y2 = as.integer(3528 * 0.091),\n  n1 = 3534,\n  y1 = as.integer(3534 * 0.093)\n)\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nCode# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\nmcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n\n\n\n\n\nCode# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -2173.      -2.17e+3 0.998   0.719   -2.18e+3 -2.17e+3  1.00    1715.\n2 p1           0.0931   9.29e-2 0.00487 0.00494  8.52e-2  1.01e-1  1.00    3788.\n3 p2           0.0914   9.13e-2 0.00488 0.00484  8.36e-2  9.95e-2  1.00    2400.\n4 rr           0.985    9.82e-1 0.0734  0.0729   8.67e-1  1.11e+0  1.00    2532.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nCode# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 0.9: \", prob_rr_less_09, \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 0.9:  0.12 \nProbability RR 0.9 to 1.1:  0.81 \nProbability RR &gt; 1.1:  0.066"
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html#graphical-presentation",
    "href": "posts/2024-11-04-Personal_beliefs/index.html#graphical-presentation",
    "title": "Personal beliefs",
    "section": "Graphical presentation",
    "text": "Graphical presentation\nUnderstanding these results is again facilitated with graphical presentations\n\nCode# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on CLEAR OASIS\",\n       subtitle = \"with a vague prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.80, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.89, y = max(density_df$y, na.rm = TRUE) * 0.57, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.8, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = .92, y = max(density_df$y, na.rm = TRUE) * 0.77, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.1, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.19, y = max(density_df$y, na.rm = TRUE) * 0.37, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThis analysis shows that there is only a small probability (12%) of a clinically significant benefit (where this has been arbitrarily defined as a minimum 10% decrease in RR). There is a 7% of clinical harm (RR &gt;1.1) and a 81% probability of practical equivalence. Recall that this analysis examines only the current study but the lead PI claimed he was a priori a “believer” so to be consistent with this prior belief, this posterior belief based on a vague prior should be replaced with an informative prior belief that is updated with this new data. This infromative prior is combined with the current data following the laws of probability, i.e. a weighted average according with weights proportional to the precision of the prior and current data\n\nCode# stan code for 2 arm binomial with informative prior\nstancode &lt;- '\ndata {\n  int&lt;lower=0&gt; n1;  // Total number of trials in the placebo arm\n  int&lt;lower=0&gt; y1;  // Number of successes in the placebo arm\n  int&lt;lower=0&gt; n2;  // Total number of trials in the intervention arm\n  int&lt;lower=0&gt; y2;  // Number of successes in the intervention arm\n  real&lt;lower=0&gt; alpha1; // Alpha for beta prior on p1\n  real&lt;lower=0&gt; beta1;  // Beta for beta prior on p1\n  real&lt;lower=0&gt; alpha2; // Alpha for beta prior on p2\n  real&lt;lower=0&gt; beta2;  // Beta for beta prior on p2\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p1; // Probability of success in the placebo arm\n  real&lt;lower=0, upper=1&gt; p2; // Probability of success in the intervention arm\n}\n\nmodel {\n  p1 ~ beta(alpha1, beta1); // Informative prior for p1 based on COLCOT\n  p2 ~ beta(alpha2, beta2); // Informative prior for p2 based on COLCOT\n\n  y1 ~ binomial(n1, p1); // Likelihood for the placebo arm\n  y2 ~ binomial(n2, p2); // Likelihood for the intervention arm\n}\n\ngenerated quantities {\n  real rr = p2 / p1;  // Relative Risk of intervention arm over placebo arm\n}'\nwriteLines(stancode, \"binom_2_priorCOLCOT.stan\")\n# file.exists(\"binom_2_priorCOLCOT.stan\")\n\n\n\nCode# prior from COLCOT\n\nalpha1 = as.integer(2379 * 0.071)\nbeta1 &lt;- 2379 - alpha1\nalpha2 = as.integer(2366 * 0.055)\nbeta2 &lt;- 2366 - alpha2\n\n# Data for the model\ndata_list &lt;- list(\n  n1 = 3534,\n  y1 = as.integer(3534 * 0.093),\n  n2 = 3528,\n  y2 = as.integer(3528 * 0.091),\n  alpha1 = alpha1,\n  beta1 = beta1,\n  alpha2 = alpha2,\n  beta2 = beta2\n)\n\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2_priorCOLCOT.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nCode# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\nmcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n\n\n\n\n\nCode# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -3297.      -3.30e+3 1.02    0.749   -3.30e+3 -3.30e+3  1.00    1896.\n2 p1           0.0839   8.38e-2 0.00370 0.00367  7.80e-2  9.01e-2  1.00    3545.\n3 p2           0.0765   7.65e-2 0.00349 0.00341  7.08e-2  8.22e-2  1.00    3455.\n4 rr           0.913    9.13e-1 0.0576  0.0590   8.21e-1  1.01e+0  1.00    3283.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nCode# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 0.9: \", prob_rr_less_09, \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 0.9:  0.42 \nProbability RR 0.9 to 1.1:  0.58 \nProbability RR &gt; 1.1:  0.0013"
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/index.html#graphical-presentation-1",
    "href": "posts/2024-11-04-Personal_beliefs/index.html#graphical-presentation-1",
    "title": "Personal beliefs",
    "section": "Graphical presentation",
    "text": "Graphical presentation\nUnderstanding these results is facilitated with graphical presentations\n\nCode# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on CLEAR OASIS\",\n       subtitle = \"with an informative (COLCOT) prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.80, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.86, y = max(density_df$y, na.rm = TRUE) * 0.56, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.8, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = .90, y = max(density_df$y, na.rm = TRUE) * 0.76, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.00, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.07, y = max(density_df$y, na.rm = TRUE) * 0.33, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThe PI who was a “believer” prior to his study now claims after the study to be a non-believer (to the extent that he apparently stopped the medication that was previously prescribed to his father). Yet based on his positive prior beliefs (if these can be approximated by the COLCOT RCT positive result) there is a suggestion that there remains a 42% probability of a clinically meaningful decrease in CV risk with colchicine, a 58% probability of clinical equivalence with placebo and no probability of increased cardiovascular outcomes.\nMany clinicians do seem overly influenced by the last trial, particularly if they were intimately involved in it. This of course does not imply that I would prescribe colchicine for this condition as any decision should also consider cost, other side effects, and patient preferences incuding there personalized choices about what represents a clinically meaningful thresholds. For the current PI to now be a disbeliever, perhaps his cutpoint for efficacy was RR &lt; 0.80 and indeed this posterior probability is indeed very small. However this would imply that his prior efficacy belief based on COLCOT should also be based on the a probability of RR &lt; 0.80 which was only 60%. This seems a fairly modest probability to have been a strong “believer” in this therapy before the current study. The reconciliation of these different beliefs is somewhat difficult. My main point is dichotomizing beliefs is about as useful as the dichotomization of p values according to &lt; or &gt; 0.05,IOW not very useful at all. Bayesian reasoning can help avoid the cognitive difficulties that are often present in interpreting RCTs, especially in the context of previous knowledge."
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html",
    "href": "upload/libs/reveal.js/examples/markdown.html",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 1.1\nNote: This will only appear in the speaker notes window.\n\n\n\nContent 1.2\n\n\n\nContent 2.1\n\n\n\nContent 3.1\n\n\n\nContent 3.2\n\n\n\n\n\n\nExternal Image\n\n\n\n\n\nExternal Image"
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html#external-1.1",
    "href": "upload/libs/reveal.js/examples/markdown.html#external-1.1",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 1.1\nNote: This will only appear in the speaker notes window."
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html#external-1.2",
    "href": "upload/libs/reveal.js/examples/markdown.html#external-1.2",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 1.2"
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html#external-2",
    "href": "upload/libs/reveal.js/examples/markdown.html#external-2",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 2.1"
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html#external-3.1",
    "href": "upload/libs/reveal.js/examples/markdown.html#external-3.1",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 3.1"
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html#external-3.2",
    "href": "upload/libs/reveal.js/examples/markdown.html#external-3.2",
    "title": "Markdown Demo",
    "section": "",
    "text": "Content 3.2"
  },
  {
    "objectID": "upload/libs/reveal.js/examples/markdown.html#external-3.3",
    "href": "upload/libs/reveal.js/examples/markdown.html#external-3.3",
    "title": "Markdown Demo",
    "section": "",
    "text": "External Image\n\n\n\n\n\nExternal Image"
  },
  {
    "objectID": "upload/libs/reveal.js/test/simple.html",
    "href": "upload/libs/reveal.js/test/simple.html",
    "title": "jay's website",
    "section": "",
    "text": "var a = 1;"
  },
  {
    "objectID": "upload/libs/reveal.js/test/simple.html#slide-1.1",
    "href": "upload/libs/reveal.js/test/simple.html#slide-1.1",
    "title": "jay's website",
    "section": "",
    "text": "var a = 1;"
  },
  {
    "objectID": "upload/libs/reveal.js/test/simple.html#slide-1.2",
    "href": "upload/libs/reveal.js/test/simple.html#slide-1.2",
    "title": "jay's website",
    "section": "Slide 1.2",
    "text": "Slide 1.2"
  },
  {
    "objectID": "upload/libs/reveal.js/test/simple.html#slide-2",
    "href": "upload/libs/reveal.js/test/simple.html#slide-2",
    "title": "jay's website",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "upload/libs/reveal.js/CONTRIBUTING.html",
    "href": "upload/libs/reveal.js/CONTRIBUTING.html",
    "title": "jay's website",
    "section": "",
    "text": "Please keep the issue tracker limited to bug reports, feature requests and pull requests.\n\n\nIf you have personal support or setup questions the best place to ask those are StackOverflow.\n\n\n\nWhen reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.\n\n\n\n\nShould follow the coding style of the file you work in, most importantly:\n\nTabs to indent\nSingle-quoted strings\n\nShould be made towards the dev branch\nShould be submitted from a feature/topic branch (not your master)\n\n\n\n\nPlease do not submit plugins as pull requests. They should be maintained in their own separate repository. More information here: https://github.com/hakimel/reveal.js/wiki/Plugin-Guidelines"
  },
  {
    "objectID": "upload/libs/reveal.js/CONTRIBUTING.html#contributing",
    "href": "upload/libs/reveal.js/CONTRIBUTING.html#contributing",
    "title": "jay's website",
    "section": "",
    "text": "Please keep the issue tracker limited to bug reports, feature requests and pull requests.\n\n\nIf you have personal support or setup questions the best place to ask those are StackOverflow.\n\n\n\nWhen reporting a bug make sure to include information about which browser and operating system you are on as well as the necessary steps to reproduce the issue. If possible please include a link to a sample presentation where the bug can be tested.\n\n\n\n\nShould follow the coding style of the file you work in, most importantly:\n\nTabs to indent\nSingle-quoted strings\n\nShould be made towards the dev branch\nShould be submitted from a feature/topic branch (not your master)\n\n\n\n\nPlease do not submit plugins as pull requests. They should be maintained in their own separate repository. More information here: https://github.com/hakimel/reveal.js/wiki/Plugin-Guidelines"
  },
  {
    "objectID": "posts/2024-12-30-Statistical_models/index.html",
    "href": "posts/2024-12-30-Statistical_models/index.html",
    "title": "A reanalysis of PLATO",
    "section": "",
    "text": "PLATO NEJM 2009 was a multinational RCT of antiplatelets, ticagrelor vs. clopidogrel published in 2009(Wallentin et al. 2009) and has had &gt; 5,000 citations. The primary end point was a composite of death from vascular causes, myocardial infarction, or stroke — occurred in 9.8% of ticagrelor subjects vs. 11.7% of clopidogrel subjects (HR, 0.84; 95% CI 0.77 to 0.92; P&lt;0.001). \nThere have been several publications questioning the integrity of the data and in December the BMJ published an excellent summary of these issues here.  \nI think there are some pertinent statistical issues which deserve further comment.  \nThe FDA initially refused approval despite PLATO’s HR, 0.84; 95% CI 0.77 to 0.92; P&lt;0.001 with concerns were centered on data integrity and the discordance between the overall and US results, \\(HR_{US}\\) 1.27, 95%CI 0.92 - 1.75. One year later with no change in the dataset, the FDA approved the drug. Apparently corporate reassurance prevailed and instead of looking at subgroups, the FDA accepted the pooled results presented in the NEJM publication1. Pooling the data implies that all ancillary post MI care is identical whether the patient was randomized in Bulgaria, South Africa or any of the other 41 participating countries. This is a strong assumption. Ignoring clustering within specific geographic countries or regions will underestimation the sampling errors resulting in falsely precise confidence intervals. On the other hand, examining only the US subgroup is wasteful as it excludes about 90% of the randomized patient data.\nHierarchical modeling, a compromise between and the no and complete pooling, with their optimal statistical properties(Gelman 2006) offers a sortie from this quandary. Individual country / region effects are shrunken towards the global mean and their variance is improved by this borrowing of information from other regions. For example, this model gives a \\(HR_{US}\\) 1.05 95% CIs 0.79-1.35. As predicted, the regional US estimate is shrunk towards the global mean with improved precision due to this borrowing of information. However, the overall summary mean will have wider confidence intervals (\\(HR_{mean}\\) 0.89 95% CIs 0.72-1.10) as this now includes both within and between region sampling variations. The next, as yet unperformed, study(Riley, Higgins, and Deeks, n.d.) is predicted to fall in the 95% CI 0.46 – 1.37. (see figure)\n\n\n\n\n\n\n\n\nIn other words, even accepting the PLATO data as is, simply by choosing a more realistic statistical model with a better estimate of the underlying the uncertainty we see that conventional statistical significance for ticagrelor is not met. Ignoring this uncertainty leads to publications and guidelines proclaiming ticagrelor superiority, when a more reasonable interpretation is that while there appears to be a positive ticagrelor signal this needs confirmation in further studies, especially for US patients. Interestingly, this second US study was never done. Moreover, later randomized trials have not been kind to ticagrelor. An Asian population who for genetic reasons were believed to have inferior clopidogrel activation showed not the expected improved but worse outcomes with ticagrelor (HR, 1.47; 95% CI: 0.88-2.44)(Goto et al., n.d.). Another RCT of elderly Dutch patients also has showed worse outcome with ticagrelor compared clopidogrel (absolute risk difference -4%, 95% CI -10.0 to 1.4; p=0.03 for non-inferiority)(Gimbel et al., n.d.).\nThus, even ignoring the questionable data integrity, ticagrelor superiority in PLATO is seen to be attributed to the choice of statistical model, specifically one that ignores a large portion of the study variability. While one may choose to reject the proposed hierarchical model as a better alternative, there should be a consensus that when conclusions differ drastically according to the choice of statistical model, the data can hardly be considered robust.\nHow then did ticagrelor attain its overwhelming supremacy? As I was once offered $750 by the ticagrelor sponsor to attend a dinner extolling PLATO’s benefits by a leading writer of the Canadian guidelines (I declined), one must wonder about the role of marketing in overcoming any scientific shortcomings.\nThese observations have been published as a rapid response letter in the BMJ"
  },
  {
    "objectID": "posts/2024-12-30-Statistical_models/index.html#plato-rct",
    "href": "posts/2024-12-30-Statistical_models/index.html#plato-rct",
    "title": "A reanalysis of PLATO",
    "section": "",
    "text": "PLATO NEJM 2009 was a multinational RCT of antiplatelets, ticagrelor vs. clopidogrel published in 2009(Wallentin et al. 2009) and has had &gt; 5,000 citations. The primary end point was a composite of death from vascular causes, myocardial infarction, or stroke — occurred in 9.8% of ticagrelor subjects vs. 11.7% of clopidogrel subjects (HR, 0.84; 95% CI 0.77 to 0.92; P&lt;0.001). \nThere have been several publications questioning the integrity of the data and in December the BMJ published an excellent summary of these issues here.  \nI think there are some pertinent statistical issues which deserve further comment.  \nThe FDA initially refused approval despite PLATO’s HR, 0.84; 95% CI 0.77 to 0.92; P&lt;0.001 with concerns were centered on data integrity and the discordance between the overall and US results, \\(HR_{US}\\) 1.27, 95%CI 0.92 - 1.75. One year later with no change in the dataset, the FDA approved the drug. Apparently corporate reassurance prevailed and instead of looking at subgroups, the FDA accepted the pooled results presented in the NEJM publication1. Pooling the data implies that all ancillary post MI care is identical whether the patient was randomized in Bulgaria, South Africa or any of the other 41 participating countries. This is a strong assumption. Ignoring clustering within specific geographic countries or regions will underestimation the sampling errors resulting in falsely precise confidence intervals. On the other hand, examining only the US subgroup is wasteful as it excludes about 90% of the randomized patient data.\nHierarchical modeling, a compromise between and the no and complete pooling, with their optimal statistical properties(Gelman 2006) offers a sortie from this quandary. Individual country / region effects are shrunken towards the global mean and their variance is improved by this borrowing of information from other regions. For example, this model gives a HRUS 1.05 95% CIs 0.79-1.35. As predicted, the regional US estimate is shrunk towards the global mean with improved precision due to this borrowing of information. However, the overall summary mean will have wider confidence intervals (HRmean 0.90 95% CIs 0.72-1.10) as this now includes both within and between region sampling variations. The next, as yet unperformed, study(Riley, Higgins, and Deeks, n.d.) is predicted to fall in the 95% CI 0.46 – 1.37. (see figure)\n\n\n\n\n\n\n\n\nIn other words, even accepting the PLATO data as is, simply by choosing a more realistic statistical model with a better estimate of the underlying the uncertainty we see that conventional statistical significance for ticagrelor is not met. Ignoring this uncertainty leads to publications and guidelines proclaiming ticagrelor superiority, when a more reasonable interpretation is that while there appears to be a positive ticagrelor signal this needs confirmation in further studies, especially for US patients. Interestingly, this second US study was never done. Moreover, later randomized trials have not been kind to ticagrelor. An Asian population who for genetic reasons were believed to have inferior clopidogrel activation showed not the expected improved but worse outcomes with ticagrelor (HR, 1.47; 95% CI: 0.88-2.44)(Goto et al., n.d.). Another RCT of elderly Dutch patients also has showed worse outcome with ticagrelor compared clopidogrel (absolute risk difference -4%, 95% CI -10.0 to 1.4; p=0.03 for non-inferiority)(Gimbel et al., n.d.).\nThus, even ignoring the questionable data integrity, ticagrelor superiority in PLATO is seen to be attributed to the choice of statistical model, specifically one that ignores a large portion of the study variability. While one may choose to reject the proposed hierarchical model as a better alternative, there should be a consensus that when conclusions differ drastically according to the choice of statistical model, the data can hardly be considered robust.\nHow then did ticagrelor attain its overwhelming supremacy? As I was once offered $750 by the ticagrelor sponsor to attend a dinner extolling PLATO’s benefits by a leading writer of the Canadian guidelines (I declined), one must wonder about the role of marketing in overcoming any scientific shortcomings.\nReferences"
  },
  {
    "objectID": "posts/2024-12-30-Statistical_models/index.html#plato",
    "href": "posts/2024-12-30-Statistical_models/index.html#plato",
    "title": "A reanalysis of PLATO",
    "section": "",
    "text": "PLATO NEJM 2009 was a multinational RCT of antiplatelets, ticagrelor vs. clopidogrel published in 2009(Wallentin et al. 2009) and has had &gt; 5,000 citations. The primary end point was a composite of death from vascular causes, myocardial infarction, or stroke — occurred in 9.8% of ticagrelor subjects vs. 11.7% of clopidogrel subjects (HR, 0.84; 95% CI 0.77 to 0.92; P&lt;0.001). \nThere have been several publications questioning the integrity of the data and in December the BMJ published an excellent summary of these issues here.  \nI think there are some pertinent statistical issues which deserve further comment.  \nThe FDA initially refused approval despite PLATO’s HR, 0.84; 95% CI 0.77 to 0.92; P&lt;0.001 with concerns were centered on data integrity and the discordance between the overall and US results, \\(HR_{US}\\) 1.27, 95%CI 0.92 - 1.75. One year later with no change in the dataset, the FDA approved the drug. Apparently corporate reassurance prevailed and instead of looking at subgroups, the FDA accepted the pooled results presented in the NEJM publication1. Pooling the data implies that all ancillary post MI care is identical whether the patient was randomized in Bulgaria, South Africa or any of the other 41 participating countries. This is a strong assumption. Ignoring clustering within specific geographic countries or regions will underestimation the sampling errors resulting in falsely precise confidence intervals. On the other hand, examining only the US subgroup is wasteful as it excludes about 90% of the randomized patient data.\nHierarchical modeling, a compromise between and the no and complete pooling, with their optimal statistical properties(Gelman 2006) offers a sortie from this quandary. Individual country / region effects are shrunken towards the global mean and their variance is improved by this borrowing of information from other regions. For example, this model gives a \\(HR_{US}\\) 1.05 95% CIs 0.79-1.35. As predicted, the regional US estimate is shrunk towards the global mean with improved precision due to this borrowing of information. However, the overall summary mean will have wider confidence intervals (\\(HR_{mean}\\) 0.89 95% CIs 0.72-1.10) as this now includes both within and between region sampling variations. The next, as yet unperformed, study(Riley, Higgins, and Deeks, n.d.) is predicted to fall in the 95% CI 0.46 – 1.37. (see figure)\n\n\n\n\n\n\n\n\nIn other words, even accepting the PLATO data as is, simply by choosing a more realistic statistical model with a better estimate of the underlying the uncertainty we see that conventional statistical significance for ticagrelor is not met. Ignoring this uncertainty leads to publications and guidelines proclaiming ticagrelor superiority, when a more reasonable interpretation is that while there appears to be a positive ticagrelor signal this needs confirmation in further studies, especially for US patients. Interestingly, this second US study was never done. Moreover, later randomized trials have not been kind to ticagrelor. An Asian population who for genetic reasons were believed to have inferior clopidogrel activation showed not the expected improved but worse outcomes with ticagrelor (HR, 1.47; 95% CI: 0.88-2.44)(Goto et al., n.d.). Another RCT of elderly Dutch patients also has showed worse outcome with ticagrelor compared clopidogrel (absolute risk difference -4%, 95% CI -10.0 to 1.4; p=0.03 for non-inferiority)(Gimbel et al., n.d.).\nThus, even ignoring the questionable data integrity, ticagrelor superiority in PLATO is seen to be attributed to the choice of statistical model, specifically one that ignores a large portion of the study variability. While one may choose to reject the proposed hierarchical model as a better alternative, there should be a consensus that when conclusions differ drastically according to the choice of statistical model, the data can hardly be considered robust.\nHow then did ticagrelor attain its overwhelming supremacy? As I was once offered $750 by the ticagrelor sponsor to attend a dinner extolling PLATO’s benefits by a leading writer of the Canadian guidelines (I declined), one must wonder about the role of marketing in overcoming any scientific shortcomings.\nThese observations have been published as a rapid response letter in the BMJ"
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "",
    "text": "In the CLEAR OASIS 9 trial acute MI patients were randomized to colchicine (n = 3,528) or placebo (n = 3,534) right after percutaneous coronary intervention (PCI).\nThe primary outcome of major adverse CV events (MACE), composite of CV death, MI, stroke, or ischemia-driven revascularization, for colchicine vs. placebo at 5 years, was: 9.1% vs. 9.3%, hazard ratio (HR) 0.99 (95% confidence interval [CI] 0.85-1.16), p = 0.93.\nSecondary outcomes for colchicine vs. placebo at 5 years:\nCV death: 3.3% vs. 3.2%, HR 1.03 (95% CI 0.80-1.34)\nAll-cause death: 4.6% vs. 5.1%, HR 0.90 (95% CI 0.73-1.12)\nMI: 2.9% vs. 3.1%, HR 0.88 (95% CI 0.66-1.17)\nIschemia-driven revascularization: 4.6% vs. 4.7%, HR 1.01 (95% CI 0.81-1.17)\nLSM C-reactive protein (CRP) levels at 3 months: 3.0 vs. 4.3 mg/dL, p &lt; 0.001\n\nShow the codelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(lme4)\n\n\nThere was an interesting quote from the principal investigator (PI) who said that before this study ““I was a believer in colchicine” but not a believer after the study. This dichotomization of beliefs is very common among physicians, perhaps influenced by the null hypothesis significance testing paradigm and the almighty p value that remain the cornerstone of most medical research.\nLet’s take a look at this from a probabilistic viewpoint."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html#most-recent-trial",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html#most-recent-trial",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "",
    "text": "In the CLEAR OASIS 9 trial acute MI patients were randomized to colchicine (n = 3,528) or placebo (n = 3,534) right after percutaneous coronary intervention (PCI).\nThe primary outcome of major adverse CV events (MACE), composite of CV death, MI, stroke, or ischemia-driven revascularization, for colchicine vs. placebo at 5 years, was: 9.1% vs. 9.3%, hazard ratio (HR) 0.99 (95% confidence interval [CI] 0.85-1.16), p = 0.93.\nSecondary outcomes for colchicine vs. placebo at 5 years:\nCV death: 3.3% vs. 3.2%, HR 1.03 (95% CI 0.80-1.34)\nAll-cause death: 4.6% vs. 5.1%, HR 0.90 (95% CI 0.73-1.12)\nMI: 2.9% vs. 3.1%, HR 0.88 (95% CI 0.66-1.17)\nIschemia-driven revascularization: 4.6% vs. 4.7%, HR 1.01 (95% CI 0.81-1.17)\nLSM C-reactive protein (CRP) levels at 3 months: 3.0 vs. 4.3 mg/dL, p &lt; 0.001\n\nShow the codelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(lme4)\n\n\nThere was an interesting quote from the principal investigator (PI) who said that before this study ““I was a believer in colchicine” but not a believer after the study. This dichotomization of beliefs is very common among physicians, perhaps influenced by the null hypothesis significance testing paradigm and the almighty p value that remain the cornerstone of most medical research.\nLet’s take a look at this from a probabilistic viewpoint."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html#what-is-the-prior-evidence",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html#what-is-the-prior-evidence",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "What is the prior evidence?",
    "text": "What is the prior evidence?\nThe PI stated before his trial was completed he was a “believer”, then what was his evidence to support this belief. Perhaps his belief was founded on the on the results from the COLCOT study where patients were randomized to the same dose of colchicine (n=2366) or placebo (n=2379) a mean of 13.5 days after their acute MI. The primary efficacy end point was a composite of death from cardiovascular causes, resuscitated cardiac arrest, myocardial infarction, stroke, or urgent hospitalization for angina leading to coronary revascularization. The primary end point occurred in 5.5% of the patients in the colchicine group, as compared with 7.1% of those in the placebo group (hazard ratio, 0.77; 95% confidence interval [CI], 0.61 to 0.96; P=0.02).\nPresumably others didn’t quite share his belief or the necessary equipoise would not have been present to proceed with the CLEAR OASIS 9 trial. A presentation of these results from a probabilistic viewpoint may help understand these discrepancies."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html#bayesian-view-of-the-prior-evidence",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html#bayesian-view-of-the-prior-evidence",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "Bayesian view of the prior evidence",
    "text": "Bayesian view of the prior evidence\nLets’ first write some Stan code for a two arm binomial trial with vague priors\n\nShow the code# stan code for 2 arm binomial\nstancode &lt;- '\ndata {\n  int&lt;lower=0&gt; n1;  // Total number of trials in the placebo arm\n  int&lt;lower=0&gt; y1;  // Number of successes in the placebo arm\n  int&lt;lower=0&gt; n2;  // Total number of trials in the intervention arm\n  int&lt;lower=0&gt; y2;  // Number of successes in the intervention arm\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p1; // Probability of success in the placebo arm\n  real&lt;lower=0, upper=1&gt; p2; // Probability of success in the intervention arm\n}\n\nmodel {\n  p1 ~ beta(1, 1); // Default prior for p1\n  p2 ~ beta(1, 1); // Default prior for p2\n\n  y1 ~ binomial(n1, p1); // Likelihood for the placebo arm\n  y2 ~ binomial(n2, p2); // Likelihood for the intervention arm\n}\n\ngenerated quantities {\n  real rr = p2 / p1;  // Relative Risk of intervention arm over placebo arm\n}\n'\nwriteLines(stancode, \"binom_2.stan\")\n\n# check file exists\n# file.exists(\"binom_2.stan\")\n# cat(readLines(\"binom_2.stan\"), sep = \"\\n\")\n\n\nNow will execute this program with the COLCOT data in order to fully appreciate the entire posterior probability distribution as opposed to be a limted fixed viewpoint of an uninteresting null hypothesis as is the conventional statistical approach.\n\nShow the code# Data for the COLCOT model\ndata_list &lt;- list(\n  n2 = 2366,\n  y2 = as.integer(2366 * 0.055),\n  n1 = 2379,\n  y1 = as.integer(2379 * 0.071)\n)\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nShow the code# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\n# mcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1117.      -1.12e+3 1.07    0.756   -1.12e+3 -1.12e+3  1.00    1916.\n2 p1           0.0711   7.09e-2 0.00535 0.00521  6.26e-2  8.03e-2  1.00    3368.\n3 p2           0.0553   5.51e-2 0.00477 0.00476  4.76e-2  6.35e-2  1.00    2737.\n4 rr           0.782    7.79e-1 0.0897  0.0864   6.43e-1  9.35e-1  1.00    3179.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nShow the code# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\nprob_rr_less_08 &lt;- mean(rr_vector &lt; 0.8)\nprob_rr_less_10 &lt;- mean(rr_vector &lt; 1.0)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 1.0 (P (statistical signifiance)): \", prob_rr_less_10, \n    \"\\nProbability RR &lt; 0.8: \", prob_rr_less_08, \n    \"\\nProbability RR &lt; 0.9: \", prob_rr_less_09,\n    \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \n    \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 1.0 (P (statistical signifiance)):  0.9865 \nProbability RR &lt; 0.8:  0.5985 \nProbability RR &lt; 0.9:  0.89975 \nProbability RR 0.9 to 1.1:  0.0985 \nProbability RR &gt; 1.1:  0.00175 \n\n\nThis may also be appreciated graphically\n\nShow the code# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on COLCOT\",\n       subtitle = \"with a vague prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.64, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.79, y = max(density_df$y, na.rm = TRUE) * 0.73, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.9, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.05, y = max(density_df$y, na.rm = TRUE) * 0.53, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.15, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.10, y = max(density_df$y, na.rm = TRUE) * 0.35, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThis figure explains why there may be different beliefs about the colchicine effect with some being enthusiastic about a 90% probability of a clinical benefit (assuming a 10% reduction in RR is an appropriate clinical threshold). Conversely others may be more conservative and want a larger reduction in RR, given the inconvenience and costs of taking another medication. The probability of an at least 20% reduction is only 60% underscoring the need for some to await further studies to better define any clinical benefits."
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html#bayesian-approach-with-stan",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html#bayesian-approach-with-stan",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "Bayesian approach with Stan",
    "text": "Bayesian approach with Stan\nFor the current CLEAR OASIS results, initially let’s assume a vague priors so that the posterior probability distribution is completely dominated by the current data. We will estimate the probabilities for each arm and their relative risk (RR). From the RR posterior probability density one can further calculate probabilities for various cutpoints. For example, one might assume that we are particularly interested in probabilities that exceed a clinically meaningful benefit or harm. These cutpoints can be individually chosen but let’s again assume a benefit threshold of RR &lt; 0.9 and harm threshold of RR &gt; 1.1.\n\nShow the codelibrary(cmdstanr)\nlibrary(bayesplot)\n\n\n# Data for the model\ndata_list &lt;- list(\n  n2 = 3528,\n  y2 = as.integer(3528 * 0.091),\n  n1 = 3534,\n  y1 = as.integer(3534 * 0.093)\n)\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nShow the code# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\nmcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n\n\n\n\n\nShow the code# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -2173.      -2.17e+3 0.998   0.719   -2.18e+3 -2.17e+3  1.00    1715.\n2 p1           0.0931   9.29e-2 0.00487 0.00494  8.52e-2  1.01e-1  1.00    3788.\n3 p2           0.0914   9.13e-2 0.00488 0.00484  8.36e-2  9.95e-2  1.00    2400.\n4 rr           0.985    9.82e-1 0.0734  0.0729   8.67e-1  1.11e+0  1.00    2532.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nShow the code# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 0.9: \", prob_rr_less_09, \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 0.9:  0.122 \nProbability RR 0.9 to 1.1:  0.8125 \nProbability RR &gt; 1.1:  0.0655"
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html#graphical-presentation",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html#graphical-presentation",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "Graphical presentation",
    "text": "Graphical presentation\nUnderstanding these results is again facilitated with graphical presentations\n\nShow the code# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on CLEAR OASIS\",\n       subtitle = \"with a vague prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.80, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.89, y = max(density_df$y, na.rm = TRUE) * 0.57, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.8, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = .92, y = max(density_df$y, na.rm = TRUE) * 0.77, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.1, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.19, y = max(density_df$y, na.rm = TRUE) * 0.37, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThis analysis shows that there is only a small probability (12%) of a clinically significant benefit (where this has been arbitrarily defined as a minimum 10% decrease in RR). There is a 7% of clinical harm (RR &gt;1.1) and a 81% probability of practical equivalence. Recall that this analysis examines only the current study but the lead PI claimed he was a priori a “believer” so to be consistent with this prior belief, this posterior belief based on a vague prior should be replaced with an informative prior belief that is updated with this new data. This infromative prior is combined with the current data following the laws of probability, i.e. a weighted average according with weights proportional to the precision of the prior and current data\n\nShow the code# stan code for 2 arm binomial with informative prior\nstancode &lt;- '\ndata {\n  int&lt;lower=0&gt; n1;  // Total number of trials in the placebo arm\n  int&lt;lower=0&gt; y1;  // Number of successes in the placebo arm\n  int&lt;lower=0&gt; n2;  // Total number of trials in the intervention arm\n  int&lt;lower=0&gt; y2;  // Number of successes in the intervention arm\n  real&lt;lower=0&gt; alpha1; // Alpha for beta prior on p1\n  real&lt;lower=0&gt; beta1;  // Beta for beta prior on p1\n  real&lt;lower=0&gt; alpha2; // Alpha for beta prior on p2\n  real&lt;lower=0&gt; beta2;  // Beta for beta prior on p2\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p1; // Probability of success in the placebo arm\n  real&lt;lower=0, upper=1&gt; p2; // Probability of success in the intervention arm\n}\n\nmodel {\n  p1 ~ beta(alpha1, beta1); // Informative prior for p1 based on COLCOT\n  p2 ~ beta(alpha2, beta2); // Informative prior for p2 based on COLCOT\n\n  y1 ~ binomial(n1, p1); // Likelihood for the placebo arm\n  y2 ~ binomial(n2, p2); // Likelihood for the intervention arm\n}\n\ngenerated quantities {\n  real rr = p2 / p1;  // Relative Risk of intervention arm over placebo arm\n}'\nwriteLines(stancode, \"binom_2_priorCOLCOT.stan\")\n# file.exists(\"binom_2_priorCOLCOT.stan\")\n\n\n\nShow the code# prior from COLCOT\n\nalpha1 = as.integer(2379 * 0.071)\nbeta1 &lt;- 2379 - alpha1\nalpha2 = as.integer(2366 * 0.055)\nbeta2 &lt;- 2366 - alpha2\n\n# Data for the model\ndata_list &lt;- list(\n  n1 = 3534,\n  y1 = as.integer(3534 * 0.093),\n  n2 = 3528,\n  y2 = as.integer(3528 * 0.091),\n  alpha1 = alpha1,\n  beta1 = beta1,\n  alpha2 = alpha2,\n  beta2 = beta2\n)\n\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2_priorCOLCOT.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nShow the code# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\nmcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n\n\n\n\n\nShow the code# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -3297.      -3.30e+3 1.02    0.749   -3.30e+3 -3.30e+3  1.00    1896.\n2 p1           0.0839   8.38e-2 0.00370 0.00367  7.80e-2  9.01e-2  1.00    3545.\n3 p2           0.0765   7.65e-2 0.00349 0.00341  7.08e-2  8.22e-2  1.00    3455.\n4 rr           0.913    9.13e-1 0.0576  0.0590   8.21e-1  1.01e+0  1.00    3283.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nShow the code# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 0.9: \", prob_rr_less_09, \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 0.9:  0.41925 \nProbability RR 0.9 to 1.1:  0.5795 \nProbability RR &gt; 1.1:  0.00125"
  },
  {
    "objectID": "posts/2024-11-04-Personal_beliefs/Colchicine.html#graphical-presentation-1",
    "href": "posts/2024-11-04-Personal_beliefs/Colchicine.html#graphical-presentation-1",
    "title": "Personal beliefs - An example from Colchicine - CLEAR OASIS",
    "section": "Graphical presentation",
    "text": "Graphical presentation\nUnderstanding these results is facilitated with graphical presentations\n\nShow the code# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on CLEAR OASIS\",\n       subtitle = \"with an informative (COLCOT) prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.80, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.86, y = max(density_df$y, na.rm = TRUE) * 0.56, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.8, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = .90, y = max(density_df$y, na.rm = TRUE) * 0.76, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.00, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.07, y = max(density_df$y, na.rm = TRUE) * 0.33, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThe PI who was a “believer” prior to his study now claims after the study to be a non-believer (to the extent that he apparently stopped the medication that was previously prescribed to his father). Yet based on his positive prior beliefs (if these can be approximated by the COLCOT RCT positive result) there is a suggestion that there remains a 42% probability of a clinically meaningful decrease in CV risk with colchicine, a 58% probability of clinical equivalence with placebo and no probability of increased cardiovascular outcomes.\nMany clinicians do seem overly influenced by the last trial, particularly if they were intimately involved in it. This of course does not imply that I would prescribe colchicine for this condition as any decision should also consider cost, other side effects, and patient preferences incuding there personalized choices about what represents a clinically meaningful thresholds. For the current PI to now be a disbeliever, perhaps his cutpoint for efficacy was RR &lt; 0.80 and indeed this posterior probability is indeed very small. However this would imply that his prior efficacy belief based on COLCOT should also be based on the a probability of RR &lt; 0.80 which was only 60%. This seems a fairly modest probability to have been a strong “believer” in this therapy before the current study. The reconciliation of these different beliefs is somewhat difficult. My main point is dichotomizing beliefs is about as useful as the dichotomization of p values according to &lt; or &gt; 0.05,IOW not very useful at all.. Bayesian reasoning can help avoid the cognitive difficulties that are often present in interpreting recent RCTs, especially in the context of previous knowledge."
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html",
    "href": "posts/2024-12-29-Personal_beliefs/index.html",
    "title": "Personal beliefs",
    "section": "",
    "text": "In the CLEAR OASIS 9 trial acute MI patients were randomized to colchicine (n = 3,528) or placebo (n = 3,534) right after percutaneous coronary intervention (PCI).\nThe primary outcome of major adverse CV events (MACE), composite of CV death, MI, stroke, or ischemia-driven revascularization, for colchicine vs. placebo at 5 years, was: 9.1% vs. 9.3%, hazard ratio (HR) 0.99 (95% confidence interval [CI] 0.85-1.16), p = 0.93.\nSecondary outcomes for colchicine vs. placebo at 5 years:\nCV death: 3.3% vs. 3.2%, HR 1.03 (95% CI 0.80-1.34)\nAll-cause death: 4.6% vs. 5.1%, HR 0.90 (95% CI 0.73-1.12)\nMI: 2.9% vs. 3.1%, HR 0.88 (95% CI 0.66-1.17)\nIschemia-driven revascularization: 4.6% vs. 4.7%, HR 1.01 (95% CI 0.81-1.17)\nLSM C-reactive protein (CRP) levels at 3 months: 3.0 vs. 4.3 mg/dL, p &lt; 0.001\n\nCodelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(lme4)\n\n\nThere was an interesting quote from the principal investigator (PI) who said that before this study ““I was a believer in colchicine” but not a believer after the study. This dichotomization of beliefs is very common among physicians, perhaps influenced by the null hypothesis significance testing paradigm and the almighty p value that remain the cornerstone of most medical research.\nLet’s take a look at this from a probabilistic viewpoint."
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html#most-recent-trial",
    "href": "posts/2024-12-29-Personal_beliefs/index.html#most-recent-trial",
    "title": "Personal beliefs",
    "section": "",
    "text": "In the CLEAR OASIS 9 trial acute MI patients were randomized to colchicine (n = 3,528) or placebo (n = 3,534) right after percutaneous coronary intervention (PCI).\nThe primary outcome of major adverse CV events (MACE), composite of CV death, MI, stroke, or ischemia-driven revascularization, for colchicine vs. placebo at 5 years, was: 9.1% vs. 9.3%, hazard ratio (HR) 0.99 (95% confidence interval [CI] 0.85-1.16), p = 0.93.\nSecondary outcomes for colchicine vs. placebo at 5 years:\nCV death: 3.3% vs. 3.2%, HR 1.03 (95% CI 0.80-1.34)\nAll-cause death: 4.6% vs. 5.1%, HR 0.90 (95% CI 0.73-1.12)\nMI: 2.9% vs. 3.1%, HR 0.88 (95% CI 0.66-1.17)\nIschemia-driven revascularization: 4.6% vs. 4.7%, HR 1.01 (95% CI 0.81-1.17)\nLSM C-reactive protein (CRP) levels at 3 months: 3.0 vs. 4.3 mg/dL, p &lt; 0.001\n\nCodelibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(lme4)\n\n\nThere was an interesting quote from the principal investigator (PI) who said that before this study ““I was a believer in colchicine” but not a believer after the study. This dichotomization of beliefs is very common among physicians, perhaps influenced by the null hypothesis significance testing paradigm and the almighty p value that remain the cornerstone of most medical research.\nLet’s take a look at this from a probabilistic viewpoint."
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html#what-is-the-prior-evidence",
    "href": "posts/2024-12-29-Personal_beliefs/index.html#what-is-the-prior-evidence",
    "title": "Personal beliefs",
    "section": "What is the prior evidence?",
    "text": "What is the prior evidence?\nThe PI stated before his trial was completed he was a “believer”, then what was his evidence to support this belief. Perhaps his belief was founded on the on the results from the COLCOT study where patients were randomized to the same dose of colchicine (n=2366) or placebo (n=2379) a mean of 13.5 days after their acute MI. The primary efficacy end point was a composite of death from cardiovascular causes, resuscitated cardiac arrest, myocardial infarction, stroke, or urgent hospitalization for angina leading to coronary revascularization. The primary end point occurred in 5.5% of the patients in the colchicine group, as compared with 7.1% of those in the placebo group (hazard ratio, 0.77; 95% confidence interval [CI], 0.61 to 0.96; P=0.02).\nPresumably others didn’t quite share his belief or the necessary equipoise would not have been present to proceed with the CLEAR OASIS 9 trial. A presentation of these results from a probabilistic viewpoint may help understand these discrepancies."
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html#bayesian-view-of-the-prior-evidence",
    "href": "posts/2024-12-29-Personal_beliefs/index.html#bayesian-view-of-the-prior-evidence",
    "title": "Personal beliefs",
    "section": "Bayesian view of the prior evidence",
    "text": "Bayesian view of the prior evidence\nLets’ first write some Stan code for a two arm binomial trial with vague priors\n\nCode# stan code for 2 arm binomial\nstancode &lt;- '\ndata {\n  int&lt;lower=0&gt; n1;  // Total number of trials in the placebo arm\n  int&lt;lower=0&gt; y1;  // Number of successes in the placebo arm\n  int&lt;lower=0&gt; n2;  // Total number of trials in the intervention arm\n  int&lt;lower=0&gt; y2;  // Number of successes in the intervention arm\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p1; // Probability of success in the placebo arm\n  real&lt;lower=0, upper=1&gt; p2; // Probability of success in the intervention arm\n}\n\nmodel {\n  p1 ~ beta(1, 1); // Default prior for p1\n  p2 ~ beta(1, 1); // Default prior for p2\n\n  y1 ~ binomial(n1, p1); // Likelihood for the placebo arm\n  y2 ~ binomial(n2, p2); // Likelihood for the intervention arm\n}\n\ngenerated quantities {\n  real rr = p2 / p1;  // Relative Risk of intervention arm over placebo arm\n}\n'\nwriteLines(stancode, \"binom_2.stan\")\n\n# check file exists\n# file.exists(\"binom_2.stan\")\n# cat(readLines(\"binom_2.stan\"), sep = \"\\n\")\n\n\nNow will execute this program with the COLCOT data in order to fully appreciate the entire posterior probability distribution as opposed to be a limted fixed viewpoint of an uninteresting null hypothesis as is the conventional statistical approach.\n\nCode# Data for the COLCOT model\ndata_list &lt;- list(\n  n2 = 2366,\n  y2 = as.integer(2366 * 0.055),\n  n1 = 2379,\n  y1 = as.integer(2379 * 0.071)\n)\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nCode# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\n# mcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1117.      -1.12e+3 1.07    0.756   -1.12e+3 -1.12e+3  1.00    1916.\n2 p1           0.0711   7.09e-2 0.00535 0.00521  6.26e-2  8.03e-2  1.00    3368.\n3 p2           0.0553   5.51e-2 0.00477 0.00476  4.76e-2  6.35e-2  1.00    2737.\n4 rr           0.782    7.79e-1 0.0897  0.0864   6.43e-1  9.35e-1  1.00    3179.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nCode# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\nprob_rr_less_08 &lt;- mean(rr_vector &lt; 0.8)\nprob_rr_less_10 &lt;- mean(rr_vector &lt; 1.0)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 1.0 (P (statistical signifiance)): \", prob_rr_less_10, \n    \"\\nProbability RR &lt; 0.8: \", prob_rr_less_08, \n    \"\\nProbability RR &lt; 0.9: \", prob_rr_less_09,\n    \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \n    \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 1.0 (P (statistical signifiance)):  0.99 \nProbability RR &lt; 0.8:  0.6 \nProbability RR &lt; 0.9:  0.9 \nProbability RR 0.9 to 1.1:  0.099 \nProbability RR &gt; 1.1:  0.0018 \n\n\nThis may also be appreciated graphically\n\nCode# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on COLCOT\",\n       subtitle = \"with a vague prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.64, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.79, y = max(density_df$y, na.rm = TRUE) * 0.73, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.9, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.05, y = max(density_df$y, na.rm = TRUE) * 0.53, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.15, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.10, y = max(density_df$y, na.rm = TRUE) * 0.35, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThis figure explains why there may be different beliefs about the colchicine effect with some being enthusiastic about a 90% probability of a clinical benefit (assuming a 10% reduction in RR is an appropriate clinical threshold). Conversely others may be more conservative and want a larger reduction in RR, given the inconvenience and costs of taking another medication. The probability of an at least 20% reduction is only 60% underscoring the need for some to await further studies to better define any clinical benefits."
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html#bayesian-approach-with-stan",
    "href": "posts/2024-12-29-Personal_beliefs/index.html#bayesian-approach-with-stan",
    "title": "Personal beliefs",
    "section": "Bayesian approach with Stan",
    "text": "Bayesian approach with Stan\nFor the current CLEAR OASIS results, initially let’s assume a vague priors so that the posterior probability distribution is completely dominated by the current data. We will estimate the probabilities for each arm and their relative risk (RR). From the RR posterior probability density one can further calculate probabilities for various cutpoints. For example, one might assume that we are particularly interested in probabilities that exceed a clinically meaningful benefit or harm. These cutpoints can be individually chosen but let’s again assume a benefit threshold of RR &lt; 0.9 and harm threshold of RR &gt; 1.1.\n\nCode# Data for the model\ndata_list &lt;- list(\n  n2 = 3528,\n  y2 = as.integer(3528 * 0.091),\n  n1 = 3534,\n  y1 = as.integer(3534 * 0.093)\n)\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nCode# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\nmcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n\n\n\n\n\nCode# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -2173.      -2.17e+3 1.04    0.726   -2.18e+3 -2.17e+3  1.00    1907.\n2 p1           0.0930   9.29e-2 0.00490 0.00474  8.51e-2  1.01e-1  1.00    3010.\n3 p2           0.0912   9.11e-2 0.00492 0.00479  8.35e-2  9.95e-2  1.00    3217.\n4 rr           0.983    9.80e-1 0.0741  0.0727   8.67e-1  1.11e+0  1.00    3183.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nCode# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 0.9: \", prob_rr_less_09, \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 0.9:  0.12 \nProbability RR 0.9 to 1.1:  0.81 \nProbability RR &gt; 1.1:  0.064"
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html#graphical-presentation",
    "href": "posts/2024-12-29-Personal_beliefs/index.html#graphical-presentation",
    "title": "Personal beliefs",
    "section": "Graphical presentation",
    "text": "Graphical presentation\nUnderstanding these results is again facilitated with graphical presentations\n\nCode# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on CLEAR OASIS\",\n       subtitle = \"with a vague prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.80, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.89, y = max(density_df$y, na.rm = TRUE) * 0.57, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.8, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = .92, y = max(density_df$y, na.rm = TRUE) * 0.77, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.1, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.19, y = max(density_df$y, na.rm = TRUE) * 0.37, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThis analysis shows that there is only a small probability (12%) of a clinically significant benefit (where this has been arbitrarily defined as a minimum 10% decrease in RR). There is a 7% of clinical harm (RR &gt;1.1) and a 81% probability of practical equivalence. Recall that this analysis examines only the current study but the lead PI claimed he was a priori a “believer” so to be consistent with this prior belief, this posterior belief based on a vague prior should be replaced with an informative prior belief that is updated with this new data. This infromative prior is combined with the current data following the laws of probability, i.e. a weighted average according with weights proportional to the precision of the prior and current data\n\nCode# stan code for 2 arm binomial with informative prior\nstancode &lt;- '\ndata {\n  int&lt;lower=0&gt; n1;  // Total number of trials in the placebo arm\n  int&lt;lower=0&gt; y1;  // Number of successes in the placebo arm\n  int&lt;lower=0&gt; n2;  // Total number of trials in the intervention arm\n  int&lt;lower=0&gt; y2;  // Number of successes in the intervention arm\n  real&lt;lower=0&gt; alpha1; // Alpha for beta prior on p1\n  real&lt;lower=0&gt; beta1;  // Beta for beta prior on p1\n  real&lt;lower=0&gt; alpha2; // Alpha for beta prior on p2\n  real&lt;lower=0&gt; beta2;  // Beta for beta prior on p2\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; p1; // Probability of success in the placebo arm\n  real&lt;lower=0, upper=1&gt; p2; // Probability of success in the intervention arm\n}\n\nmodel {\n  p1 ~ beta(alpha1, beta1); // Informative prior for p1 based on COLCOT\n  p2 ~ beta(alpha2, beta2); // Informative prior for p2 based on COLCOT\n\n  y1 ~ binomial(n1, p1); // Likelihood for the placebo arm\n  y2 ~ binomial(n2, p2); // Likelihood for the intervention arm\n}\n\ngenerated quantities {\n  real rr = p2 / p1;  // Relative Risk of intervention arm over placebo arm\n}'\nwriteLines(stancode, \"binom_2_priorCOLCOT.stan\")\n# file.exists(\"binom_2_priorCOLCOT.stan\")\n\n\n\nCode# prior from COLCOT\n\nalpha1 = as.integer(2379 * 0.071)\nbeta1 &lt;- 2379 - alpha1\nalpha2 = as.integer(2366 * 0.055)\nbeta2 &lt;- 2366 - alpha2\n\n# Data for the model\ndata_list &lt;- list(\n  n1 = 3534,\n  y1 = as.integer(3534 * 0.093),\n  n2 = 3528,\n  y2 = as.integer(3528 * 0.091),\n  alpha1 = alpha1,\n  beta1 = beta1,\n  alpha2 = alpha2,\n  beta2 = beta2\n)\n\n\n# Compile and fit the model\nmod &lt;- cmdstan_model(\"binom_2_priorCOLCOT.stan\")\nfit &lt;- mod$sample(data = data_list, chains = 4, parallel_chains = 4,        refresh = 0, seed = 123)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nCode# Extracting posterior samples\nposterior_samples &lt;- fit$draws()\n\n# Plotting\ncolor_scheme_set(\"blue\")\nmcmc_trace(posterior_samples, pars = c(\"p1\", \"p2\", \"rr\"), nrow = 3)\n\n\n\n\n\n\nCode# Plot relative risk distributions with no formatting\n# mcmc_areas(posterior_samples, pars = \"rr\", prob = 0.95)\n\n# print summary\nfit$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -3297.      -3.30e+3 0.980   0.667   -3.30e+3 -3.30e+3  1.00    1915.\n2 p1           0.0839   8.38e-2 0.00355 0.00348  7.81e-2  8.97e-2  1.00    3516.\n3 p2           0.0765   7.65e-2 0.00343 0.00349  7.09e-2  8.23e-2  1.00    3237.\n4 rr           0.914    9.13e-1 0.0567  0.0576   8.23e-1  1.01e+0  1.00    3281.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nCode# Correct extraction of relative risk samples\nrr_samples &lt;- fit$draws(variables = \"rr\")\nrr_vector &lt;- as.vector(rr_samples)  # Convert to a simple vector for easier handling\n\n# Calculating probabilities\nprob_rr_less_09 &lt;- mean(rr_vector &lt; 0.9)\nprob_rr_between_09_11 &lt;- mean(rr_vector &gt;= 0.9 & rr_vector &lt;= 1.1)\nprob_rr_greater_11 &lt;- mean(rr_vector &gt; 1.1)\n\n# Print the probabilities\ncat(\"Probability RR &lt; 0.9: \", prob_rr_less_09, \"\\nProbability RR 0.9 to 1.1: \", prob_rr_between_09_11, \"\\nProbability RR &gt; 1.1: \", prob_rr_greater_11, \"\\n\")\n\nProbability RR &lt; 0.9:  0.41 \nProbability RR 0.9 to 1.1:  0.59 \nProbability RR &gt; 1.1:  0.0013"
  },
  {
    "objectID": "posts/2024-12-29-Personal_beliefs/index.html#graphical-presentation-1",
    "href": "posts/2024-12-29-Personal_beliefs/index.html#graphical-presentation-1",
    "title": "Personal beliefs",
    "section": "Graphical presentation",
    "text": "Graphical presentation\nUnderstanding these results is facilitated with graphical presentations\n\nCode# Assuming rr_vector is already defined\nd &lt;- density(rr_vector)\ndensity_df &lt;- data.frame(x = d$x, y = d$y)\n\n# Assign categories based on the x values\ndensity_df$category &lt;- ifelse(density_df$x &lt; 0.9, \"benefit\",\n                              ifelse(density_df$x &lt;= 1.1, \"equivalence\", \"harm\"))\n\np &lt;- ggplot(density_df, aes(x = x, y = y)) +\n  geom_area(data = subset(density_df, category == \"benefit\"), fill = \"green\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"equivalence\"), fill = \"yellow\", alpha = 0.5) +\n  geom_area(data = subset(density_df, category == \"harm\"), fill = \"red\", alpha = 0.5) +\n  geom_vline(xintercept = 0.9, linetype = \"dashed\", color = \"black\") +\n  geom_vline(xintercept = 1.1, linetype = \"dashed\", color = \"black\") +\n  labs(title=\"Density Plot of Relative Risk based on CLEAR OASIS\",\n       subtitle = \"with an informative (COLCOT) prior\") +\n  theme_classic() +\n  scale_x_continuous(expand = c(0, 0)) +  # No expansion on x-axis\n  scale_y_continuous(expand = c(0, 0)) +  # No expansion on y-axis\n  xlab(\"Relative Risk\") +\n  ylab(\"Density\")\n\np &lt;- p + \n  annotate(\"text\", x = 0.80, y = max(density_df$y, na.rm = TRUE) * 0.6, label = \"Green\", color = \"green\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0.86, y = max(density_df$y, na.rm = TRUE) * 0.56, label = \" = region of clinical benefit\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 0.8, y = max(density_df$y, na.rm = TRUE) * 0.8, label = \"Yellow\", color = \"yellow\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = .90, y = max(density_df$y, na.rm = TRUE) * 0.76, label = \" = region of practical equivalence\", color = \"black\", size = 5) +\n  annotate(\"text\", x = 1.00, y = max(density_df$y, na.rm = TRUE) * 0.4, label = \"Red\", color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.07, y = max(density_df$y, na.rm = TRUE) * 0.33, label = \" = region of clinical harm\", color = \"black\", size = 5)\n\np\n\n\n\n\n\n\n\nThe PI who was a “believer” prior to his study now claims after the study to be a non-believer (to the extent that he apparently stopped the medication that was previously prescribed to his father). Yet based on his positive prior beliefs (if these can be approximated by the COLCOT RCT positive result) there is a suggestion that there remains a 42% probability of a clinically meaningful decrease in CV risk with colchicine, a 58% probability of clinical equivalence with placebo and no probability of increased cardiovascular outcomes.\nMany clinicians do seem overly influenced by the last trial, particularly if they were intimately involved in it. This of course does not imply that I would prescribe colchicine for this condition as any decision should also consider cost, other side effects, and patient preferences incuding there personalized choices about what represents a clinically meaningful thresholds. For the current PI to now be a disbeliever, perhaps his cutpoint for efficacy was RR &lt; 0.80 and indeed this posterior probability is indeed very small. However this would imply that his prior efficacy belief based on COLCOT should also be based on the a probability of RR &lt; 0.80 which was only 60%. This seems a fairly modest probability to have been a strong “believer” in this therapy before the current study. The reconciliation of these different beliefs is somewhat difficult. My main point is dichotomizing beliefs is about as useful as the dichotomization of p values according to &lt; or &gt; 0.05,IOW not very useful at all. Bayesian reasoning can help avoid the cognitive difficulties that are often present in interpreting RCTs, especially in the context of previous knowledge."
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html",
    "href": "posts/2025-01-27-Small_print/index.html",
    "title": "Small print",
    "section": "",
    "text": "I’m teaching a seminar course to our PhD students and last week someone made mention of the small print in scientific publications and this reminded me of an old study with some interesting small print that I never bothered to write up, as it had no chance of ever being published at the time. Self help books tell us Don’t sweat the small stuff and it’s all small stuff. In the scientific field that advice is problematic for several reasons  \n\nOften we don’t know what is truly small stuff\n\nThe devil may well be in the details\n\nSmall stuff can have a huge impact on data interpretation, inferences, and policy decisions"
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html#background",
    "href": "posts/2025-01-27-Small_print/index.html#background",
    "title": "Small print",
    "section": "",
    "text": "I’m teaching a seminar course to our PhD students and last week someone made mention of the small print in scientific publications and this reminded me of an old study with some interesting small print that I never bothered to write up, as it had no chance of ever being published at the time. Self help books tell us Don’t sweat the small stuff and it’s all small stuff. In the scientific field that advice is problematic for several reasons  \n\nOften we don’t know what is truly small stuff\n\nThe devil may well be in the details\n\nSmall stuff can have a huge impact on data interpretation, inferences, and policy decisions"
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html#the-study-in-question",
    "href": "posts/2025-01-27-Small_print/index.html#the-study-in-question",
    "title": "Small print",
    "section": "The study in question",
    "text": "The study in question\nIn 2001, one of the leading cardiovascular specialty journals published a study entitled “Cardiovascular thrombotic events in controlled, clinical trials of rofecoxib”(Konstam et al. 2001). This was a meta-analysis of 23 RCTs examining the drug rofecoxib (Vioxx) and the abstract (“The BIG PRINT”) is shown here"
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html#the-results-more-big-print",
    "href": "posts/2025-01-27-Small_print/index.html#the-results-more-big-print",
    "title": "Small print",
    "section": "The results (more BIG PRINT)",
    "text": "The results (more BIG PRINT)"
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html#small-print",
    "href": "posts/2025-01-27-Small_print/index.html#small-print",
    "title": "Small print",
    "section": "Small print",
    "text": "Small print\nThere is some revealing information in the small footnote that accompanied the publication and is reproduced below  \n All authors either were employees or consultants of the drug manufacturer. Moreover the stdy was fast tracked (before journals were even formally doing fast tracking) with 1 day between submission and acceptance! The study appeared online 12 days after submission. The backdrop for this rapidity was the publication in JAMA at the end of August 2001 looking at the 3 published rofecoxib RCTs that concluded there was an increased CV risk. This footnote certainly raises disconcerting flags mandating a closer look at the results and conclusions, which, as shown next, were hardly found to be reassuring."
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html#interpretative-problems",
    "href": "posts/2025-01-27-Small_print/index.html#interpretative-problems",
    "title": "Small print",
    "section": "Interpretative problems",
    "text": "Interpretative problems\n\n20 of the 23 trials were unpublished  \n\nThe interpetation of the forest plot is incorrect. Consider the rofecoxib versus placebo comparison the OR = 0.84, 95%CI 0.51 - 1.38. The correct interpretation is not that there is no evidence for an excess of CV events but rather the study is underpowered as the data is compatible with sampling errors of 49% reduction up to a 38% increase in events. As the magnitude of these possibilities are certainly clinically meaningful the result should be interpreted as inconclusive. Similar comments apply to the rofecoxib versus non-naproxen NSAIDs. Remember “absence of evidence is not evidence of absence”(Altman and Bland 1995) \n\nThe biggest interpretative error is in the 3rd comparison where these apologists claim the statistically significant difference between rofecoxib and naproxen were not due to a danger with rofecoxib but rather to the beneficiasl antiplatelt effects of naproxen.  \n\nWhat is the evidence of these naproxen benefits? Well there are no references in the paper that support this statement and the reason for that lack is shown in this PubMed search done at the time of this publication"
  },
  {
    "objectID": "posts/2025-01-27-Small_print/index.html#the-follow-up",
    "href": "posts/2025-01-27-Small_print/index.html#the-follow-up",
    "title": "Small print",
    "section": "The follow-up",
    "text": "The follow-up\nRofecoxib stayed on the market until Sept 2004 when the manufacturer voluntarily withdrew it. A report from the FDA suggested that “An estimated 88 000-140 000 excess cases of serious coronary heart disease probably occurred in the United States over the market life of rofecoxib. The US national estimate of the case-fatality rate (fatal acute myocardial infarction plus sudden cardiac death) was 44%, which suggests that many of the excess cases attributable to rofecoxib use were fatal.”  \nReturning to the original topic of “small print”, the footnote on page 1 of the paper with the 1 day timeline of submission and acceptance and the extensive conflicts of interest (COI) was a red flag as to possible quality issues. While the COI among the authors is nothing new, the actions of the journal were more surprising. An examination of the full paper certainly confirmed these initial suspicions of highly suspect quality."
  },
  {
    "objectID": "posts/2021-10-04-new-covid-drug/index.html#update-22025",
    "href": "posts/2021-10-04-new-covid-drug/index.html#update-22025",
    "title": "New Covid drug",
    "section": "Update 2/2025",
    "text": "Update 2/2025\nMOVe-OUT was published in the NEJM  \n \n \nInterestingly, later RCTs comparing molnupiravir tp placebo, PANORAMIC(n &gt; 25000), MOVe-AHEAD(n &gt; 1500), showed no benefit for their primary outcome measures!"
  },
  {
    "objectID": "posts/2025-04-04-Checklists/index.html",
    "href": "posts/2025-04-04-Checklists/index.html",
    "title": "Collider risk stratification bias",
    "section": "",
    "text": "Smoking is a well-established risk factor for endothelial damage and increased thrombogenesis, yet Presch(Presch et al. 2025) reports paradoxically that smoking is associated with a lower risk of repeat revascularization. It is hard to conceive why the endothelium would react differently if it had experienced a previous revascularization procedure. More likely the observed protective role is due to collider risk stratification bias which has successfully explained the obesity, aspirin, and previous smoking paradoxes among many others[Etminan et al. (2021)](Banack and Kaufman 2014)(Choi et al. 2014). This bias arises when study selection (in this case undergoing revascularization) is a collider, affected by both smoking and other (unmeasured) risk factors, creating a spurious association via an induced back door pathway. Study selection here was determined by the performance of revascularization which is not randomly assigned. Smokers are more likely to undergo initial revascularization and therefore non-smokers who receive revascularization tend to have substantially higher levels of other (unmeasured) risk factors, strong predictors for future revascularization. A simulation study assigning a smoking HR = 1.36 is reduced to 0.87 with 95% CI (0.82 to 0.92) when the general population is conditioned on the performance of an initial revascularization (the collider)(Brophy 2025). This reversal is a classic example of collider bias, where conditioning on a collider variable, influenced by both the exposure and an unmeasured confounder distorts the true effect."
  },
  {
    "objectID": "posts/2025-04-04-Checklists/index.html#recent-study",
    "href": "posts/2025-04-04-Checklists/index.html#recent-study",
    "title": "Collider risk stratification bias",
    "section": "",
    "text": "Smoking is a well-established risk factor for endothelial damage and increased thrombogenesis, yet Presch(Presch et al. 2025) reports paradoxically that smoking is associated with a lower risk of repeat revascularization. It is hard to conceive why the endothelium would react differently if it had experienced a previous revascularization procedure. More likely the observed protective role is due to collider risk stratification bias which has successfully explained the obesity, aspirin, and previous smoking paradoxes among many others[Etminan et al. (2021)](Banack and Kaufman 2014)(Choi et al. 2014). This bias arises when study selection (in this case undergoing revascularization) is a collider, affected by both smoking and other (unmeasured) risk factors, creating a spurious association via an induced back door pathway. Study selection here was determined by the performance of revascularization which is not randomly assigned. Smokers are more likely to undergo initial revascularization and therefore non-smokers who receive revascularization tend to have substantially higher levels of other (unmeasured) risk factors, strong predictors for future revascularization. A simulation study assigning a smoking HR = 1.36 is reduced to 0.87 with 95% CI (0.82 to 0.92) when the general population is conditioned on the performance of an initial revascularization (the collider)(Brophy 2025). This reversal is a classic example of collider bias, where conditioning on a collider variable, influenced by both the exposure and an unmeasured confounder distorts the true effect."
  },
  {
    "objectID": "posts/2025-04-04-Checklists/index.html#background-on-collider-risk-stratification",
    "href": "posts/2025-04-04-Checklists/index.html#background-on-collider-risk-stratification",
    "title": "Collider risk stratification bias",
    "section": "Background on collider risk stratification",
    "text": "Background on collider risk stratification\nConsider a simplified data generating mechanism (DGM) for the development of coronary artery disease, reequiring revascularizaion, in the general population. Let’s assume that causal factors are smoking and all other risk factors are combined into one other variable. The causal diagram is therefore as follows\n\nCodelibrary(ggplot2)\nlibrary(ggdag)\n\nrevasc_dag &lt;- collider_triangle(\n  x = \"Smoking\",\n  y = \"Other (unmeasured) risk factors\",\n  m = \"Revascularization\"\n)\n\nggdag(revasc_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\nHere for the general population we have assumed that smoking and the other risk factors are independent (no connecting arrow). But look what happens if we select (stratify or condition) on receiving a revascularization procedure.\n\nCodeimg &lt;- ggdag_dseparated(revasc_dag,\n                 controlling_for = \"m\",\n                 text = FALSE, use_labels = \"label\"\n)\nimg\n\n\n\n\n\n\nCodeggsave(\"images/collider/preview-image.jpg\")\n\nSaving 7 x 5 in image\n\n\nThis has induced a relationship between smoking and the other risk factors. If the patient is a non-smoker then they must have other risk factors since they required a revascularization procedure. This means that in general that amount the sampe population of those have received an initial revascularization procedure, the non-smokers ill be sicker than the smokers. Alternatively the smokers will look to be protected by their smoking even though the underlying data generating mechaism (with an increased risk due to smoking) has not been altered. \nCollider-stratification bias is responsible for many cases of bias, including selection bias, missing data, and publication bias."
  },
  {
    "objectID": "posts/2025-04-04-Checklists/index.html#simulated-data",
    "href": "posts/2025-04-04-Checklists/index.html#simulated-data",
    "title": "Collider risk stratification bias",
    "section": "Simulated data",
    "text": "Simulated data\nTo concretely demonstrate this spurious association we will perform a numerical simulation. Assume 10,000 individuals with a 30% smoking prevalence and a normal disribution of the other (unmeasured) risk factors, (U). The probability of receiving a percutaneous coronary intervention (PCI) is modeled using a logistic model where both smoking and U increase the chance of PCI.\n\nCode# Set a seed for reproducibility\noptions(digits = 2)\n# ---------------------------\n# Chunk 1: Generate Full Population Data\n# ---------------------------\nset.seed(123)\n\n# Population parameters for outcome data generating model (DGM):\nn &lt;- 100000               # Total individuals\np_smoking &lt;- 0.3          # Smoking prevalence\nbeta_smoking &lt;- 0.3       # True effect of smoking (log-odds); OR ≈ exp(0.3) ≈ 1.35\nbeta_U &lt;- 1               # Effect of U on outcome\nintercept_outcome &lt;- -2   # Outcome intercept for repeat PCI\n\n# Simulate predictors:\nsmoking &lt;- rbinom(n, 1, p_smoking)\nU &lt;- rnorm(n)             # Unobserved risk factor\n\n# Generate outcome (repeat PCI) for full population using the same DGM:\nlogit_outcome &lt;- intercept_outcome + beta_smoking * smoking + beta_U * U\np_outcome &lt;- exp(logit_outcome) / (1 + exp(logit_outcome))\nrepeat_PCI &lt;- rbinom(n, 1, p_outcome)\n\n# ---------------------------\n# Chunk 2: Generate PCI via a Deterministic (Threshold) Rule\n# ---------------------------\n# Define PCI as follows:\n#   For smokers (smoking == 1): PCI = 1 if U &gt; 0.5.\n#   For non-smokers (smoking == 0): PCI = 1 if U &gt; 1.5.\nPCI &lt;- ifelse(smoking == 1, as.numeric(U &gt; 0.5), as.numeric(U &gt; 1.1))\n\n# Combine into one data frame:\ndf &lt;- data.frame(smoking, U, PCI, repeat_PCI)\n\n# ---------------------------\n# Chunk 3: Full Population Analysis\n# ---------------------------\n# Fit a logistic regression for repeat PCI on smoking (ignoring U) in the full population:\nmodel_full &lt;- glm(repeat_PCI ~ smoking, data = df, family = binomial)\ntemp &lt;- summary(model_full)\nsummary(model_full)\n\n\nCall:\nglm(formula = repeat_PCI ~ smoking, family = binomial, data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.6896     0.0104  -162.2   &lt;2e-16 ***\nsmoking       0.2717     0.0179    15.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 90396  on 99999  degrees of freedom\nResidual deviance: 90171  on 99998  degrees of freedom\nAIC: 90175\n\nNumber of Fisher Scoring iterations: 4\n\nCode# exp(coef(model_full))  # Display odds ratios\n\nsmoke_est &lt;- exp(temp$coefficients[2,1])\nsmoke_CI &lt;-exp(temp$coefficients[2,1] + c(-1,1)*1.96*temp$coefficients[2,2])\n\n\nThis confirms the DGM is performing as expected with an increased smoking risk = 1.31 with 95% CI (1.27 to 1.36)  Note in this illustrative case, a deterministic model for PCI with non-smokers having higher (non-measured) risk factors has been used. In the real world this world, a probabilistic mnodel would be employed\n \nNow let’s see the effect of smoking on the study population, i.e. only those that underwent an initial revascularization procedure. Here we assume the same DGM for a future repeat PCI as initially applied to the general population i.e. smoking risk = 1.3\n\nCode# ---------------------------\n# Chunk 4: Analysis in PCI-Selected Sample\n# ---------------------------\n# Restrict the data to individuals with PCI == 1:\ndata_PCI &lt;- subset(df, PCI == 1)\n\n# Fit the logistic regression model for repeat PCI on smoking in the PCI-selected sample:\nmodel_PCI &lt;- glm(repeat_PCI ~ smoking, data = data_PCI, family = binomial)\ntemp1 &lt;- summary(model_PCI)\nsummary(model_PCI)\n\n\nCall:\nglm(formula = repeat_PCI ~ smoking, family = binomial, data = data_PCI)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.3680     0.0206  -17.84  &lt; 2e-16 ***\nsmoking      -0.1367     0.0298   -4.58  4.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 25340  on 18905  degrees of freedom\nResidual deviance: 25319  on 18904  degrees of freedom\nAIC: 25323\n\nNumber of Fisher Scoring iterations: 4\n\nCode# exp(coef(model_PCI))  # Display odds ratios in the selected sample\n\n\nsmoke_est_sample &lt;- exp(temp1$coefficients[2,1])\nsmoke_CI_sample &lt;- exp(temp1$coefficients[2,1] + c(-1,1)*1.96*temp1$coefficients[2,2])\n\n\nUsing the exact same data generating mechanism, with increased risk with smoking (RR =1.3) for a repeat revascularization procedure, the smoking parameter in the study sample has been reduced to 0.87 with 95% CI (0.82 to 0.92) due to collider risk stratification bias  \nThis simulation illustrates collider-stratification bias: even though the true effect of smoking in the full population is harmful (OR ≈ 1.35) and remains harmful for a repeat procedure, when you condition on PCI smoking looks protective simply because the non-smokers are otherwise at increased risk due to the spurious induced associated between the other (unmeasured) risk factors and smoking."
  },
  {
    "objectID": "posts/2025-04-04-Checklists/index.html#visual-interpretation",
    "href": "posts/2025-04-04-Checklists/index.html#visual-interpretation",
    "title": "Collider risk stratification bias",
    "section": "Visual interpretation",
    "text": "Visual interpretation\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodelibrary(patchwork)  # for side-by-side plots\n\n# Assume df and data_PCI already exist, with columns:\n#   U: unobserved risk factor\n#   smoking: 0/1\n#   PCI: 0/1\n#   repeat_PCI: outcome\n\n# Create a \"Group\" variable to label each dataset\ndf$Group &lt;- \"Full Population\"\ndata_PCI$Group &lt;- \"PCI Selected\"\n\n# Plot 1: Full population\np1 &lt;- ggplot(filter(df, Group == \"Full Population\"),\n             aes(x = U, fill = factor(smoking))) +\n  geom_density(alpha = 0.5) +\n  coord_cartesian(xlim = c(-3, 3)) +  # adjust as needed\n  labs(\n    title = \"Distribution of U in the Full Population\",\n    x = \"U (Unobserved Risk Factor)\",\n    y = \"Density\",\n    fill = \"Smoking\\n(0=No,1=Yes)\"\n  ) +\n  theme_minimal()\n\n# Plot 2: PCI-selected, x-axis starting at 0\np2 &lt;- ggplot(filter(data_PCI, Group == \"PCI Selected\"),\n             aes(x = U, fill = factor(smoking))) +\n  geom_density(alpha = 0.5) +\n  coord_cartesian(xlim = c(0, NA)) +  # start x-axis at 0\n  labs(\n    title = \"Distribution of U in PCI-Selected Sample\",\n    x = \"U (Unobserved Risk Factor)\",\n    y = \"Density\",\n    fill = \"Smoking\\n(0=No,1=Yes)\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\np1 + p2\n\n\n\n\n\n\n\nFull Population (left plot) shows how the unobserved risk factor U is distributed equally among both smokers (red) and non-smokers (blue) in the entire dataset. in th estudy sample, PCI-selected sample (right plot) because of the selection mechanism you can more clearly see that non-smokers in the PCI sample have U-values &gt; the values in smokers. Hence the false impression that smoking is protective."
  },
  {
    "objectID": "posts/2025-04-04-Collider/index.html",
    "href": "posts/2025-04-04-Collider/index.html",
    "title": "Collider risk stratification bias",
    "section": "",
    "text": "Smoking is a well-established risk factor for endothelial damage and increased thrombogenesis, yet Presch(Presch et al. 2025) reports paradoxically that smoking is associated with a lower risk of repeat revascularization. It is hard to conceive why the endothelium would react differently if it had experienced a previous revascularization procedure. More likely the observed protective role is due to collider risk stratification bias which has successfully explained the obesity, aspirin, and previous smoking paradoxes among many others[Etminan et al. (2021)](Banack and Kaufman 2014)(Choi et al. 2014). This bias arises when study selection (in this case undergoing revascularization) is a collider, affected by both smoking and other (unmeasured) risk factors, creating a spurious association via an induced back door pathway. Study selection here was determined by the performance of revascularization which is not randomly assigned. Smokers are more likely to undergo initial revascularization and therefore non-smokers who receive revascularization tend to have substantially higher levels of other (unmeasured) risk factors, strong predictors for future revascularization. A simulation study assigning a smoking HR = 1.36 is reduced to 0.87 with 95% CI (0.82 to 0.92) when the general population is conditioned on the performance of an initial revascularization (the collider)(Brophy 2025). This reversal is a classic example of collider bias, where conditioning on a collider variable, influenced by both the exposure and an unmeasured confounder distorts the true effect."
  },
  {
    "objectID": "posts/2025-04-04-Collider/index.html#recent-study",
    "href": "posts/2025-04-04-Collider/index.html#recent-study",
    "title": "Collider risk stratification bias",
    "section": "",
    "text": "Smoking is a well-established risk factor for endothelial damage and increased thrombogenesis, yet Presch(Presch et al. 2025) reports paradoxically that smoking is associated with a lower risk of repeat revascularization. It is hard to conceive why the endothelium would react differently if it had experienced a previous revascularization procedure. More likely the observed protective role is due to collider risk stratification bias which has successfully explained the obesity, aspirin, and previous smoking paradoxes among many others[Etminan et al. (2021)](Banack and Kaufman 2014)(Choi et al. 2014). This bias arises when study selection (in this case undergoing revascularization) is a collider, affected by both smoking and other (unmeasured) risk factors, creating a spurious association via an induced back door pathway. Study selection here was determined by the performance of revascularization which is not randomly assigned. Smokers are more likely to undergo initial revascularization and therefore non-smokers who receive revascularization tend to have substantially higher levels of other (unmeasured) risk factors, strong predictors for future revascularization. A simulation study assigning a smoking HR = 1.36 is reduced to 0.87 with 95% CI (0.82 to 0.92) when the general population is conditioned on the performance of an initial revascularization (the collider)(Brophy 2025). This reversal is a classic example of collider bias, where conditioning on a collider variable, influenced by both the exposure and an unmeasured confounder distorts the true effect."
  },
  {
    "objectID": "posts/2025-04-04-Collider/index.html#background-on-collider-risk-stratification",
    "href": "posts/2025-04-04-Collider/index.html#background-on-collider-risk-stratification",
    "title": "Collider risk stratification bias",
    "section": "Background on collider risk stratification",
    "text": "Background on collider risk stratification\nConsider a simplified data generating mechanism (DGM) for the development of coronary artery disease, reequiring revascularizaion, in the general population. Let’s assume that causal factors are smoking and all other risk factors are combined into one other variable. The causal diagram is therefore as follows\n\nCodelibrary(ggplot2)\nlibrary(ggdag)\n\nrevasc_dag &lt;- collider_triangle(\n  x = \"Smoking\",\n  y = \"Other (unmeasured) risk factors\",\n  m = \"Revascularization\"\n)\n\nggdag(revasc_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\nHere for the general population we have assumed that smoking and the other risk factors are independent (no connecting arrow). But look what happens if we select (stratify or condition) on receiving a revascularization procedure.\n\nCodeimg &lt;- ggdag_dseparated(revasc_dag,\n                 controlling_for = \"m\",\n                 text = FALSE, use_labels = \"label\"\n)\nimg\n\n\n\n\n\n\nCodeggsave(\"images/collider/preview-image.jpg\")\n\n\nThis has induced a relationship between smoking and the other risk factors. If the patient is a non-smoker then they must have other risk factors since they required a revascularization procedure. This means that in general that amount the sampe population of those have received an initial revascularization procedure, the non-smokers ill be sicker than the smokers. Alternatively the smokers will look to be protected by their smoking even though the underlying data generating mechaism (with an increased risk due to smoking) has not been altered. \nCollider-stratification bias is responsible for many cases of bias, including selection bias, missing data, and publication bias."
  },
  {
    "objectID": "posts/2025-04-04-Collider/index.html#simulated-data",
    "href": "posts/2025-04-04-Collider/index.html#simulated-data",
    "title": "Collider risk stratification bias",
    "section": "Simulated data",
    "text": "Simulated data\nTo concretely demonstrate this spurious association we will perform a numerical simulation. Assume 10,000 individuals with a 30% smoking prevalence and a normal disribution of the other (unmeasured) risk factors, (U). The probability of receiving a percutaneous coronary intervention (PCI) is modeled using a logistic model where both smoking and U increase the chance of PCI.\n\nCode# Set a seed for reproducibility\noptions(digits = 2)\n# ---------------------------\n# Chunk 1: Generate Full Population Data\n# ---------------------------\nset.seed(123)\n\n# Population parameters for outcome data generating model (DGM):\nn &lt;- 100000               # Total individuals\np_smoking &lt;- 0.3          # Smoking prevalence\nbeta_smoking &lt;- 0.3       # True effect of smoking (log-odds); OR ≈ exp(0.3) ≈ 1.35\nbeta_U &lt;- 1               # Effect of U on outcome\nintercept_outcome &lt;- -2   # Outcome intercept for repeat PCI\n\n# Simulate predictors:\nsmoking &lt;- rbinom(n, 1, p_smoking)\nU &lt;- rnorm(n)             # Unobserved risk factor\n\n# Generate outcome (repeat PCI) for full population using the same DGM:\nlogit_outcome &lt;- intercept_outcome + beta_smoking * smoking + beta_U * U\np_outcome &lt;- exp(logit_outcome) / (1 + exp(logit_outcome))\nrepeat_PCI &lt;- rbinom(n, 1, p_outcome)\n\n# ---------------------------\n# Chunk 2: Generate PCI via a Deterministic (Threshold) Rule\n# ---------------------------\n# Define PCI as follows:\n#   For smokers (smoking == 1): PCI = 1 if U &gt; 0.5.\n#   For non-smokers (smoking == 0): PCI = 1 if U &gt; 1.5.\nPCI &lt;- ifelse(smoking == 1, as.numeric(U &gt; 0.5), as.numeric(U &gt; 1.1))\n\n# Combine into one data frame:\ndf &lt;- data.frame(smoking, U, PCI, repeat_PCI)\n\n# ---------------------------\n# Chunk 3: Full Population Analysis\n# ---------------------------\n# Fit a logistic regression for repeat PCI on smoking (ignoring U) in the full population:\nmodel_full &lt;- glm(repeat_PCI ~ smoking, data = df, family = binomial)\ntemp &lt;- summary(model_full)\nsummary(model_full)\n\n\nCall:\nglm(formula = repeat_PCI ~ smoking, family = binomial, data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.6896     0.0104  -162.2   &lt;2e-16 ***\nsmoking       0.2717     0.0179    15.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 90396  on 99999  degrees of freedom\nResidual deviance: 90171  on 99998  degrees of freedom\nAIC: 90175\n\nNumber of Fisher Scoring iterations: 4\n\nCode# exp(coef(model_full))  # Display odds ratios\n\nsmoke_est &lt;- exp(temp$coefficients[2,1])\nsmoke_CI &lt;-exp(temp$coefficients[2,1] + c(-1,1)*1.96*temp$coefficients[2,2])\n\n\nThis confirms the DGM is performing as expected with an increased smoking risk = 1.31 with 95% CI (1.27 to 1.36)  Note in this illustrative case, a deterministic model for PCI with non-smokers having higher (non-measured) risk factors has been used. In the real world this world, a probabilistic mnodel would be employed\n \nNow let’s see the effect of smoking on the study population, i.e. only those that underwent an initial revascularization procedure. Here we assume the same DGM for a future repeat PCI as initially applied to the general population i.e. smoking risk = 1.3\n\nCode# ---------------------------\n# Chunk 4: Analysis in PCI-Selected Sample\n# ---------------------------\n# Restrict the data to individuals with PCI == 1:\ndata_PCI &lt;- subset(df, PCI == 1)\n\n# Fit the logistic regression model for repeat PCI on smoking in the PCI-selected sample:\nmodel_PCI &lt;- glm(repeat_PCI ~ smoking, data = data_PCI, family = binomial)\ntemp1 &lt;- summary(model_PCI)\nsummary(model_PCI)\n\n\nCall:\nglm(formula = repeat_PCI ~ smoking, family = binomial, data = data_PCI)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.3680     0.0206  -17.84  &lt; 2e-16 ***\nsmoking      -0.1367     0.0298   -4.58  4.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 25340  on 18905  degrees of freedom\nResidual deviance: 25319  on 18904  degrees of freedom\nAIC: 25323\n\nNumber of Fisher Scoring iterations: 4\n\nCode# exp(coef(model_PCI))  # Display odds ratios in the selected sample\n\n\nsmoke_est_sample &lt;- exp(temp1$coefficients[2,1])\nsmoke_CI_sample &lt;- exp(temp1$coefficients[2,1] + c(-1,1)*1.96*temp1$coefficients[2,2])\n\n\nUsing the exact same data generating mechanism, with increased risk with smoking (RR =1.3) for a repeat revascularization procedure, the smoking parameter in the study sample has been reduced to 0.87 with 95% CI (0.82 to 0.92) due to collider risk stratification bias  \nThis simulation illustrates collider-stratification bias: even though the true effect of smoking in the full population is harmful (OR ≈ 1.35) and remains harmful for a repeat procedure, when you condition on PCI smoking looks protective simply because the non-smokers are otherwise at increased risk due to the spurious induced associated between the other (unmeasured) risk factors and smoking."
  },
  {
    "objectID": "posts/2025-04-04-Collider/index.html#visual-interpretation",
    "href": "posts/2025-04-04-Collider/index.html#visual-interpretation",
    "title": "Collider risk stratification bias",
    "section": "Visual interpretation",
    "text": "Visual interpretation\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodelibrary(patchwork)  # for side-by-side plots\n\n# Assume df and data_PCI already exist, with columns:\n#   U: unobserved risk factor\n#   smoking: 0/1\n#   PCI: 0/1\n#   repeat_PCI: outcome\n\n# Create a \"Group\" variable to label each dataset\ndf$Group &lt;- \"Full Population\"\ndata_PCI$Group &lt;- \"PCI Selected\"\n\n# Plot 1: Full population\np1 &lt;- ggplot(filter(df, Group == \"Full Population\"),\n             aes(x = U, fill = factor(smoking))) +\n  geom_density(alpha = 0.5) +\n  coord_cartesian(xlim = c(-3, 3)) +  # adjust as needed\n  labs(\n    title = \"Distribution of U in the Full Population\",\n    x = \"U (Unobserved Risk Factor)\",\n    y = \"Density\",\n    fill = \"Smoking\\n(0=No,1=Yes)\"\n  ) +\n  theme_minimal()\n\n# Plot 2: PCI-selected, x-axis starting at 0\np2 &lt;- ggplot(filter(data_PCI, Group == \"PCI Selected\"),\n             aes(x = U, fill = factor(smoking))) +\n  geom_density(alpha = 0.5) +\n  coord_cartesian(xlim = c(0, NA)) +  # start x-axis at 0\n  labs(\n    title = \"Distribution of U in PCI-Selected Sample\",\n    x = \"U (Unobserved Risk Factor)\",\n    y = \"Density\",\n    fill = \"Smoking\\n(0=No,1=Yes)\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side\np1 + p2\n\n\n\n\n\n\n\nFull Population (left plot) shows how the unobserved risk factor U is distributed equally among both smokers (red) and non-smokers (blue) in the entire dataset. in th estudy sample, PCI-selected sample (right plot) because of the selection mechanism you can more clearly see that non-smokers in the PCI sample have U-values &gt; the values in smokers. Hence the false impression that smoking is protective."
  }
]