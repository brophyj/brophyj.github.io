[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Does it make a difference - sedentary break\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nWho counts?\n\n\n\n\n\nTuesday, the 5th of March, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nEndotoxin exposure in asthmatic children and matched healthy controls\n\n\n\n\n\n\nBias\n\n\n\nWhat counts?\n\n\n\n\n\nMonday, the 19th of February, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the prevalence of depression in medical students?\n\n\n\n\n\n\nEvidence based medicine\n\n\nMeta-analysis\n\n\n\nNo reliable answer from this study\n\n\n\n\n\nMonday, the 5th of February, 2024\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nA personal (and disturbing) conversation with ChatGPT\n\n\n\n\n\n\nAI\n\n\n\nHonest but troubling\n\n\n\n\n\nTuesday, the 18th of April, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMisinformed conclusions\n\n\n\n\n\n\nEvidence based medicine\n\n\nBayesian stats\n\n\n\ncaveat lector\n\n\n\n\n\nSaturday, the 15th of April, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nSomething old and something older\n\n\n\n\n\n\nBias\n\n\nEvidence based medicine\n\n\n\nImmortal time bias, longstanding and persistent\n\n\n\n\n\nWednesday, the 22nd of March, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nAbsence of evidence is not evidence of absence\n\n\n\n\n\n\nBayesian stats\n\n\n\nBayes to the rescue?\n\n\n\n\n\nSunday, the 19th of March, 2023\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does scientific authorship mean?\n\n\n\n\n\n\nEvidence based medicine\n\n\nBayesian stats\n\n\n\nSometimes, not much\n\n\n\n\n\nMonday, the 5th of December, 2022\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMedication Withdrawal - When Reasonable People Disagree\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nUnderstanding the past\n\n\n\n\n\nFriday, the 17th of December, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nInaccurate References; Guile, Duplicity, and Perfidious Publications\n\n\n\n\n\n\nEvidence based medicine\n\n\nGuidelines\n\n\nBayesian stats\n\n\n\nTrusting the guidelines?\n\n\n\n\n\nSunday, the 12th of December, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-Offs Versus Obfuscation\n\n\n\n\n\n\nBias\n\n\nEvidence based medicine\n\n\n\nReporting biases?\n\n\n\n\n\nFriday, the 26th of November, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid testing in travellers\n\n\n\n\n\n\nStatistical analysis\n\n\n\nWorthwhile, I think not\n\n\n\n\n\nThursday, the 18th of November, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nHeparin in hospitalized COVID-19 patients\n\n\n\n\n\n\nStatistical analysis\n\n\n\nFaulty stats?\n\n\n\n\n\nTuesday, the 12th of October, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nNew Covid drug\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nHeavy on the hype?\n\n\n\n\n\nMonday, the 4th of October, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid vaccine myocarditis\n\n\n\n\n\n\nEvidence based medicine\n\n\nGuidelines\n\n\nBayesian stats\n\n\n\nWhere is the common sense?\n\n\n\n\n\nWednesday, the 29th of September, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nEXCEL - clearing the haze with Bayes\n\n\n\n\n\n\nBayesian stats\n\n\n\nAdding clarity?\n\n\n\n\n\nFriday, the 15th of January, 2021\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate on the science behind masks\n\n\n\n\n\n\nBayesian stats\n\n\n\nGood enough?\n\n\n\n\n\nTuesday, the 24th of November, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID, masks and poor science\n\n\n\n\n\n\nBias\n\n\nStatistical analysis\n\n\n\nQuestionable interpretation?\n\n\n\n\n\nFriday, the 17th of July, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nPharma, bribes and MDs\n\n\n\n\n\n\nBias\n\n\n\nCorrelation or causality?\n\n\n\n\n\nWednesday, the 8th of July, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nGlad to be at the back of the queue\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nHow good is the evidence?\n\n\n\n\n\nMonday, the 6th of July, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nVirtual Learning & Corporate Sponsorship\n\n\n\n\n\n\nBias\n\n\n\nTime to worry?\n\n\n\n\n\nFriday, the 15th of May, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nMontreal, a global Covid perspective\n\n\n\n\n\n\nVisualizations\n\n\n\nInformation bias?\n\n\n\n\n\nTuesday, the 12th of May, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid visualizations\n\n\n\n\n\n\nVisualizations\n\n\n\nAn aid to understanding\n\n\n\n\n\nTuesday, the 5th of May, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nCABG vs. PCI in LM\n\n\n\n\n\n\nEvidence based medicine\n\n\n\nGood enough?\n\n\n\n\n\nThursday, the 16th of April, 2020\n\n\nJay Brophy\n\n\n\n\n\n\n\n\n\n\n\n\nDig is OK\n\n\n\n\n\n\nEvidence based medicine\n\n\n\n\n\n\n\n\n\nFriday, the 19th of July, 2019\n\n\nJay Brophy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "2024",
    "section": "",
    "text": "Selected publications \n\n\nDownload full (current) CV\n\n\n2024\n\nMiceline Mésidor, Caroline Sirois, Jason Robert Guertin, Mireille E Schnitzer, Bernard Candas, Claudia Blais, Benoit Cossette, Paul Poirier, James M Brophy, Lisa Lix, Mina Tadrous, Awa Diop, Denis Hamel, Denis Talbot. Effect of statin use for the primary prevention of cardiovascular disease among older adults: A cautionary tale concerning target trials emulation. Journal of Clinical Epidemiology\nJames M Brophy Is science really self correcting? Something old and older. Medium.\nNon peer review article based on a previous blog. Although I think this contains much interesting material, including the value of teaching,an exploration of immortal time bias, and its historical perspective, it was refused by &gt; 6 (I lost count of exactly how many!) medical journals for “not meeting their priorities”.\n\n\n\n2023\n\nJames M Brophy. A Bayesian Perspective-Extracorporeal CPR for Refractory Out-of-Hospital Cardiac Arrest medRxiv, 2023.02. 13.23285890\n\n\n\n2022\n\nTodd C Lee, Srinivas Murthy, Olivier Del Corpo, Julien Senécal, Guillaume Butler-Laporte, Zahra N Sohani, James M Brophy, Emily G McDonald. Remdesivir for the treatment of COVID-19: a systematic review and meta-analysis. Clinical Microbiology and Infection 28 (9), 1203-1210\n\n\n\n2021\n\nJames M Brophy. Bayesian analyses of cardiovascular trials—bringing added value to the table. Canadian Journal of Cardiology 37 (9), 1415-1427 2021\n\nJames M Brophy. Key Issues in the Statistical Interpretation of Randomized Clinical Trials. Canadian Journal of Cardiology 37 (9), 1312-1321\n\n\n\n2020\n\nJames M Brophy. Bayesian interpretation of the EXCEL trial and other randomized clinical trials of left main coronary artery revascularization. JAMA Internal Medicine 180 (7), 986-992"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Aortic stenosis",
    "section": "",
    "text": "Selected projects \n\n\nAortic stenosis\n\n\n\n\n\n\n\nAortic stenosis treatments\n\n\nA CIHR funded project investigating the impact of patient, institutional, physician, and regional data on the trajectory of aortic stenosis (AS) treatment choices, outcomes, and cost effectiveness\nOverview\nAortic stenosis (AS) is the most prevalent heart valve disorder in developed countries. Historically, surgical aortic valve replacement (SAVR) has been the standard treatment for symptomatic AS. Trans-catheter aortic valve replacement (TAVR) has now been demonstrated to significantly reduce mortality for inoperable AS patients compared to medical treatment, and to have comparable or improved outcomes compared across a spectrum of SAVR candidates. The most recent randomized trial has shown TAVR to be noninferior to surgery with respect to the composite end point of death or disabling stroke at 24 months follow-up in patients at low surgical risk. TAVR has, and will continue to, dramatically change the way patients with AS are managed and may be seen as “disruptive” technology.\nIt is important to have a clear understanding of the diagnostic, treatment and outcomes experienced by these patients. This is necessary to be sure the appropriate resources are available to meet these demands in a fair, transparent, and equitable manner.\n\n\nComparative effectiveness studies of dual antiplatelet therapy (DAPT)\n\n\n\n\n\n\n\nDAPT\n\n\nTC4 Ticagrelor compared to clopidogrel in acute coronary syndromes - the TC4 comparative effectiveness study\nCIHR funded project (2019-2023) looking at the comparative effectiveness of ticagrelor versus clopidogrel in acute coronary syndromes.\nOverview Cardiovascular diseases, especially acute coronary syndromes (ACS) are a leading cause of mortality and morbidity. Antiplatelet medications have a long history of improving ACS survival and for the prevention of a secondary cardiac event. Based on the consistency and strength of multiple randomized controlled trials (RCTs), the dual antiplatelet therapy (DAPT), clopidogrel and aspirin, following an ACS event, had long been considered the gold standard in the leading clinical guidelines. More recently, a new DAPT regimen, ticagrelor and aspirin, has become the recommended strategy. However this is based largely on one large RCT with a limited number of North American patients (who did not show any incremental benefit). This project aimed to further explore the comparative effectiveness of these different treatments by performing a\ni) Bayesian network meta-analysis of all previous RCTs\nii) Pharmacoepidemiology study using Quebec administrative databases (manuscript in progress)\niii) time clustered, randomized registry trial of 1,000 ACS patients (manuscript in progress)\n\n\n(Mostly) Clinical Epidemiology with R\nThis is some introductory teaching material related to clinical epidemiology using R.\nHopefully at some point this material will be updated, expanded and repackaged using Quarto\n\n\n\nAortic stenosis treatments\nDAPT"
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "",
    "text": "On Nov 15 2021, the Lancet published an individual patient data (IPD) meta-analysis “Percutaneous coronary intervention with drug-eluting stents versus coronary artery bypass grafting in left main coronary artery disease” that concluded\n Among patients with left main coronary artery disease and, largely, low or intermediate coronary anatomical complexity, there was no statistically significant difference in 5-year all-cause death between PCI and CABG, although a Bayesian approach suggested a difference probably exists (more likely than not &lt;0·2% per year) favouring CABG. There were trade-offs in terms of the risk of myocardial infarction, stroke, and revascularisation. A heart team approach to communicate expected outcome differences might be useful to assist patients in reaching a treatment decision.   The paper is remarkable for several reasons:\n1. In addition to standard frequentist analyses, it included a Bayesian assessment of total mortality.\n2. The main text of the paper did not consider the composite primary endpoint, generally a composite of several cardiovascular outcomes including death, myocardial infarction, stroke or revascularizations, of these trials but reported only the individual endpoints.\n3. The conclusion speaks of trade-offs in the risk of myocardial infarction, stroke, and revascularisation implying there are supporting data for both interventions which does not accurately reflect the reality.\n4. While most would agree with the concept of heart team approach, this is only valid if fully transparent evidence is provided and by relegating the primary endpoint to supplemental material and excluding this from the more informative Bayesian analyses, the authors risk undermining their recommendation."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#bayesian-approach",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#bayesian-approach",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nThe Bayesian analysis is helpful as it moves discussion away from null hypothesis significance testing and towards the parameter estimation of the difference in mortality between the 2 approaches. I published a Bayesian analysis of the same datasets, using aggregate data,in JAMA Internal Medicine 18 months ago and although not referenced in the current article found the exact same probability for increased mortality with CABG. Given that one of the authors heavily criticized this previous work as being biased with analytical errors and omissions, I am thankful for the validation of my earlier results. However, given that this IPD meta-analysis would also include a systematic review, one could wonder if ignoring this previous work was not an intentional attempt at its scotomization."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#which-endpoint",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#which-endpoint",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Which endpoint",
    "text": "Which endpoint\nIt is surprising that the authors elected to focus their IPD not on the primary endpoint of the trials which was generally a composite of total mortality, myocardial infarction, stroke and, depending on the study, ischemia driven revascularization either as a primary or secondary outcome, but rather on the individual components of the composite outcome. It is only deep in the supplemental material that the results of the composite outcome can be found without any additional Bayesian analysis. It would be appreciated if the authors could confirm the earlier results of a 90% probability that the composite endpoint, including repeat revascularizations, is increased by at least 5 extra events per 100 patients treated with PCI."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#trade-offs",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#trade-offs",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Trade-offs",
    "text": "Trade-offs\nThe authors state ““There were trade-offs in terms of the risk of myocardial infarction, stroke, and revascularisation.” In addition, to the 86% probability of increased mortality at 5 years, the authors report statistically significant increases of 135% (HR 2·35 (1·71–3·23)) in spontaneous myocardial infarction and 78% (HR 1·78 (1·51–2·10) in the need for repeat revascularizations with PCI. Provided the patient has a reasonable life expectancy, all outcomes favor CABG except for stroke and that difference is not statistically significant (HR 0·84 (0·59–1·21)). In absolute terms, for every 1000 patients treated with PCI versus CABG, one would expect 35 more patients to have a spontaneous MI & 76 more to undergo repeat revascularization over 5 years, with no discernible difference in stroke. Therefore the question arises what ‘trade-offs’ are the authors referring to? Allowing the authors to highlight the increased stroke risk at 1 year with CABG seems more like a failure of the peer review system as this outcome was never prespecified in any of the original studies and seems purely data driven."
  },
  {
    "objectID": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#heart-team-approach",
    "href": "posts/2021-11-26-trade-offs-versus-obfuscation/index.html#heart-team-approach",
    "title": "Trade-Offs Versus Obfuscation",
    "section": "Heart team approach",
    "text": "Heart team approach\nThe authors recommend a heart team approach to communicating this information and I suspect that the medical community is largely in agreement, provided the totality of the evidence is presented in a transparent and unbiased manner, ideally incorporating the additional advantages of Bayesian analyses. Despite four of the 15 authors coming from surgical backgrounds, one may question if this article is an objective and nonpartisan accounting of the evidence. This reader is left with the impression of a cabal of conflicted cardiologists attempting to obfuscate the evidence, aided and abetted by an ineffective peer review system."
  },
  {
    "objectID": "posts/2020-04-16-cabg/index.html",
    "href": "posts/2020-04-16-cabg/index.html",
    "title": "CABG vs. PCI in LM",
    "section": "",
    "text": "This is a blog post about a recent NEJM article. \nBased on the primary outcome result (difference, 2.8 percentage points; 95% confidence interval [CI], −0.9 to 6.5; P = 0.13), the EXCEL authors conclude “In patients with left main coronary artery disease of low or intermediate anatomical complexity, there was no significant difference between PCI and CABG”. Leaving aside that CABG was statistically superior in terms of total mortality, ischemia driven revascularization, and non-procedural myocardial infarction, it is questionable that the primary outcome data actually support their conclusion. EXCEL was originally designed as a non-inferiority (NI) study with a margin of 4.2%, which en passant many would consider too liberal. Even accepting that margin, how can a primary outcome with a 95% upper CI of 6.5%, exceeding the original NI margin be used to support their conclusion?\nDid you find this page helpful? Consider sharing it 🙌\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {CABG Vs. {PCI} in {LM}},\n  date = {2020-04-16},\n  url = {https://brophyj.github.io/posts/2020-04-16-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “CABG Vs. PCI in LM.” April 16, 2020. https://brophyj.github.io/posts/2020-04-16-my-blog-post/."
  },
  {
    "objectID": "posts/2021-10-04-new-covid-drug/index.html",
    "href": "posts/2021-10-04-new-covid-drug/index.html",
    "title": "New Covid drug",
    "section": "",
    "text": "On October 1 2021, Merck issed a press release stating that following an interim analysis of the Phase 3 MOVe-OUT trial, 7.3% (28/385) of patients treated with molnupiravir experienced the primary endpoint (death or hospitalization through day 29), compared with 14.1% (53/377) of placebo-treated patients. The study population were non-hospitalized adult patients with mild-to-moderate COVID-19. Given the almost 50% reduction in the primary outcome (p=0.0012), it is perhaps not surprising that the press release goes on to state that “Merck plans to submit an application for Emergency Use Authorization (EUA) to the U.S. FDA as soon as possible based on these findings and plans to submit marketing applications to other regulatory bodies worldwide.”\nWhile definitive conclusions are impossible without examining the complete data set and seeing the final publication, one may wonder if the data are as compelling as the press release suggests. 1.p values overestimate the strength of the evidence For example, if before the MOVe-OUT trial was performed, it was beleived there was a 50:50 chance of the null hypothesis (H\\(_o\\)) of no effect being true and the experiemental data resulted in a p = 0.0012 (z=3.29) as observed, then the minimum Bayes Factor is approximately 1/190 (1/BF) and the posterior probability that H\\(_o\\) is true falls to 0.5% (posterior odds = Bayes factor * prior odds). However a skeptic might say that the majority of drug trials fail and that interim results often provide over-estimates of the true effect (see below) therefore the prior odds that H\\(_o\\) is true may be considerably higher than 50:50, perhaps more like 75:25 or even 90:10. Under those prior beliefs, the probability of H\\(_o\\) still being true even after the MOVe-OUT trial data is 1.3% and 3.9% respectively. The MOVe-OUT results still appear strongly supportive of a benefit for molnupiravir but perhaps not quite as compelling as initially implied by p = 0.0012.\n2.Impact of stopping at interim analysis A meta-analysis has examined the results from 91 truncated RCTs with 424 matching nontruncated RCTs and concluded “Truncated RCTs were associated with greater effect sizes than RCTs not stopped early. This difference was independent of the presence of statistical stopping rules and was greatest in smaller studies.” Overall the relative risk (RR) in the truncated trials was 71% of the nontruncated ones. However in smaller trials with less than 200 events the truncated trials reported RR of only 37% what was observed in the nontruncated ones (see figure below from their publication).\n\n\n\n\n\n\n\nRRs vs the Total Number of Outcome Events in Truncated RCTs - Red line is average for small trials &lt; 200 events\n\n\n\n Of course, this does not mean the observed MOVe-OUT RR of 0.48 would be reduced by 63% (to OR = 19%) in a nontruncated trial but it does offer some idea of the potential uncertainities that can arise with trials that are stopped premarturely, even those accompanied by a very small p value.\nAs a concrete example of the dangers of interim analyses, consider the randomized PROWESS trial of drotrecogin alfa (AKA activated protein C or APC) involving 1690 patients with sepsis that was stopped prematurely when it showed a 6% mortality difference (p = 0.006). The drug was approved, although later when nontruncated trials showed no benefit the sponsor elected in 2011 to withdraw the drug from the market, judging that its risks exceeded any potential small benefits. 3.Fragility index (FI) as a measure of trial robustness The FI is the minimum number of patients who would have to change their event status to turn a statistically significant result to a non-significant result. For this data, the FI = 8. In other words, a reversal in statistical significance is achieved by inversing the status of 2 events in the placebo group and 6 non-events in the experimental group. This represents a change in status in only approximately 1% of the study sample. I don’t think the complete loss of statistical significance by a reversal in outcomes in only 1% of the study population provides the necessary alacrity for early approval. 4.Differentiating between statistical and clinical significance While the p value gives the probability of observing this or more extreme data given the null hypothesis, it is most often of more interest to calaculate the inverse probability, namely the probability of a hypothesis given the data. This inverse probability moves inference into the realm of Bayesian statistics. The achilles heel of Bayesian is felt by some to be its requirement of a prior distribution. To minimize the impact of the prior, one can choose a non-informative one whereby the final (posterior) probability is determined uniquely by the observed data. The probability distribution function (PDF) for the difference between molnupiravir and placebo from MOVe-OUT using a non-informative prior is plotted below.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nOne of the main advantages of this approach is that it assists in differentiating between statistical and clinical significance. We are less interested in the probability that there are simply fewer events with molnupiravir and more interested in the probability that this difference exceeds some clinically meaningful threshold. Thus while there is a 99.9% probability of fewer events with molnupiravir, there is only a 95% probability that the benefit is as large as a 3% decrease in the combined death & hospitalization outcome. Again this speaks to the high probability of benefit with molnupiravir but there is less certainty as to the magnitude of any relevant clinical benefit. 5.Safety There were no concerning safety signals noted from the MOVe-OUT trial but with only 775 patients there was insufficient power to rule out potential side effects. 6.Generalizability Recruitment in Latin America, Europe, and Africa accounted for 55%, 23% and 15% of the MOVe-OUT study population, respectively. This leaves at most 7% or approximately 50 patients randomized from North America centers. Are we certain that drug effects will not vary across these different populations with their varying ancillary treatments and health care systems? Perhaps there is no no between country variation but this is another point of uncertainty that still requires some resolution. 7.Cost Given a 5 day treatment is expected to cost about $700 and that Merck has projected to produce 10 million courses of treatment in 2021, the cost of molnupiravir could reach several billion dollars, highlighting again the need for further studies to reduce the uncertainty regarding the magnitude of any benefits, of any risks, and the generalizability of the results. In conclusion from my perspective, although these initial trial results are very promising, it would seem prudent to request a confirmation study, ideally mainly in a North American population, before committing a large amount of resources to this drug. In most healthcare systems, resources are not a continually expanding commodity and a commitment to this drug will come with a reduction of resources elsewhere and hence the need to make maximally informed decisions.\n\n\n\nRRs vs the Total Number of Outcome Events in Truncated RCTs - Red line is average for small trials &lt; 200 events\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {New {Covid} Drug},\n  date = {2021-10-04},\n  url = {https://brophyj.github.io/posts/2021-10-04-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “New Covid Drug.” October 4, 2021. https://brophyj.github.io/posts/2021-10-04-my-blog-post/."
  },
  {
    "objectID": "posts/2020-07-17-covid-poor/index.html",
    "href": "posts/2020-07-17-covid-poor/index.html",
    "title": "COVID, masks and poor science",
    "section": "",
    "text": "A CDC report that concludes“face masks, evidence from randomized controlled trials of these measures did not support a substantial effect on transmission of laboratory-confirmed influenza”\nis just WRONG.\nIt uses an incorrect fixed effects statistical model, thereby ignoring between study variations.Moreover its interpretation is plagued by the common null hypothesis statistical testing cognitive biases of dichotomania and nullism.\nProper analysis (see Figure below) shows average benefit could be as great as 56% reduction and next study could show a benefit of up to 82% reduction.\nUnlikely any increased risk so more reasonable conclusion “MASK UP while awaiting more studies to refine the exact size of any benefit.”\n\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {COVID, Masks and Poor Science},\n  date = {2020-07-17},\n  url = {https://brophyj.github.io/posts/2020-07-17-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “COVID, Masks and Poor Science.” July\n17, 2020. https://brophyj.github.io/posts/2020-07-17-my-blog-post/."
  },
  {
    "objectID": "posts/2021-11-18-covid-testing-in-travellers/index.html",
    "href": "posts/2021-11-18-covid-testing-in-travellers/index.html",
    "title": "Covid testing in travellers",
    "section": "",
    "text": "At present, guidelines require mandatory molecular based COVID-19 tests for all travellers entering Canada regardless of their vaccination status. Ever wonder about the (cost) effectiveness of this policy?\nThe data below is from Canada Public Health for July to end of October 2021.\n\n\n\n\n\n\n\n\nThis shows that the average test positivity rate for vaccinated travellers is approximately 0.16% or 16 in 10,000. With a laboratory cost of approximately 200 dollars for a RT-PCR test, the cost of identifying these 16 individuals is approximately 2,000,000 dollars. Other costs to this strategy include the time wasted arranging for testing and the illusory sense of security associated with a negative RT-PCR test, given the high false negative rate in the first days following exposure. At what level of detection does this policy of screening all vaccinated travellers no longer make sense? Simply counting those identified without quantifying the overall benefits and costs seems inane. Will we maintain this policy if detection rates fall to 10 in 10,000? 5 in 10,000? 1 in 10,000? What criteria will we use, the most risk averse imaginable? In the realm of clinical medicine, testing of individuals whose prior probability of disease is 0.1%, or even 1%, is strongly discouraged as being inappropriate. Why then should public health get a free pass on this unselective testing?\nOverall during the past 4 months, returning vaccinated Canadian travellers (605,000) have spent approximately 120 million dollars on testing to identify a total of 1031 RT-PCR positive individuals. In other words, 120,000 dollars is being spent to detect each positive individual. The benefits from identifying these largely asymptomatic individuals are unknown as their infectious and transmissibility rates remain undefined. One could reasonably speculate that rather than imposing this unproven testing strategy on vaccinated travellers, a simple health tax of $100 - 200 per traveler with a less restrained allocation policy allowing money to flow to both COVID or non-COVID health conditions would purchase more health benefits.\nOne might wonder how this mandatory sampling of vaccinated travellers would compare with random sampling of the general population. The REACT study from the UK randomly sampled 146,701 individuals in Jan 2021 and discovered 1251 positive cases, a rate of 0.9%, or almost 6 times the current rate in vaccinated Canadian travellers. Now this data comes from a different time and different country so its comparability to current Canadian data is close to nil. Nevertheless, it does raise the question of whether the yield of testing vaccinated air travellers is any better than a policy of random testing of the general population, a procedure that is not presently recommended\nImposing a national policy simply because it identifies a few extra cases without considering the total implications of the policy lacks intellectual rigour and justification. The positivity yield in non-vaccinated travellers is about 4 fold higher than in the vaccinated cohort. Similarly, in random sampling of the general population positivity yields are increased about 6 fold in symptomatic individuals. More detailed modeling with enhanced contemporary local population data, is necessary to make optimally informed decisions. However, a targeted testing of unvaccinated or symptomatic travellers, instead of the current unselected approach, may well represent a first pass, more sensible, and cost-effective approach to Covid detection. Replacing the cost and inconvenience of indiscriminate Covid testing in vaccinated travellers with a simple health tax would likely purchase increased societal health benefits over the present capricious system.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Covid Testing in Travellers},\n  date = {2021-11-18},\n  url = {https://brophyj.github.io/posts/2021-11-18-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Covid Testing in Travellers.” November\n18, 2021. https://brophyj.github.io/posts/2021-11-18-my-blog-post/."
  },
  {
    "objectID": "posts/2020-05-05-covid/index.html",
    "href": "posts/2020-05-05-covid/index.html",
    "title": "Covid visualizations",
    "section": "",
    "text": "The majority of Covid-19 cases in Quebec are on the island of Montreal. Using daily reports from Quebec public health, I have made a few graphs that explore the rate of infections during the month of April for the different Montreal neighbourhoods.\nData visualizations help to appreciate the time and geographic variations of Covid-19 infections. Infections started in central higher SES areas (? areas with more spring break travel) before spreading to predominately lower SES areas click here for interactive Montreal maps\n\nObviously inferences of Montreal cases are limited by the uncertain quality of the data. With this caveat, here are some personal observations\n* Early in the month, cases were highest in higher SES areas, likely highlighting ^ travel exposure in these areas\n* As month progressed, saw the usual gradient of more cases in areas with lower SES and higher population densities\n* No region has definitively “turned the corner”, but the majority of areas seems to be leveling off, giving some hope that the announced progressive easing of current restrictions will not lead to a complete disaster\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Covid Visualizations},\n  date = {2020-05-05},\n  url = {https://brophyj.github.io/posts/2020-05-05-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Covid Visualizations.” May 5, 2020. https://brophyj.github.io/posts/2020-05-05-my-blog-post/."
  },
  {
    "objectID": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html",
    "href": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html",
    "title": "A personal (and disturbing) conversation with ChatGPT",
    "section": "",
    "text": "ME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine and Epidemiology at McGill University in Montreal, Canada. He is a practicing physician and a clinical pharmacologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. He has published extensively in these areas and has received numerous awards for his research, teaching, and clinical work. Dr. Brophy is also the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP), which aims to improve the quality of care for patients with cardiovascular disease.\n\nME: Dr. Brophy is not the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP).\n\nChatGPT: I apologize for the mistake in my previous response. You are correct that Dr. James (Jay) Brophy is not the current Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP). However, he was a founding member of the CHIP and served as its Director from 2002 to 2006. He continues to be an active researcher and clinician at McGill University and the McGill University Health Centre.\n\nME: he was NOT a founding member of the CHIP and did NOT serve as its Director from 2002 to 2006.\n\nChatGPT: I apologize for the confusion in my previous responses. You are correct that Dr. James (Jay) Brophy was not a founding member of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP) and did not serve as its Director from 2002 to 2006. I do not have access to the most up-to-date information regarding his current position or role at McGill University.\n\nME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. He is also a clinical pharmacologist and an epidemiologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has been a faculty member at McGill since 1992 and has published extensively in his areas of research. He has also received numerous awards for his research, teaching, and clinical work.\n\nME: He is NOT a clinical pharmacologist and has been a faculty member at McGill since 2001\n\nChatGPT: I apologize for my previous mistakes. You are correct that Dr. James (Jay) Brophy is not a clinical pharmacologist, and he has been a faculty member at McGill University since 2001. Dr. Brophy is a Professor of Medicine and Epidemiology at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. His research interests include cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has made significant contributions to his field and has received numerous awards for his work.\n\nMe: The world of James (Jay) Brophy according to Chat GPT contained a few kernels of truth but lots of misinformation. The system does seem to learn from conversations and make corrections. But this is highly dependent on trustworthy interactions with the system. For example, I left the flattering, but perhaps somewhat questionable phrase “has made significant contributions to his field”. While this interaction left me slightly amused, it mostly has frightened me as I ponder if anyone could, deliberately or not, modify ChatGPT’s vision of me with misinformation? While I’m only a tiny speck in the universe, the possibility of this tool creating streams of consequential misinformation seems staggering.\n\nChatGPT: I understand your concerns, and you bring up a valid point about the potential for misinformation to be spread through language models like ChatGPT. While language models can be very useful for generating text and answering questions, they are not infallible and can be influenced by biased or inaccurate information. It’s important to remember that language models like ChatGPT are only as good as the data they are trained on and the quality of the interactions they have with users. As a language model, ChatGPT is designed to learn from its interactions with users and improve over time, but it is still subject to limitations and biases. However, efforts are being made to improve the accuracy and reliability of language models, such as developing methods to detect and correct misinformation and building models that are more transparent and explainable. It’s important to be mindful of the limitations of language models and to use them as one tool among many in seeking information and knowledge. It’s also important to critically evaluate the information provided by language models and verify it through other sources when necessary.\n\nMe: This is not terribly reassuring.\n\nChatGPT: I understand that my previous response may not have fully addressed your concerns. While it is true that language models like ChatGPT are not perfect and can be influenced by biased or inaccurate information, there are also measures in place to mitigate the risks of misinformation. For example, some language models are trained on diverse and large datasets to minimize bias, and researchers are developing methods to detect and correct misinformation. Additionally, language models are often evaluated on their ability to provide accurate and helpful responses to a variety of questions. However, it’s important to acknowledge that language models are not infallible and can never replace critical thinking and verification through other sources. It’s essential to approach any information provided by language models with a critical eye and to seek out multiple sources of information when making important decisions. In summary, while language models like ChatGPT have limitations, there are also measures in place to minimize the risks of misinformation. However, critical thinking and verification through multiple sources remain essential to ensure the accuracy and reliability of information.\n\nMe: caveat lector\n\nChatGPT: “Caveat lector” or “reader beware” is another important principle to keep in mind when seeking information, whether from language models or other sources. It reminds us that as readers, we have a responsibility to critically evaluate the information presented to us, to consider the potential biases and limitations of the source, and to seek out multiple sources of information to verify and validate what we’ve learned. By being vigilant and discerning readers, we can better navigate the complex and ever-changing landscape of information and knowledge.\n\nMe: Good luck\n\nChatGPT: Thank you! If you have any other questions or concerns, feel free to ask."
  },
  {
    "objectID": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html#a-personal-and-disturbing-conversation-with-chatgpt",
    "href": "posts/2023-04-18-a-personal-and-disturbing-conversation-with-chatgpt/index.html#a-personal-and-disturbing-conversation-with-chatgpt",
    "title": "A personal (and disturbing) conversation with ChatGPT",
    "section": "",
    "text": "ME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine and Epidemiology at McGill University in Montreal, Canada. He is a practicing physician and a clinical pharmacologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. He has published extensively in these areas and has received numerous awards for his research, teaching, and clinical work. Dr. Brophy is also the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP), which aims to improve the quality of care for patients with cardiovascular disease.\n\nME: Dr. Brophy is not the Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP).\n\nChatGPT: I apologize for the mistake in my previous response. You are correct that Dr. James (Jay) Brophy is not the current Director of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP). However, he was a founding member of the CHIP and served as its Director from 2002 to 2006. He continues to be an active researcher and clinician at McGill University and the McGill University Health Centre.\n\nME: he was NOT a founding member of the CHIP and did NOT serve as its Director from 2002 to 2006.\n\nChatGPT: I apologize for the confusion in my previous responses. You are correct that Dr. James (Jay) Brophy was not a founding member of the McGill University Health Centre Cardiovascular Health Improvement Program (CHIP) and did not serve as its Director from 2002 to 2006. I do not have access to the most up-to-date information regarding his current position or role at McGill University.\n\nME: Who is James (Jay) Brophy MD at McGill University?\n\nChatGPT: Dr. James (Jay) Brophy is a Professor of Medicine at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. He is also a clinical pharmacologist and an epidemiologist with research interests in cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has been a faculty member at McGill since 1992 and has published extensively in his areas of research. He has also received numerous awards for his research, teaching, and clinical work.\n\nME: He is NOT a clinical pharmacologist and has been a faculty member at McGill since 2001\n\nChatGPT: I apologize for my previous mistakes. You are correct that Dr. James (Jay) Brophy is not a clinical pharmacologist, and he has been a faculty member at McGill University since 2001. Dr. Brophy is a Professor of Medicine and Epidemiology at McGill University and a staff physician at the McGill University Health Centre in Montreal, Canada. His research interests include cardiovascular disease prevention, clinical trials, and health services research. Dr. Brophy has made significant contributions to his field and has received numerous awards for his work.\n\nMe: The world of James (Jay) Brophy according to Chat GPT contained a few kernels of truth but lots of misinformation. The system does seem to learn from conversations and make corrections. But this is highly dependent on trustworthy interactions with the system. For example, I left the flattering, but perhaps somewhat questionable phrase “has made significant contributions to his field”. While this interaction left me slightly amused, it mostly has frightened me as I ponder if anyone could, deliberately or not, modify ChatGPT’s vision of me with misinformation? While I’m only a tiny speck in the universe, the possibility of this tool creating streams of consequential misinformation seems staggering.\n\nChatGPT: I understand your concerns, and you bring up a valid point about the potential for misinformation to be spread through language models like ChatGPT. While language models can be very useful for generating text and answering questions, they are not infallible and can be influenced by biased or inaccurate information. It’s important to remember that language models like ChatGPT are only as good as the data they are trained on and the quality of the interactions they have with users. As a language model, ChatGPT is designed to learn from its interactions with users and improve over time, but it is still subject to limitations and biases. However, efforts are being made to improve the accuracy and reliability of language models, such as developing methods to detect and correct misinformation and building models that are more transparent and explainable. It’s important to be mindful of the limitations of language models and to use them as one tool among many in seeking information and knowledge. It’s also important to critically evaluate the information provided by language models and verify it through other sources when necessary.\n\nMe: This is not terribly reassuring.\n\nChatGPT: I understand that my previous response may not have fully addressed your concerns. While it is true that language models like ChatGPT are not perfect and can be influenced by biased or inaccurate information, there are also measures in place to mitigate the risks of misinformation. For example, some language models are trained on diverse and large datasets to minimize bias, and researchers are developing methods to detect and correct misinformation. Additionally, language models are often evaluated on their ability to provide accurate and helpful responses to a variety of questions. However, it’s important to acknowledge that language models are not infallible and can never replace critical thinking and verification through other sources. It’s essential to approach any information provided by language models with a critical eye and to seek out multiple sources of information when making important decisions. In summary, while language models like ChatGPT have limitations, there are also measures in place to minimize the risks of misinformation. However, critical thinking and verification through multiple sources remain essential to ensure the accuracy and reliability of information.\n\nMe: caveat lector\n\nChatGPT: “Caveat lector” or “reader beware” is another important principle to keep in mind when seeking information, whether from language models or other sources. It reminds us that as readers, we have a responsibility to critically evaluate the information presented to us, to consider the potential biases and limitations of the source, and to seek out multiple sources of information to verify and validate what we’ve learned. By being vigilant and discerning readers, we can better navigate the complex and ever-changing landscape of information and knowledge.\n\nMe: Good luck\n\nChatGPT: Thank you! If you have any other questions or concerns, feel free to ask."
  },
  {
    "objectID": "posts/2021-09-29-covid-myocarditis/index.html",
    "href": "posts/2021-09-29-covid-myocarditis/index.html",
    "title": "Covid vaccine myocarditis",
    "section": "",
    "text": "On Sept 16 2021, 12 researchers from the Ottawa Heart Institute published a pre-print claiming that the COVID 19 mRNA vaccination myocarditis rate was 1 / 1000.\nOn Sept 24 2021, the authors reported the paper had been withdrawn due to numerical errors, although how the errors occurred was not explained. The biggest error was in the denominator of the the number of vaccinated individuals at risk for myocarditis. The authors said the data were publicly available for download at: https://open.ottawa.ca and they claimed that 32,379 doses were administered during their two month study period. The source population was apparently all vaccinated individuals in the greater Ottawa area. However, if the data is downloaded (it is a simple csv file), it can be seen that approximately 820,000, not 32,000, were given in the study period as shown in the following figure.\n\n\n\n\n\n\n\n\nNow, anyone can make an error but there are several simple ways to minimize your chances of making one.\n1. Plot the data. The above plot quickly shows that a figure of 32,000 is simply not possible.\n2. Use your common sense. Most regional people know that the greater Ottawa population is approximately 1,000,000. Most local people also know that vaccination roll out began seriously sometime around March and that 75% of the population is now vaccinated. In other words, about 750,000 people have been vaccinated over the last 8 months. Consequently for the two month study period, any semi literate individual would have expected something around a few hundred thousand and not 32,000 vaccinations which is ridiculously low and implausible. Yet 12 professionals from the Ottawa Heart Institute were not able to detect this error! Speaking of 12, why would it take 12 individuals from the same institution to collect a case series of 32 individuals.\n3. Respect the responsibility and accountability that comes with authorship. Do these 12 authors all truly meet authorship criteria? Do they know what authorship criteria are? The dilution of authorship responsibility undoubtedly dilutes accountability. In the limit, as the number of authors increases individual accountability can be expected to trend towards zero.\n4. Be conscientious and sweat the small stuff. This implies someone proof reading the manuscript and realizing that i) in addition to an erroneous denominator ii) it also had no page numbers iii) used data as a singular noun iv) described the case series as a cohort study.\n5. Make necessary corrections. Interestingly, when the errors were discovered the authors made no attempt to correct their paper, preferring instead to withdraw it.\n6. Avoid exaggeration and sensationalism. The authors appear to have attempted to maximize their cases by including non myocarditis cases and potentially false positive cases. For example, the authors elected to include i) two cases without myocarditis but with a diagnosis of pericarditis in isolation ii) at least some cases had no signs of inflammation or myocardial damage (the range of CRP and troponins included normal values) iii) some cases attributed to the vaccine occurred up to 26 days after exposure. Thus the number of potential vaccine induced cases of myocarditis is likely less than reported, perhaps more in the vicinity of 26-28.\n7. Make appropriate contrasts. The authors have attributed all cases of myocarditis in these vaccinated individuals to the vaccine. In doing so, they failed to consider the background risk of myocarditis, independently of any relationship to the vaccine. Instead they attributed every case to the vaccine. It should be noted that the annual baseline incidence rate of myocarditis, before Covid and the vaccine, was estimated at 10 - 20 cases /100000. So for the 800,000 vaccinated individuals one may expect between 80-160 cases over a year or between 13 and 27 cases over the 2 month study period even if there was no vaccination myocarditis association. In other words, their data is compatible both with a doubling of the myocarditis risk (about 13 extra cases / 100,000 vaccinations or 99.99% probability of not getting myocarditis) but also compatible with a possible scenario of no excess risk at all.\nI personally think these errors and omissions are too numerous and egregious to be easily forgiven. At least 3 of the authors are full professors of medicine and one wonders what example they have set for their more junior colleagues.\nThere is also a strong history that withdrawn articles unfortunately continue to live on after their retraction. It seems likely that the anti-vaxer movement will quote this false rate of 1 /1,000 cases of vaccine induced myocarditis, rather than the unbiased risk of between 0 and 13 extra myocarditis cases / 100,000 vaccinated people.\nWhile this may provide a teachable moment, it nevertheless remains a publication legacy that researchers should strive to avoid.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Covid Vaccine Myocarditis},\n  date = {2021-09-29},\n  url = {https://brophyj.github.io/posts/2021-09-29-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Covid Vaccine Myocarditis.” September\n29, 2021. https://brophyj.github.io/posts/2021-09-29-my-blog-post/."
  },
  {
    "objectID": "posts/2021-12-12-inaccurate-references-guile-duplicity-and-perfidious-publications/index.html",
    "href": "posts/2021-12-12-inaccurate-references-guile-duplicity-and-perfidious-publications/index.html",
    "title": "Inaccurate References; Guile, Duplicity, and Perfidious Publications",
    "section": "",
    "text": "Good scientific writing is characterized by clarity of thought and often buttressed by appropriate peer reviewed references. This is of extreme importance when the scientific writing is that of consensus guidelines. But are references always faithfully quoted and even if that is true, are the original references in fact accurate and truthful?\nConsider the recent AHA / ACC Guidelines for Coronary Artery Revascularization. According to these guidelines, class 1 recommendation arises when benefits &gt;&gt;&gt; risks and class 2a when benefits &gt;&gt; risks. In the section dealing with left main disease, we find the above recommendation.\nWhat is the evidence supporting the claim “PCI is reasonable to improve survival”? Reference 9 is a Bayesian cross-design and network meta-analysis of 12 studies (4 RCTs and 8 observational studies) comparing CABG with PCI and of 7 studies (2 randomized clinical trials and 5 observational studies) comparing CABG with medical therapy. This 2013 publication had a primary outcome of 1 year mortality and concluded;\n1) no mortality difference between CABG and PCI\n2) medical therapy is associated with higher 1-year mortality than PCI\nSo while the guideline writers have accurately quoted the reference, the actual publication seems more smoke and mirrors than hard evidence. Accepting the authors’ results that the posterior median for the comparative PCI / CABG mortality summary OR was 1.04 with 95% Bayesian credible interval 0.74 to 1.39 does not however provide evidence for concluding there is no mortality difference between the 2 techniques – unless, as first observed by the late George Diamond, the original Bayesian cardiologist, one believes potential relative mortality increases from 26% in favor of one technique to 39% in favor of the other are clinically unimportant.\nThe inability of these results to lead to definitive conclusions is illustrated by the following plot of this probability density function.\n\nThis demonstrates that the probability of increased mortality with PCI is 60% and 25% probability that the mortality OR is increased by at least 15% compared to CABG.\nMoreover, there are other notable limitations to this publication which one may have expected an illustrious group of guideline writers to have noticed. First according equal weighting given to RCTs and observational studies may be questioned. Second an important quality element in a network meta-analysis is to assure consistency between the observed direct effects and the calculated indirect effects. However according to their figure below, there were in fact NO studies, observational or randomized, comparing PCI to medical therapy rather this OR was calculated only by considering the indirect evidence coming from studies comparing CABG to medical therapy and CABG to PCI.\n\nAnother major limitation to the use of this publication as a reference is that the outcome was mortality at 1 year and longer-term mortality is clearly important and now available from 5-year follow-up studies from 4 RCTs. Why were these very available 5 year comparative data not considered? Could it be because new Bayesian analyses of these RCTs show compared to CABG a 85% probability of increased mortality with PCI & 98% probability &gt; 4 extra composite events / 100 PCI treated patients.\nA “squeezing” of the difference between CABG and PCI for left main disease is also occurring via a downgrading of the strength of the CABG recommendation to level of evidence (LOE) B-R, defined as a moderate quality single RCT or moderate quality meta-analysis. In fact reference 10 supporting a CABG mortality benefit over medical therapy comes from a meta-analysis of 7 RCTs including 3 large ones of undisputable quality. The LOE supporting CABG in left main should definitively have been scored A. One could therefore reasonably argue that there has been a systematic misuse of references to attempt to narrow the difference between CABG and PCI in the treatment of left main disease.\nMore disconcerting than the questionable interpretation of the left main data in the body of the guideline text, is the slippery path towards complete equivalence between the revascularization techniques as articulated on page 1 of the “Top 10 Take-home Messages” in the guidelines document. Message #3 casts aside all pretense to evidence, objectivity and nuance stating “Percutaneous revascularization is a reasonable option to improve survival, compared with medical therapy, in selected patients with low to medium anatomic complexity of coronary artery disease and left main disease that is equally suitable for surgical or percutaneous revascularization.”\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Inaccurate {References;} {Guile,} {Duplicity,} and\n    {Perfidious} {Publications}},\n  date = {2021-12-12},\n  url = {https://brophyj.github.io/posts/2021-12-12-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Inaccurate References; Guile, Duplicity, and\nPerfidious Publications.” December 12, 2021. https://brophyj.github.io/posts/2021-12-12-my-blog-post/."
  },
  {
    "objectID": "posts/2020-11-24-the-science-behind-masks/index.html",
    "href": "posts/2020-11-24-the-science-behind-masks/index.html",
    "title": "Update on the science behind masks",
    "section": "",
    "text": "While there is no doubt regarding the physical, mental, and economic carnage due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), there is considerable debate regarding the evidence of masks to reduce its spread. At times, the debate seems governed more by partisan political views than by scientific evidence. I have commented on this issue of masks in an earlier post.\nParadoxically, the publication Nov 18 2020 of the first randomized trial evaluating masks, DANMASK-19, during the current covid-19 pandemic and a meta- analysis of older trials seems to have heightened rather than reduced the uncertainty regarding their effectiveness.\nI explored this paradox in a BMJ opinion article which I briefly summarize in this post.\n\nNull hypothesis significance testing can create as many problems as it hopes to avoid. Main issues in poor interpretations of “negative” studies due to nullism and dichotomania.\nDANMASK study deemed “negative” but actually provides as much evidence for 33% reduction in infections as for no effect\nBayesian paradigm can help. Figure shows the probability of masks preventing infections (with a non-informative prior) in the wearer is 81%, quite a different interpretation than simply a “negative” study \nBayesian approach also allows interpreting DANMASK in the context of prior knowledge about masks in non-Covid settings as presented in Cochrane review. The data is consistent and robust (84% probability of benefit) \nEpi 101 - Don’t confuse rates with overall risk. DANMASK and previous pre-Covid mask studies reported infections avoided over 4 weeks. Not the same as risk prevented over the duration of a long (&gt;9 mo) pandemic -&gt; 36 (95%CI 12 -57) fewer infections per 1000 mask wearers\nBenefits could be greater in higher incidence setting, with better mask compliance, and if protection to others is considered\nEvidence is definitely not perfect but likely “good enough” given low cost and few side effects.\n\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Update on the Science Behind Masks},\n  date = {2020-11-24},\n  url = {https://brophyj.github.io/posts/2020-11-24-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Update on the Science Behind Masks.”\nNovember 24, 2020. https://brophyj.github.io/posts/2020-11-24-my-blog-post/."
  },
  {
    "objectID": "posts/2023-03-19-absence-of-evidence-is-not-evidence-of-absence/index.html",
    "href": "posts/2023-03-19-absence-of-evidence-is-not-evidence-of-absence/index.html",
    "title": "Absence of evidence is not evidence of absence",
    "section": "",
    "text": "A previous post referred to the difficulty some researchers / reviewers have in distinguishing between absence of evidence and evidence of absence, even though this issue has been well discussed for almost 30 years (see here and here). Despite the fact that these publications have several thousand citations, this remains an important problem, even in high impact medical journals.\nFor example, I recently read a NEJM randomized clinical trial (RCT) investigating whether the addition of extracorporeal cardiopulmonary resuscitation (eCPR) to standard conventional CPR (cCPR) can improve survival and diminish anoxic brain injury. Out-of-hospital cardiac arrest is a frequent event and its devastating consequences are only partially mitigated by rapid commencement of basic life support with high-quality cCPR. There remains a substantial subset of individuals who do not respond rapidly to these measures and whether eCPR can improve outcomes is an important clinical question. In this trial, the primary outcome, 30 day survival without significant neurological deficit, was an odds ratio of 1.4 (95% confidence interval, 0.5 to 3.5; P = 0.52) in favor eCPR. This lead to the abstract conclusion “In patients with refractory out-of-hospital cardiac arrest, extracorporeal CPR and conventional CPR had similar effects on survival with a favorable neurological outcome”.\nThis trial addressed an important clinical question in the most challenging of research environments and the authors are to be congratulated on their trial design, its execution, and a nuanced discussion in the body of the article. However the constraints of standard statistical analyses limits the quantitative appreciation of their data, and prevents a full and comprehensive data exploitation and updating of past knowledge. Trials that fail to meet statistical significance and are often incorrectly thought of as “negative” trials, and the null hypothesis significance testing (NHST) paradigm favors this confusion between “absence of evidence and evidence of absence”.\nThe goal of this post is to demonstrate that a Bayesian perspective, by concentrating on posterior probabilities, permits additional insights into the specific clinical question and intrinsically avoids these misinterpretations.\nThe post does not reiterate the many other reasons to be wary of null hypothesis significance testing (NHST), p values and confidence intervals (see here). Rather it assumes the reader has perhaps heard that Bayesian methods mirror our intuitive learning and diagnostic processes and is curious about its potential application to RCT analyses and interpretations. Bayesian approaches provide benefits over standard statistical analyses by avoiding dichotomizing results into statistical significance or not, with an obligatory loss of information and understanding. This is accomplished by concentrating on direct estimation of the parameters of interest and providing direct probability statements regarding their uncertainty (herein the risk of survival with intact neurological status). The price to be paid for these benefits is the need to specify a prior distribution before seeing the current data.\nThese probability statements arise from the posterior distribution according to the Bayes Thereom, expressed as follows: \\[ \\text{Posterior}  = \\frac{\\text{Probability of the data} * \\text{Prior}}{\\text{Normalizing Constant}} \\]\nTherefore, in addition to the current data summarized by the probability of the data (likelihood function), prior probability distributions are required. Because our main focus is the analysis and interpretation of the current RCT alone, an initial analysis may use a default vague parameter prior \\[log(\\theta) \\sim Normal [0, 2.50]\\], thereby assuring that the posterior distribution is dominated by the observed data.\nPosterior distributions are summarized with medians and 95% highest-density intervals (credible intervals (CrI)), defined as the narrowest interval containing 95% of the probability density function. Bayesian analyses permit not only calculations of the posterior probability of any additional survival with eCPR (OR &gt;1.00), but also of clinically meaningful benefits. While there is no universal definition for a clinically meaningful benefit, a survival OR &gt;1.10 may be an acceptable threshold for many. Bayesian analyses also allows calculation of the probability between any two points. For example, rather than simply comparing if the survival of one treatment is better than another, one can calculate a range of practical equivalence (ROPE) between treatments. While different ranges may be proposed, +/- 10% seems a reasonable small difference that many would consider as equivalent.\nThe graphical presentation of these results is shown below \nThis Bayesian analysis, using a default vague prior, produces an odds ratio (OR) 1.32 with 95% CrI 0.54 - 3.22) which aligns with the original analysis (OR, 1.4; 95% CI 0.5 - 3.5) confirms the minimal impact of the default vague prior and reveals a Bayesian analysis completely dominated by the observed data. The eCPR probability density function for improved survival with eCPR is 73% with a 66% probability that this exceeds the clinically defined meaningful cutpoint of at least a 10% survival benefit. The probability of equivalence between the two techniques is 13%.\nThe “take home” message from this Bayesian reanalysis is that standard statistical analyses resulting in a conclusion of “similar survival effects of eCPR to cCPR” may be overly simplified and potentially inaccurate. This Bayesian analysis demonstrates that at present definitive conclusions regarding the superiority, inferiority, or equivalence of either approach are impossible. Rather the possibility of a clinically meaningful benefit, or less likely the possibility of clinically meaningful harm, has not been reasonably excluded and continued research is necessary to clarify the residual uncertainties. The final dilemma confronting clinicians is that even Bayesian analyses of randomized trials provide only probability estimates for average treatment effects and not for the more elusive individual treatment effect.\nOf course, an added benefit of the Bayesian analysis is its ability to incorporate previous knowledge. The statistical code for this analysis can be found here and a more detailed non-peer reviewed manuscript may be found here.\n\n\n\nCitationBibTeX citation:@online{brophy2023,\n  author = {Brophy, Jay},\n  title = {Absence of Evidence Is Not Evidence of Absence},\n  date = {2023-03-19},\n  url = {https://brophyj.github.io/posts/2023-03-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2023. “Absence of Evidence Is Not Evidence of\nAbsence.” March 19, 2023. https://brophyj.github.io/posts/2023-03-19-my-blog-post/."
  },
  {
    "objectID": "posts/2020-04-15-virtual/index.html",
    "href": "posts/2020-04-15-virtual/index.html",
    "title": "Virtual Learning & Corporate Sponsorship",
    "section": "",
    "text": "One positive from the Covid-19 pandemic, is we have had to learn more techniques about online or virtual learning. The proliferation of these tools, such as Zoom, and our increased familiarity with their use will perhaps have a lasting impact on our learning processes. For example, with increased ease of online learning, perhaps we will reduce our carbon footprint for small, medium and even large medical conferences as we reduce our traveling and need for physical presence at these venues.\nCertainly in the academic world, we had to rapidly adapt (within a 2 week period) from giving standard onsite lectures to the virtual teaching world. Surprisingly, the transition was relatively painless. For example, I gave the last 12 hours of the winter semester of my intermediate epidemiology course to 71 MSc students via Zoom. The cost of this is farily trivial. I believe the price of a premium Zoom subscription is about $20 / month.\nGiven these facts I was not surprised to receive a recent email about an upcoming virtual course on heart failure.\n\n\n\n\nHowever, what did surprise me was seeing that this symposium had 12 corporate sponsors!\n\n\n\n\nThis raises several questions:\n- Why are academics so tied in with coroporate interests?\n- Are academics only willing to participate if personal financial rewards are present?\n- Coroporations are not philatrophic organizations, so what is the payoff for them?\n- Is it a quasi guarantee that their products will be highlighted in a positive manner?\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Virtual {Learning} \\& {Corporate} {Sponsorship}},\n  date = {2020-05-15},\n  url = {https://brophyj.github.io/posts/2020-05-15-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Virtual Learning & Corporate\nSponsorship.” May 15, 2020. https://brophyj.github.io/posts/2020-05-15-my-blog-post/."
  },
  {
    "objectID": "posts/2020-07-06-glad/index.html",
    "href": "posts/2020-07-06-glad/index.html",
    "title": "Glad to be at the back of the queue",
    "section": "",
    "text": "June 30 headlines reported that the US had bought all the reserves of remdesivir, leaving none for “the next three months of one of the two drugs proven to work against covid-19, leaving none for the UK, Europe, or most of the rest of the world.”\nIn a BMJ opinion piece, I outline why, contrary to many current sentiments, this action, although reflective of the apotheosis of a self centered nation with no appreciation for international collaboration, is not necessarily bad for the rest of the world. Basically, I summarize the many scientific limitations of current studies and their likely over-exaggeration of any meaningful benefits. Also not to be ignored are remdesivir’s lack of clinical success in treating other viral diseases, its exorbitant price, and cognitive biases about its purported benefits stemming from the overhyping of initial results.\nI argue that it is better to have the plutocratic American healthcare system dominate this market with an expenditure of $1.5 billion for such uncertain benefits. The money other countries save can surely be better spent on further research for this and other drugs as well as for public health measures, including testing, contact tracing, and maintaining universal healthcare, all notable lacunae in the American system.\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Glad to Be at the Back of the Queue},\n  date = {2020-07-06},\n  url = {https://brophyj.github.io/posts/2020-07-06-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Glad to Be at the Back of the Queue.”\nJuly 6, 2020. https://brophyj.github.io/posts/2020-07-06-my-blog-post/."
  },
  {
    "objectID": "posts/2021-01-15-excel-clearing-the-haze-with-bayes/index.html",
    "href": "posts/2021-01-15-excel-clearing-the-haze-with-bayes/index.html",
    "title": "EXCEL - clearing the haze with Bayes",
    "section": "",
    "text": "Yesterday I saw a MEDSCAPE article published last fall “Is the Tide Turning on the ‘Grubby’ Affair of EXCEL and the European Guidelines?” which chronicles this saga rather well. In fact, I was interviewed for the article but had missed its publication. The authors of the MEDSCAPE article detail some additional obfuscation of the data by the EXCEL authors which I, like most readers and even the NEJM editors, were unaware of.\nIn October 2017 I was asked by the European Society of Cardiology to perform a statistical review of their proposed left main revascularization guidelines and recommendation that percutaneous coronary stenting (PCI) was equivalent to coronary artery bypass grafting (CABG) for patients with left main disease. Despite being unaware of the recently reported shenanigans by the EXCEL authors, I concluded, largely based on Bayesian inference, that the evidence supporting their recommendation for equivalence of the 2 techniques was weak and their recommendation was misleading. For example, there remained an approximately 10% probability that the composite endpoint of death/MI could be augmented by at least 2 persons / 100 treated with PCI. As I believed most patients would find these differences to be clinically meaningful, I did not think that both procedures should receive the same class of recommendation. Nevertheless, as the evidence was not absolutely definitive, I recognized that others might reach a different conclusion and so was not completely surprised when the guideline recommendation was not modified.\nThe additional information in this MEDSCAPE article only strengthens my previous opinion as to the non-equivalence of these 2 techniques and the need for the ESC to revisit their guidelines. What I find most interesting is that the Bayesian approach brought clarity to this issue even before the most recent revelations.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {EXCEL - Clearing the Haze with {Bayes}},\n  date = {2021-01-15},\n  url = {https://brophyj.github.io/posts/2021-01-15-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “EXCEL - Clearing the Haze with Bayes.”\nJanuary 15, 2021. https://brophyj.github.io/posts/2021-01-15-my-blog-post/."
  },
  {
    "objectID": "posts/2020-07-08-bribes/index.html",
    "href": "posts/2020-07-08-bribes/index.html",
    "title": "Pharma, bribes and MDs",
    "section": "",
    "text": "$678 million fine testifies to how well these pharma bribes work, including the so-called CME events. Any bets that this is continuing today? Maybe more sophisticated and better camouflaged but still very present.\n\n\n\n\n\n\nIMHO\n\n\n\nBiggest bang for pharma’s buck is to target guideline writers.\n\n\nHere is an empirical example that may, or may not, be related to this story. In 2012 Canadian, European and US guideline writers offered guidelines on dual antiplatelet therapy (DAPT) for acute coronary syndromes. The comparison is straightforward as there was only 1 published RCT (PLATO), examining the standard clopidogrel therapy and the newer ticagrelor as so all guideline writers were working from the same evidence base. CDN and European guideline writers “recommended ticagrelor over clopidogrel” while the US writers (0 of 9 writers had a conflict of interest), looking at the same data concluded “does not endorse one over the other”. Look at the 2X2table below between conflicts of interest among the different guideline writers and the ticagrelor manufacturer.\nPlay of chance or …?\n\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Pharma, Bribes and {MDs}},\n  date = {2020-07-08},\n  url = {https://brophyj.github.io/posts/2020-07-08-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Pharma, Bribes and MDs.” July 8, 2020.\nhttps://brophyj.github.io/posts/2020-07-08-my-blog-post/."
  },
  {
    "objectID": "posts/2020-05-12-montreal-covid/index.html",
    "href": "posts/2020-05-12-montreal-covid/index.html",
    "title": "Montreal, a global Covid perspective",
    "section": "",
    "text": "I was curious to see where Montreal placed in the global Covid-19 perspective.\nLooking at these figures, Montreal data looks very concerning but some caveats must be acknowledged.\nMore testing would most likely help. If we were doing more testing and detecting less serious cases, our case fatality rate would not look so alarming. More importantly, more testing may lead to earlier identification of asymptomatic cases decreasing potential exposure to our more vulnerable populations thereby hopefully improving our overall death rates.\nHowever, it is unlikely that &gt; 10-15 fold differences observed can be explained by differences in testing, respecting public health measures, Covid virulence, genetics, or health care delivery. Information bias from variable and questionable data collection probably explains a lot of the variations. With questionable data, prediction models are associated with so much uncertainty their public health utility will be limited.\nIn addition to increased testing, we therefore need to enhance the reliability of data collection. This will be essential to better understanding our current and future positions and informing public policy.\n\n\n\nCitationBibTeX citation:@online{brophy2020,\n  author = {Brophy, Jay},\n  title = {Montreal, a Global {Covid} Perspective},\n  date = {2020-05-12},\n  url = {https://brophyj.github.io/posts/2020-05-12-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2020. “Montreal, a Global Covid Perspective.”\nMay 12, 2020. https://brophyj.github.io/posts/2020-05-12-my-blog-post/."
  },
  {
    "objectID": "posts/2021-12-17-medication-withdrawal-when-reasonable-people-disagree/index.html",
    "href": "posts/2021-12-17-medication-withdrawal-when-reasonable-people-disagree/index.html",
    "title": "Medication Withdrawal - When Reasonable People Disagree",
    "section": "",
    "text": "Recently, the following case was posted on Twitter\n50 yo M LAD STEMI s/p PCI 12 yrs ago. 280 lbs then. Now WFPB diet- 170 lbs. LDL of 45 on Lipitor 10. Asymptomatic, 12K steps daily. Finds me with a hope to get off BB, ACEI & aldactone. EF 30% before now 50% What would you do?\nThe opinion was offered by several cardiologists, whom I consider thoughtful and whom I respect, that he should continue the same treatment regime. My gut feeling was much more open to the patient’s desire to withdraw some of the prescribed drugs. When reasonable people disagree it typically reflects a lack of perfect evidence and highlights the need for careful deductive reasoning. Herein are my thoughts and reasoning on this question.\nAs an older member of the cardiology fraternity, I have a longer memory than some of the seminal trials that have shaped our treatment orthodoxies. Understanding where we are today requires a clear understanding of the past. Therefore let’s begin more or less at the beginning and look at the evidence for beta-blockers. The first trial of beta-blockers post MI with encouraging results (although not meeting the conventional norm for statistical significance) for improved was published in 1974 with alprenolol.\nIn 1981 the timolol study, a multicenter double-blind placebo controlled randomized study in patients surviving acute myocardial infarction, was published. With a follow-up to 33 months (median 17 months), mortality was reduced from 13.9% in the 939 patient placebo group to 7.7% in the 945 timolol patient group (p &lt; 0.001). Longer term follow-up was published and although the patients were no longer formally in the trial, the authors state that most patients were still on their assigned therapy.\n\nWith this 6 year follow up the cumulative incidence curves showed different early slopes with a timolol benefit being evident in the first year following randomization. After year 1, the early benefit was maintained but there was no indication of an additional benefit to longer beta-blocker exposure, with parallel slopes (see dashed lines) in the reproduced figure below.\n\nIt is worth noting the consistency of these results with those from the BHAT trial of 3837 post AMI patients randomized to propranolol or placebo. Placebo mortality at 25 months was reduced from 9.8% to 7.2% again with cumulative mortality risks being parallel after 12 months. The authors concluded “the use of propranolol in patients with no counterindications who had a recent MI is recommended for at least 3 years”.\nNot only is it unknown what would have transpired to the treated group had they stopped the beta blocker after the original 33 month observation period, but there is even more uncertainty about stopping the beta-blocker in this case, 12 years after initiation. Provided there is no excessive “kinetic mortality energy” stored and waiting to be released with beta blocker cessation, the question is which placebo incidence curve will the post beta blocker curve will follow. Will it be the increased placebo risk of year 0-1, in which case maintenance of the beta blocker likely remains beneficial. Or will it be the later placebo risk, which is identical to the beta blocker risks for the same time period, implying no advantage to its continuation.\nObviously, we do not know these counterfactual results, but a reasonable bet is the later time periods are more representative of the risk estimates and that slow withdrawal of the beta blocker is entirely justified. Certainly, this seems as justified as the extrapolation of benefits extending literally a decade beyond the original observed data This viewpoint is further strengthened after accounting for medical advances occurring since the original beta blocker studies were performed. In the ensuing years, we have learned that following AMI, overall survival is improved by primary PCI, aspirin, lipid lowering drugs and ACE inhibitors, all interventions our case patient is receiving (I’m assuming he is on aspirin even if not stated). Now if we have saved his life with aspirin, saved it again with primary PCI, saved it one more time with lipid lowering drugs and finally saved it another time with ACE inhibition, what is the possibility that the beta blocker is still necessary to continue saving his life. How many times can we save this same life?\nOn the other hand given in the absence of angina, there are no positive quality of life advantages to be expected from beta blockers. In contrast, there are multiple well known negative QoL metrics associated with beta blockers including fatigue, decreased exercise tolerance, sleep disturbances and impotency. Moreover, these side effects increase with age. Other issues that may enter into play are the patient’s desires and values, the additional drug costs, and a sentiment of being over medicalized which may negatively influence compliance with other more beneficial treatments, including those outside the realm of cardiovascular medicine (see increasing vaccine hesitancy).\nNow I have only looked at the question of beta blocker withdrawal as chronologically that was probably the first medication given to the patient and likely the one with the greatest negative impact on QoL. I do recognize the heresy of a cardiologist suggesting beta blocker withdrawal. Of course, beta blockers do have a role in chronic left ventricular dysfunction but here the studies addressed a population with depressed ejection fractions and so don’t provide any compelling reason for their continuation in this patient.\nThe same detailed examination of the continued use of other medications seems appropriate and may be the source of a future blog.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Medication {Withdrawal} - {When} {Reasonable} {People}\n    {Disagree}},\n  date = {2021-12-17},\n  url = {https://brophyj.github.io/posts/2021-12-17-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Medication Withdrawal - When Reasonable People\nDisagree.” December 17, 2021. https://brophyj.github.io/posts/2021-12-17-my-blog-post/."
  },
  {
    "objectID": "posts/2019-07-19-dig-is-ok/index.html",
    "href": "posts/2019-07-19-dig-is-ok/index.html",
    "title": "Dig is OK",
    "section": "",
    "text": "This is from a tweetorial looking at the safety and efficacy of digitalis in heart failure.\n\n\n\nCitationBibTeX citation:@online{brophy2019,\n  author = {Brophy, Jay},\n  title = {Dig Is {OK}},\n  date = {2019-07-19},\n  url = {https://brophyj.github.io/posts/2019-07-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2019. “Dig Is OK.” July 19, 2019. https://brophyj.github.io/posts/2019-07-19-my-blog-post/."
  },
  {
    "objectID": "posts/2024-02-05-what-is-the-prevalence-of-depression-in-medical-students/index.html",
    "href": "posts/2024-02-05-what-is-the-prevalence-of-depression-in-medical-students/index.html",
    "title": "What is the prevalence of depression in medical students?",
    "section": "",
    "text": "Background\nI am again this year teaching a small group learning course on critical appraisal for MPH students using a unique format. Each group is divided into three smaller subgroups of four or five students. The first subgroup received only an introduction to the problem, and without looking at the chosen article, discusses their choice of study design to address the problem. The second subgroup received only the published abstract and comments on what details they would expect to see in the full article. The third subgroup read the whole article and in addition to highlighting its strengths and limitations tried to answer queries raised by the first 2 groups. The goal is to not only improve critical appraisal skills but also to think about research questions, designs and the necessary compromises that are often required in research. Hopefully this adds another dimension to typical journal clubs where either an article in uncritically endorsed enthusiastically or trashed unmercifully. One of the articles was this 2016 JAMA article was a systematic review of the prevalence of depression or depressive symptoms among medical students. The data were extracted from 167 cross-sectional studies (n=116628) and 16 longitudinal studies (n=5728) from 43 countries. The authors state they followed the Preferred Reporting Items for Systematic Reviews andMeta-analyses and Meta-analysis of Observational Studies in Epidemiology reporting guidelines. They reported overall pooled crude prevalence of depression or depressive symptoms was 27.2% (37933/122356 individuals; 95% CI, 24.7% to 29.9%, \\(I^2\\)=98.9%) and concluded the following \n\n\n\n\n\n\n\n\nLeaving aside some contextual issues such as\n- the reliability of self reporting as opposed to a structured diagnostic interview\n- the ability of screening tools to accurately diagnosis depression (low positive predictive values) I think it is important to take the time to reflect on the statistical models and their proper interpretation.\nStatistical models in meta-analysis\nThe authors reported using a random effects model. What does this mean and what the implications?\nA meta-analysis is actually nothing more than a mixed model, which is itself an extension of simple regression model that contains both explained (fixed) and unexplained (random) elements through the use of variance components.\nFirst the fixed effect, where the variance is the sampling variance for each study where the studies are assumed to all arise from one common distribution. Graphically, this is displayed as follows where the distribution remains centered for each study and its spread it determined by the respective variances. \n\n\n\n\n\n\n\n\n A typically more realistic random effects model considers not only this within sample variation but acknowledges it is unlikely that individual studies are identical and consequently also exhibit between study variation, as measured by the \\(I^2\\) statistic, as shown below \n\n\n\n\n\n\n\n\nIOW, the individual studies may be considered as drawn from a supra-population distribution of potential studies, as indicated by the green curve above and blue curve below\n\n\n\n\n\n\n\n\nReturn to the JAMA article\nRecall the results of an overall pooled crude prevalence of depression or depressive symptoms was 27.2% (37933/122356 individuals; 95% CI, 24.7% to 29.9%, \\(I^2\\)=98.9%).\nSome general caveats\nA rough guide to \\(I^2\\) interpretations is as follows:\n0% to 40%: might not be important;\n30% to 60%: may represent moderate heterogeneity\n50% to 90%: may represent substantial heterogeneity\n75% to 100%: considerable heterogeneity\nWhat does this mean?\nIt means if we are combining studies to compare apples to oranges and start including studies that are comparing apples to bananas, apples to grapes and oranges to pineapples, etc we are to end up with a fruit salad rather than an understanding of the original comparison being sought (leaving aside the possibility of performing a network meta-analysis). Even if only apple versus orange studies are included, if they are of poor quality where perhaps the “appleness” or “orangeness” has been poorly measured we are unlikely to get reliable results. Summarizing,- GIGO (garbage in - garbage out)\n- excessive unexplained heterogeneity = fruit salad\nAnalyses\nConsider their Figure 7 (chosen since the 183 studies have been grouped into 23 groups making data extraction more manageable). \n\n\n\n\n\n\n\n\nQuestion: Does this Figure represent a meta-analysis?Answer: Given that a meta-analysis is the statistical combination of the results of multiple studies to quantify the effect size across all of the studies, and given its absence (as well as any measure of the combined variation), it is hard to label this Figure a meta-analysis.\nHere is a Figure that does include the desired measures.\n\n\n\n\n\n\n\n\nThe combined effect shows a results that is comparable with the reported depression prevalence of 27.2% (95% CI, 24.7% to 29.9%) with the difference due to the original analysis not grouping the 183 studies.Question: What is the dotted line and what does it represent?Answer: This is known as the prediction interval. Remember that in a random effects model the studies come from an over-riding population distribution and the 95% CI of this distribution is the prediction interval. Therefore the next study would come from a draw from this distribution and NOT from a draw of distribution of the mean effect shown in the line above.\n\n\n\n\n\n\n\n\nQuestion: What is probability of the prevalence of depression in the next study (or in your medical school)?Answer: Although 95% CI are often interpreted as probability intervals, this is actually incorrect. To obtain probability statements one needs to switch to a Bayesian paradigm. This is beyond the scope of this presentation but fortunately, given the large amount of available data, little error is introduced by not doing the formal Bayesian analysis. This is shown by the figure below, produced by a Bayesian analysis with vague priors, which gives essential the same result as the frequentist analysis.\n\n\n\n\n\n\n\n\nThe blue areas represent the probability distributions and the thin and thick black lines the 80% and 95% credible intervals (CrI). The mean and 95% CrI are 0.314, 0.250 - 0.379, very close to the previous confidence intervals.\nSince we are interested in the next study we need the prediction interval distribution which follows\n\n\n\n\n\n\n\n\nConclusions\nWhat to think about the authors’ conclusion “the prevalence of depression among medical students is 27%”?\nIgnoring the non-trivial limitations of depression screening tools as diagnostic tools and the lack of validity for self administration, a proper understanding of the statistical model suggests that this meta-analysis of these diverse publications reveals the next study is likely to show at least some medical students have depressive symptoms but this is likely less than 75%!\nIOW, this study is not very informative and its implied precision is a misrepresentation.\nCaveat lector Figure on the top is often presented but figure on the bottom better reflects reality\n\n\n\n\n\n\n\nCaveat lector\n\n\n\n\n\n\n\n\nCaveat lector\n\n\n\n\n\n\nCaveat lector\nCaveat lector\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {What Is the Prevalence of Depression in Medical Students?},\n  date = {2024-02-05},\n  url = {https://brophyj.github.io/posts/2024-02-05-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “What Is the Prevalence of Depression in\nMedical Students?” February 5, 2024. https://brophyj.github.io/posts/2024-02-05-my-blog-post/."
  },
  {
    "objectID": "posts/2024-02-19-does-it-make-a-difference-sedentary-break/index.html",
    "href": "posts/2024-02-19-does-it-make-a-difference-sedentary-break/index.html",
    "title": "Does it make a difference - sedentary break",
    "section": "",
    "text": "Background\nRecall that the goal of the 605 course is to not only improve critical appraisal skills, but also to think about research questions, designs, and the necessary compromises that are often required in research. Hopefully this adds another dimension to typical journal clubs where either an article in uncritically endorsed enthusiastically or trashed unmercifully, although this is sometimes quite merited This week’s selected article was the 2023 publication Breaking Up Prolonged Sitting to Improve Cardiometabolic Risk: Dose–Response Analysis of a Randomized Crossover Trial concluding\n\nThe present study provides important information concerning efficacious sedentary break doses. Higher-frequency and longer-duration breaks (every 30 min for 5 min) should be considered when targeting glycemic responses, whereas lower doses may be sufficient for BP lowering.\n\nIn this paper, of 25 participants who attended a screening visit, 18 were randomized and 11 completed the randomized crossover designed study which investigated 5 different strategies to examine the acute effects of multiple doses of a light-intensity walking-based sedentary break intervention on cardiometabolic risk factors among middle- and older-age adults. The trial conditions consisted of one uninterrupted sedentary (control) condition and four acute (experimental) conditions that entailed different sedentary break frequency/duration combinations: (1) light-intensity walking every 30 min for 1min, (2) light-intensity walking every 30 min for 5 min, (3) light-intensity walking every 60 min for 1 min, and (4) light-intensity walking every 60 min for 5 min. As the largest response was for glucose differences, will restrict this commentary to that outcome.\nBefore applying any reanalyzing of their data, let’s just stop for a moment and ask ourselves the following question\n\nHow likely do we think a study with only 11 individuals can detect meaningful glucose difference in glucose measurements with these anti sedentary strategies? Would one expect the differences to be so large that they could be detected by this small a sample size?\n\nIn any case, their conclusion appears supported by their published Figure 1 as shown here\n\n\n\n\n\n\n\n\n This Figure is remarkable for 2 main points\n1. The early separation between the control and intervention groups which appears maximum for 5 minutes exercise every 30 minutes\n2. The outcome is not the glucose level from each randomized but the difference in level compared to the control (baseline) group. \nThe early outcome difference\nIn the supplemental material, the authors report summary glucose levels for each group at 15 minute intervals. Using these values, we may simulate the glucose measurements for each group. Let’s consider the values at 45 minutes after T0. Given the authors state that the first 20 minutes are assigned to a standardized meal and that no internvetion occurs before 30 minutes, any variation in the 45 minutes values can’t be due to the intervention. Yet, plotting an analyzing this data (ANOVA) reveals the following\n\n\n[1] \"Analysis of variance using outcome of group glucose - control glucose\"\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)\ngroup        3   1953     651      10 0.000046\nResiduals   40   2594      65                 \n\n\n\n\n\n\n\n\nFinding these large differences even before any of the interventions could take effect should be a red flag for reservations about the final conclusions.The outcome measure\nUsing the outcome measure as the difference in glucose levels between the active treatments strategies and the control (baseline) group is a potentially fatal flaw. The fatal flaw is using the difference from baseline as their outcome measure. Bland and Altman have published about why this “is biased and invalid, producing conclusions which are, potentially, highly misleading. The actual alpha level of this procedure can be as high as 0.50 for two groups and 0.75 for three”. In short, we need to remember what is being randomized, it is the assignment to a given treatment strategy. Individuals are not randomized to their baseline glucose levels, any more than they are randomized to their weights, heights, eye color, or any other characteristic. With small sample sizes we may well expect that even in randomized samples, there may be meaningful differences in these characteristics, therefore including them as a component of the outcome is not appropriate and may bias the results.Harrell lists all the many assumptions required to be met before analyzing change from baseline could (potentially) be used.\ni. the variable is not used as an inclusion/exclusion criterion for the study, otherwise regression to the mean will be strong\nii. if the variable is used to select patients for the study, a second post-enrollment baseline is measured and this baseline is the one used for all subsequent analysis\niii. the post value must be linearly related to the pre value\niv. the variable must be perfectly transformed so that subtraction “works” and the result is not baseline-dependent\nv. the variable must not have floor and ceiling effects\nvi. the variable must have a smooth distribution\nvii. the slope of the pre value vs. the follow-up measurement must be close to 1.0 when both variables are properly transformed (using the same transformation on both)\nWith this in mind, this study doesn’t meet these assumptions (according to their CONSORT Fig 1, glucose was an entrance criteria, therefore possible regression to the mean may be present, glucose has a definite “floor effect”, and no proof that model is linear).\nTo demonstrate the bias inherent in using change from baseline as opposed to final glucose reading alone, let’s again use the simulated data. We will analyze (ANOVA) and plot the outcome according to treatment where the outcome is i) the final glucose reading in each group or ii) ii) the change from baseline for each group.\n\n\n\n\n\n\n\n\nThis suggests the original analysis is quite wrong and there is likely no difference between the strategies being tested. This is no surprise as with only 11 (or 9) subjects), even with a crossover design, the differences would have to be to very large to reach statistical significance. See the common sense response to the opening question above.\nThere are some other additional discussion points to consider.\n1. While 18 subjects were randomized only 11 were analyzed and 2 of them had missing values. This raises the possibility of a non-quantifiable selection bias.\n2. The authors report a post-hoc power calculation, a statistically inappropriate and nonsensical technique. If a nonsignificant finding was obtained, power will always be low to detect the observed effect size, as observed power is directly related to the obtained P value, with the former providing no additional information than the latter.\n3. There is no discussion of whether these measured outcomes have any clinical relevance. Suppose against all reason, the true glucose incremental area under the curve differential for the best treatment strategy was indeed the reported -11.8 mg% over 8 hours. This translated to 1.4 mg% / hour for an 8 hour day or 0.47 mg% / hour over 24 hours. How likely would this small (i.e. trivial) a difference have any meaningful clinical effect? Important to recall the adage“Measure what is important and don’t make important what you can measure”\n\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {Does It Make a Difference - Sedentary Break},\n  date = {2024-03-05},\n  url = {https://brophyj.github.io/posts/2024-02-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “Does It Make a Difference - Sedentary\nBreak.” March 5, 2024. https://brophyj.github.io/posts/2024-02-19-my-blog-post/."
  },
  {
    "objectID": "posts/2021-10-12-heparin-in-hospitalized-covid-19-patients/index.html",
    "href": "posts/2021-10-12-heparin-in-hospitalized-covid-19-patients/index.html",
    "title": "Heparin in hospitalized COVID-19 patients",
    "section": "",
    "text": "Spyropoulos and colleagues published randomized clinical trial on therapeutic-dose heparin in hospitalized COVID-19 patients where they concluded that the primary efficacy outcome was reduced in non-intensive care unit (ICU) patients, but not in ICU patients. In the intention-to-treat population, the authors reported a relative risk (RR) between the therapeutic dose and standard dose was 0.46 (95% confidence interval [CI] 0.27 - 0.81; p = 0.004) in the non-ICU stratum and 0.92 (95% CI 0.62 - 1.39; p = 0.71) in ICU stratum. Due to this difference in statistical significance, the authors concluded that the effect was only present in the non-ICU stratum.\nAlong with colleagues Arthur Albuquerque and Carolina Santolia, we felt the statistical analysis supporting this conclusion was questionable and therefore submitted a letter to the editor which was not accepted for publication as the editors were “unable to assign your letter a sufficient priority for publication in JAMA Internal Medicine”. Consequently in addition to submitting the letter to PubPeer, I have reproduced the letter in this post.\nAs noted by Altman and Bland, statistical analysis should be targeted to the clinical question: is the effect of therapeutic-dose heparin different between ICU and non-ICU patients? To answer this question, one should directly compare the efficacy of therapeutic-dose heparin between non-ICU and ICU patients using am interaction test, which was not performed in this study. Thus, the authors cannot conclude that the treatment effect differed between these subgroups based on their simple comparison of statistically significant and statistically insignificant p values. We applied the appropriate interaction analysis and found a ratio of relative risks equal to 0.5 [95% CI 0.25, 0.99; p = 0.046] between non-ICU and ICU patients.\nBeyond applying the proper statistical test, it must also be properly interpreted. If complete equipoise existed in the null hypothesis of no therapeutic difference between the different hospital populations before this study, then after observing this data there remains a 13% probability that the null hypothesis is true. Of course, if there existed a stronger prior belief in the null hypothesis, then the posterior belief in the null hypothesis of no difference would be even greater than 13%, despite the interaction p value &lt; 0.05.\nHighly uncertain times, such as the COVID-19 pandemics, require rapid evidence to aid clinicians in making decisions at the bedside and reliable subgroup analyses are of obvious importance in enhancing personalized clinical decision-making. However, this requires both the appropriate statistical analysis and its proper interpretation.\n\n\n\nCitationBibTeX citation:@online{brophy2021,\n  author = {Brophy, Jay},\n  title = {Heparin in Hospitalized {COVID-19} Patients},\n  date = {2021-10-12},\n  url = {https://brophyj.github.io/posts/2021-10-12-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2021. “Heparin in Hospitalized COVID-19\nPatients.” October 12, 2021. https://brophyj.github.io/posts/2021-10-12-my-blog-post/."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html",
    "title": "Misinformed conclusions",
    "section": "",
    "text": "In three papers that I read this week, I believe that each suffers from misinterpretations in their conclusions[@RN6772][@RN6776][@RN6773]."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#likely-unintentional-but-misinformation-nevertheless",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#likely-unintentional-but-misinformation-nevertheless",
    "title": "Misinformed conclusions",
    "section": "",
    "text": "In three papers that I read this week, I believe that each suffers from misinterpretations in their conclusions[@RN6772][@RN6776][@RN6773]."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#example-1",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#example-1",
    "title": "Misinformed conclusions",
    "section": "Example 1",
    "text": "Example 1\n\n\n\n\n\n\n\n\nIn the first paper[@RN6772], the authors conclude\n\n“In Medicare patients with pure native AR, TAVR with the current commercially available transcatheter valves has comparable short-term outcomes. Although long-term outcomes were inferior to SAVR, the possibility of residual confounding, biasing long-term outcomes, given older and frailer TAVR patients, cannot be excluded.”\n\nWhile acknowledging that TAVR was associated with higher unadjusted (HR, 1.90; 95% CI, 1.59-2.26; P &lt; .001) and adjusted risk of all-cause mortality compared with SAVR (adjusted HR, 1.41; 95% CI, 1.03-1.93; P = .02) with long term follow-up (which may indeed be at least partially explained by residual confounding), the authors interpret the results as being comparable for the two techniques at one year. They report one year propensity score adjusted mortality as 5.7% and 6.9% mortality in the TAVR and SAVR patient, respectively (p=0.3).\nDrawing conclusions based on p values is known to be dangerous and a proposed, albeit perhaps minor, improvement is to use confidence intervals[@RN5420]. The following shows the risk difference with the 95% CI for the two mortality outcomes.\n\nCodeprop.test(c(round(c(.057*9880,.069*1147))),c(9980,1147))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(round(c(0.057 * 9880, 0.069 * 1147))) out of c(9980, 1147)\nX-squared = 2.7, df = 1, p-value = 0.1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.028287  0.003362\nsample estimates:\n prop 1  prop 2 \n0.05641 0.06888 \n\n\nThe implies that even ignoring any biases, concluding that the two approaches have similar short term outcomes is only reasonable if one believes that a possible 2.8% absolute mortality reduction with SAVR from sampling variation alone is not clinically important. Others might consider that the conclusion is another example of conflating absence of evidence with evidence of absence and that additional data is required before drawing firm conclusions. This underscores the problem with dichotomized p values and risk ratios (as opposed to risk differences) as decision making tools."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#example-2",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#example-2",
    "title": "Misinformed conclusions",
    "section": "Example 2",
    "text": "Example 2\nA NEJM article[@RN6776] reported P that\n\n“the prophylactic use of tranexamic acid during cesarean delivery did not lead to a significantly lower risk of a composite outcome of maternal death or blood transfusion than placebo” (RR 0.89; 95% CI 0.74 to 1.07; P=0.19 for the primary outcome).”\n\nThese results could also be expressed as risk differences and attributable risks.\n\nCodemat1 &lt;- matrix(c(233,201,5238,5328), nrow = 2, \n              dimnames = list(c(\"placebo\",\"tranexamic acid\"),c(\"Outcome +\",\"Outcome -\")))\nmat1\n\n                Outcome + Outcome -\nplacebo               233      5238\ntranexamic acid       201      5328\n\nCodeepiR::epi.2by2(mat1)\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          233         5238       5471        4.26 (3.74 to 4.83)\nExposed -          201         5328       5529        3.64 (3.16 to 4.16)\nTotal              434        10566      11000        3.95 (3.59 to 4.33)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 1.17 (0.97, 1.41)\nOdds ratio                                     1.18 (0.97, 1.43)\nAttrib risk in the exposed *                   0.62 (-0.10, 1.35)\nAttrib fraction in the exposed (%)            14.64 (-2.70, 29.05)\nAttrib risk in the population *                0.31 (-0.30, 0.92)\nAttrib fraction in the population (%)         7.86 (-1.76, 16.57)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 2.820 Pr&gt;chi2 = 0.093\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.096\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\nThe risk difference is 6 fewer outcomes /1000 treated (95% CI-13.5, 1.0) and the attributable fraction is 14.6% (95% CI -2.70, 29.05) of outcomes being eliminated by treatment.  While these measures do not reach statistical significance, they may provide some additional insights into the trial’s interpretation as the associated sampling variations suggest an effect size as large as 13 fewer outcomes / 1000 treated and an attributable fraction as large as 29% have not been eliminated. If one believes these potential measures are of clinical importance, then continuing research with this agent may be indicated."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#example-3",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#example-3",
    "title": "Misinformed conclusions",
    "section": "Example 3",
    "text": "Example 3\nThe third paper is entitled ” Effect of a Run-In Period on Estimated Treatment Effects in Cardiovascular Randomized Clinical Trials: A Meta-Analytic Review”[@RN6773] and the authors concluded “The use of a run-in period was not associated with a difference in the magnitude of treatment effect among cardiovascular prevention trials”.\nI found this result to be a priori very surprising. However without the raw data it is difficult to completely reproduce and assess what the authors have done. Nevertheless working with the aggregate data from the last column in Figure 1, I have performed a Bayesian analysis, with a vaguely informative prior such that the results are completely dominated by the published data.\nThe posterior probability is displayed below.\n\n\n\n\n\n\n\n\nggplot version\n\n\n\n\n\n\n\n\nBased on this analysis, the probability that run-in trials has a larger treatment effect than non-run-in trials was 99 %. There was an 79 % probability that the effect size was at least 5% greater in the run-in trials. While this effect is not large, it is more in keeping with face validity that would suggest that run-in trials, by excluding non-compilers and those developing side effects, would be expected to yield a larger effect size than a comparable trial without a run-in period."
  },
  {
    "objectID": "posts/2023-04-15-misinformed-conclusions/index.html#references",
    "href": "posts/2023-04-15-misinformed-conclusions/index.html#references",
    "title": "Misinformed conclusions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/2022-12-05-scientific-authorship/index.html",
    "href": "posts/2022-12-05-scientific-authorship/index.html",
    "title": "What does scientific authorship mean?",
    "section": "",
    "text": "I recently reviewed a paper with 18 authors which reached a conclusion of no morality difference between 2 contrasts (RR 1.0, 95% CI 0.5-2.0). Now this is an obviously faulty interpretation that mixes “absence of evidence with evidence of absence”. This got me thinking about scientific authorship and raised several existential questions.\n1. Are all 18 authors statistically incompetent or is it only the 1 or 2 authors who probably did the majority of the work that are deficient? 2. If it is only 1-2 authors who are statistically incompetent, then what of the other 16-17 authors? Did they simply not bother to critically read a manuscript to which they signed names?\n3. Do these individuals, and by extension clinicians in general, know what constitutes authorship?\n4. How likely is it that these 18 individuals all meet authorship criteria?\n5. How seriously do journals view authorship criteria and do they take any measure towards verification? The International Committee of Medical Journal Editors (ICMJE) have produced an article that is helpful in defining the role of authors and contributors which can be found here and assists in answering some of the above questions. The documents underscores that“authorship confers credit and has important academic, social, and financial implications. Authorship also implies responsibility and accountability for published work.” The ICMJE document recommends that authorship be based on the following 4 criteria:\n1. Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work;AND\n2. Drafting the work or revising it critically for important intellectual content;AND\n3. Final approval of the version to be published;AND\n4. Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\nThe ICMJE document also mentions several non-authorship contributions including one that is perhaps especially germane for clinicians “provided and cared for study patients”\nSo returning to my existential questions;\n1. I don’t know the answer but prefer to believe that the problem of statistical incompetence involved all 18 authors, although about 75% of respondents to my twitter poll believed the opposite (see next point as to why I prefer this option)\n2. I find it preferable to believe in a general problem of statistical incompetence, which is correctable with simple educational initiatives, over the alternative hypothesis of a lack of integrity, which seems more difficult to readily address.\n3. I strongly doubt that most authors are aware of the ICMJE authorship criteria\n4. I think the probability of all 18 meeting the authorship criteria is about zero.\n5. My personal experience, admittedly perhaps somewhat selected and limited, suggests that authorship criteria is not of major concern for journal editors. Not sure where this little rant takes me, other than to state the obvious that medical publishing needs to pay more attention to authorship criteria to assure its overall scientific quality, credibility and accountability. \nAlternatively, the message about authorship and the quality of the associated science is perhaps better captured by a XKCD inspired cartoon that has been adapted for publications in epidemiology\n\n\n\n\nCitationBibTeX citation:@online{brophy2022,\n  author = {Brophy, Jay},\n  title = {What Does Scientific Authorship Mean?},\n  date = {2022-12-05},\n  url = {https://brophyj.github.io/posts/2022-12-05-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2022. “What Does Scientific Authorship Mean?”\nDecember 5, 2022. https://brophyj.github.io/posts/2022-12-05-my-blog-post/."
  },
  {
    "objectID": "posts/2024-02-12-endotoxin-exposure-in-asthmatic-children-and-matched-healthy-controls/index.html",
    "href": "posts/2024-02-12-endotoxin-exposure-in-asthmatic-children-and-matched-healthy-controls/index.html",
    "title": "Endotoxin exposure in asthmatic children and matched healthy controls",
    "section": "",
    "text": "Background\nRecall that the goal of the course is to not only improve critical appraisal skills but also to think about research questions, designs, and the necessary compromises that are often required in research. Hopefully this adds another dimension to typical journal clubs where either an article in uncritically endorsed enthusiastically or trashed unmercifully, although this is sometimes quite merited For reasons I am somewhat unclear about, this week’s selected article was the 2005 publication Endotoxin exposure in asthmatic children and matched healthy controls: results of IPEADAM study. Although not described as such, this was apparently a nested case-control study from a 1999 crosssectional study by the same authors, of over 1500 completed questionnaires. Cases were children between the ages of 4 and 17 who had a study diagnosis of probable asthma. These asthmatic children were matched for sex, age and sib-ship size with children living in asthma free households and the following main conclusions reported. \n\n\n\n\n\n\n\n\nA critical analysis should consider bias and statistical issues\nBiases\nIt is helpful to consider biases in observational studies according to the following grouping\n- selection\n- misinformation\n- confounding\nSelection bias\nSelection bias is a distortion in the measure of association due to a sample selection such the measured association is not representative of what exists in the target population.\nIf the selection of cases and controls is not done independently of the exposure, then when you compare exposure distributions any observed differences are a combination of a true effect and an artifact of the way the data was collected.\nThis figure may help explain. Where would you put the reinforcements?\n\n\n\n\n\n\n\n\n\nAfter identification of the research question, the next step is to distinguish among the various populations, including the\n- target population (all children in the UK)\n- study population (all those with a chance of being in the study, ie the 1500 children who completed the original questionnaire)\n- sample population (the 200 children in Figure 1)\n- analyzed sample population (the 90 matched pairs of children)\nIn their previous work, the authors report an asthma prevalence of approximately 20%, or about 300 children based on their study population of 1500. The following questions need answering.\n- Does the study population (?1500) reflect the target population?\n- Do the 200 sample children reflect the total study population ?\n- Do the 90 matched pairs, the analytical sample population reflect the overall sample population? What about the missing cases?Many reasons to be concerned about possible selection bias in this study.\nLet’s begin by examining their Figure 1 which is reproduced below. \n\n\n\n\n\n\n\n\n\nHow many asthma cases (and controls) did they identify in this Figure?\nThe figure shows 105 cases and 93 controls, a total of 198 children. Yet the authors claim this figure shows the age distribution of 200 children. While the difference is small, it does raise further concerns about the reliability of their data analysis when there is a discordance in a simple counting of individuals.\nWhy do they only use 90 cases? Presumably to meet their matching criteria, age (+/- 1 year). Below is the best matching I could create from their data.\n\n\n\n\n\n\n\n\n\nThe question then becomes why match?\nMost people think matching is to control for confounding but this is only partly true (and indeed may be more than offset by potential selection bias) and the best reason is to improve precision in situations of sparse data.\nMoreover, in this case a better alternative would have been not to match but to control for age in regression analysis as this would have allowed an estimation of the effect of age on the detection of asthma, while using the whole study sample of 198 children, including all 105 cases.\nPerforming logistic regression glm(stat~ages, data = data_long_uncount, family = \"binomial\") shows that each year of additional age is associated with a odds ration decrease in asthma diagnosis = OR = 0.92, 95%CI 0.89 - 0.96\nMisclassification\nThe authors don’t report the raw data for the endotoxin exposure but assuming the following distribution among cases and controls gives an OR that approximates their results\n\n\n--Observed data-- \n         Outcome: Exposed+ \n       Comparing: Cases vs. Controls \n\n         Cases Controls\nExposed+    40       50\nExposed-    27       63\n\n                            2.5% 97.5%\nObserved Relative Risk: 1.3  1.0   1.8\n   Observed Odds Ratio: 1.9  1.0   3.4\n---\n                                                   \nMisclassification Bias Corrected Relative Risk: 1.3\n   Misclassification Bias Corrected Odds Ratio: 1.9\n\n\nAs the authors report from their previous work that their asthma detection questionnaire has only a 70% sensitivity and 91% specificity misclassification is present.\nThe effect of misclassification can be quantified using quantitative bias analysis (QBA) via the episensr package and misclassification function. This seems an improvement over purely qualitative heuristics, such as “non-differential misclassification biases toward the null”.\n\nCodelibrary(episensr)\nmisclassification(matrix(c(40, 50, 27, 63),\n                          dimnames = list(c(\"Exposed+\", \"Exposed-\"), c(\"Cases\", \"Controls\")),\n                         nrow = 2, byrow = TRUE),\n                  type = \"outcome\",\n                  bias_parms = c(.70,.70,.91,.91))\n\n--Observed data-- \n         Outcome: Exposed+ \n       Comparing: Cases vs. Controls \n\n         Cases Controls\nExposed+    40       50\nExposed-    27       63\n\n                            2.5% 97.5%\nObserved Relative Risk: 1.3  1.0   1.8\n   Observed Odds Ratio: 1.9  1.0   3.4\n---\n                                                   \nMisclassification Bias Corrected Relative Risk: 1.4\n   Misclassification Bias Corrected Odds Ratio: 3.6\n\n\nConceptually, this misclassification of cases and controls suggest any association between indoor air pollutants and asthma will likely be underestimated and this analysis permits an estimate of the order of magnitude.\nConfounding\nHow much residual or unmeasured confounding would be required to wipe out the observed effect? This can be determined using the EValue package as described in this paper.\n\n\n         point lower upper\nRR         1.4   1.1   1.8\nE-values   2.1   1.3    NA\n\n\nThis suggests that a moderate confounder with a risk of 2.1 fold to exposure and a 2.1 fold increase in the outcome would be required to eliminate the observed risk. Of course if the true OR is greater due to misclassification bias the size of unmeasured confounder needed to eradicate the observed association would be even larger.\nIn conclusion, there are multiple potential biases (selection, misclassification and confounding) associated with this study. Unmeasured confounding is unlikely to explain the observed association and misclassification suggest the association may be underestimated, but the inability to evaluate the magnitude and direction of the selection bias limits definitive determination of the overall direction of the bias assessment.\nAnalyses\nThe authors they performed a logistic regression where presumably probable asthma is the dependent variable. In this case understanding their table 2 is difficult.\n\n\n\n\n\n\n\n\n From Table 2 the authors make causal statements not only about endotoxins and asthma but also about other variables, including for example, dampness and single parent home.\nMaking assertions based on regression coefficients from a multivariable analysis is subject to Table 2 Fallacy. These covariate effect estimates may also be confounded even though the effect estimate for the main exposure is not confounded and their proper interpretation far from obvious. The authors state “There was no difference in the levels of exposure to house dust major allergen Der p 1, expressed in quartiles of exposure between asthmatic and non-asthmatic matched controls (Table 3)”\n\n\n\n\n\n\n\n\n Looking at the confidence intervals we see they are very wide and, for example, don’t exclude that for quartile 2 there is a 180% increase or 64% decrease in asthma prevalence compared to quartile 1. The authors have conflated “an absence of evidence with evidence of absence”. Aslo see this previous post.\nThe authors make the same error in Table 4\n\n\n\n\n\n\n\n\n when they conclude there are no interactions. The more reasonable conclusion being even if one ignores the potential biases, they lack sufficient power to draw conclusions about interactions.\nConclusion\nA critical appraisal of this publication has shown several interesting avenues for discussion, perhaps explaining why it was suggested for review. Based on the critical appraisal it would seem there is little to support their original conclusions.\n\n\nCitationBibTeX citation:@online{brophy2024,\n  author = {Brophy, Jay},\n  title = {Endotoxin Exposure in Asthmatic Children and Matched Healthy\n    Controls},\n  date = {2024-02-19},\n  url = {https://brophyj.github.io/posts/2024-02-19-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2024. “Endotoxin Exposure in Asthmatic Children and\nMatched Healthy Controls.” February 19, 2024. https://brophyj.github.io/posts/2024-02-19-my-blog-post/."
  },
  {
    "objectID": "posts/2023-03-22-something-old-and-something-older/index.html",
    "href": "posts/2023-03-22-something-old-and-something-older/index.html",
    "title": "Something old and something older",
    "section": "",
    "text": "I’m co-teaching a small group learning course on critical appraisal for our MPH students. We tried to avoid the common theme of many critical appraisal courses and journal clubs that simply try to trash whatever article is under review. Rather we divide the groups into three sections and the first one receives only an introduction to the problem, and without looking at the chosen article, discusses what sort of study design, with the associated strengths and limitations, they would consider to address the problem. The second group receives only the abstract and comments on what details they would expect to see in the article. The third group reads the whole article and in addition to highlighting its strengths and limitations tries to answer queries raised by the first 2 groups. The basic idea is not only to improve critical appraisal skills but also to think about possible research designs and the necessary compromises that are often required in doing research.\nThis week one of the other co-instructors (for reasons that will become evident later) chose an older NEJM article from 2007 on the effectiveness of influenza vaccine in the community-dwelling elderly to evaluate. Despite several strengths including a prospective cohort design with a large sample size and controling for several important confounders, the reported 50% reduction in all cause mortality for the vaccinated group is an obvious red flag that a substantial bias exists in the execution of this observational study.\nLetters to the editor immediately following the article’s publication noted that the mortality benefit exceeded all reasonable estimates, as influenza mortality during the winter months typically accounts for 5% or less of total mortality. In these letters, the perceived bias was attributed to residual and unmeasured confounding. While there is undoubtedly some degree of residual confounding, it seems unlikely that it could explain this magnitude of bias, from a theoretical ceiling of an approximate 5% mortality decrease to the reported 50% decrease. The clue to resolving this bias is found by a careful reading of their methods\nAll noninstitutionalized members of the plans were included in that season’s cohort if they were 65 years of age or older as of October 1, had been continuously enrolled in the plan for the preceding 12 months, were alive on the first day of the influenza season, and were either continuously enrolled or died during the outcome period.\nSince it is virtually impossible that the vaccinated individuals all received their vaccination on Oct 1 of each flu season, it appears that the researchers looked in the future to determine their vaccinated and unvaccinated groups. This leads to an obvious misclassification of exposure time. This neglect of the period of exposure to risk was well described by Bradford Hill (of the causal checklist fame) in a 1937 Lancet publication where he even provides a numerical example involving inoculation! The data for his example is presented below with both the true and fallacious comparisons between inoculated and non-inoculated subjects where he assumed the true attack rate was identical in both groups. By neglecting the time of inoculation, the true risk ratio of 1 gives a biased result of a 50% reduction.  Inspired by Hill’s example, I re-examined the 2007 data. Now the authors didn’t give us the raw data, not even the total number of deaths in either exposure group nor the timing of vaccinations so some assumptions must be made. They did provide the total number of vaccinated (415,249), unvaccinated (298,623) and total number of deaths (8796). It seems safe to assume that the vaccinated didn’t all get it on Oct 1. I assumed that on Oct 1 nobody was vaccinated and that subsequently on Nov 1, Dec 1 and Jan 1, 1/3 got vaccinated at each date with 1% waiting until Feb 1. Following Hill’s example, I assumed there was no difference in death rates between the 2 groups and that the death rate per month was 0.002058. Again following Hill’s work I produced the following table. Now if one analyses the data in the above Table, as per the NEJM (see Total line), the crude RR = 0.45, this is slightly different than the reported RR but to be expected as i had to guess what the actually monthly mortality rates were.\nHowever the essential point is that the unadjusted RR in my Table is derived when the data simulation has assumed a constant monthly mortality rate that is NOT different between the exposed and unexposed groups.\nIn other words, almost the entire effect can be explained by inaccurate classification of exposure times. Using the proper method of person time, as shown in Hill’s example, gives the correct RR = 1. It is quite amazing that 70 years after Hill’s publication, authors were still getting it wrong. It’s not only the original authors, reviewers and editors but also the whole medical commons. For example, in the 2007 letters to the editor that queried this unbelievable mortality result, nobody brought up this potential source of error, rather everyone tried to uniquely blame residual confounding. Moreover, as of March 23 2023, despite these initial confounding concerns, this publication has been cited 233 times, including 13 times in 2022. In only 5 of these 233 citations was the term “bias” also found and in those papers the discussion was again limited to confounding. Two potential confounding sources mentioned were i) increased vaccinations in the worried well ii) decreased vaccination in those deemed to have too many competing comorbidities for vaccination to make a difference. The majority of articles citing this 2007 publication therefore seem to have taken the results at face value. As an example, in 2023 we can find the following uncritically writing “prevent influenza-related hospitalizations and deaths among older adults” citing this 2007 publication as the reference.\nIn more modern times, this inaccurate accounting of exposure time and analysis has been termed immortal time bias by Suissa, one of the other co-instructors for our critical appraisal course and who suggested the article for review. Suissa has shown that immortal time bias and can lead to large discrepancies from the truth by either misclassification or selection bias.\nHernan and colleagues have proposed a paradigm of emulating a randomized trial to minimize the biases associated with observational studies, including immortal time. The key is align i) Elgibity ii) Assignment and iii) follow-up \\(T_o\\) times. Failure to align, as shown in the following figure may lead to i) prevalence bias ii) selection bias iii) immortal time (due to selection bias) and iv) immortal time bias (due to misclassification).\n\nSo, my general and specific take home messages are\nGeneral\n1) teaching is a great way to keep learning about the past and present\n2) answers to current problems can often be found in past work\n3) although science may be self correcting, the path to self correction can be exceedingly long\nSpecific\n1) when searching for biases, don’t stop with confounding as misclassification and selection bias may be hugely important\n2) in the presence of a large unexpected effect size, think IMMORTAL TIME BIAS (especially present in observational pharmacoepidemiology studies) as the magnitue of this bias may dwarf any confounding bias\n\n\n\nCitationBibTeX citation:@online{brophy2023,\n  author = {Brophy, Jay},\n  title = {Something Old and Something Older},\n  date = {2023-03-22},\n  url = {https://brophyj.github.io/posts/2023-03-22-my-blog-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrophy, Jay. 2023. “Something Old and Something Older.”\nMarch 22, 2023. https://brophyj.github.io/posts/2023-03-22-my-blog-post/."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "2023",
    "section": "",
    "text": "Presentations \n2023\n\n\nAn introduction to systematic reviews and (sometimes) meta-analysis\nMcGill University Medical School Lecture, Montreal QC, Nov 30 2023Summary - Review of basic systematic review and meta-analysis techniques for 2nd year medical students\n\n\n  Download slides \n\n\n\n\n\n\nA Bayesian view of contemporary randomized clinical trials – New reflections from an old lens\nMontreal Jewish General Hospital, Clinical Epidemiology Rounds, Montreal QC, Nov 15 2023Summary - Review of Bayesian principles and their advantages highlighted by re-analyses of several randomized trials recently published in the NEJM\n\n\n  Download slides \n\n\n\n\n\n\nDual antiplatelet therapy (DAPT) in 2023 - The McGill Experience\nMcGill University Health Centre, Department of Medicine Grand Rounds, Montreal QC, Oct 30 2023Summary - Review of three research studies performed at McGill - i) Bayesian network meta-analysis ii) pharmacoepidemiology study iii) cluster randomized registry trial\n\n\n  Download slides \n\n\n\n\n\n\nL’experience de l’unité d’évaluation des technologies (TAU) de CUSM - Regarder dans le rétroviseur pour mieux voir l’avenir (The Experience of McGill’s Technology Assessment Unit - Looking back to better see the future)\nSymposium en évaluation des technologies et modes d’intervention en santé, Centre hospitalier de l’Université de Montréal (CHUM), Montreal QC, Oct 19 2023Summary - Review of TAU’s lessons learned, the first hospital based technology assessment unit\n\n\n  Download slides (english)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James (Jay) Brophy",
    "section": "",
    "text": "James (Jay) Brophy is a tenured (full) professor with a joint appointment in the Departments of Medicine and Epidemiology and Biostatistics at McGill University. He is a cardiologist and does research in cardiovascular epidemiology. His research interests are eclectic and include outcomes research, pharmacoepidemiology, Bayesian statistics, health technology assessment, economic analyses and clinical research. He held a FRQS chair in health technology assessment and evidence-based medicine (2008-2023).\n He is a fellow of the Canadian Academy of Health Sciences and the Canadian Cardiovascular Society.\nHave a look at my  curriculum vitae"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "James (Jay) Brophy",
    "section": "Education",
    "text": "Education\n\nPhD Epidemiology & biostatistics - McGill University\n\nCardiology / Internal medicine - Université de Montreal\n\nMD - McMaster University\n\nM.Eng - McMaster University\n\nB.Eng - McGill University"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "James (Jay) Brophy",
    "section": "Interests",
    "text": "Interests\n\nCardiovascular medicine & epidemiology\nBayesian statistics\nEvidence based medicine\nData analysis & visualization\nKnowledge sharing\nMedical decision-making"
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Art",
    "section": "",
    "text": "(Click on image to enlarge)"
  },
  {
    "objectID": "example.html#barney-watercolours",
    "href": "example.html#barney-watercolours",
    "title": "Art",
    "section": "Barney watercolours",
    "text": "Barney watercolours\nBarney (2011-) my great friend, running partner and favorite muse\n\n\n\n\n\n\n\n\n\nResting after a busy day\n\n\n\n\n\n\n\nPround profile\n\n\n\n\n\n\n\nEnjoying the sunshine\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith a friend\n\n\n\n\n\n\n\nSnoozing\n\n\n\n\n\n\n\nSnoozing with a friend"
  },
  {
    "objectID": "example.html#barney-ipad-drawings",
    "href": "example.html#barney-ipad-drawings",
    "title": "Art",
    "section": "Barney iPad drawings",
    "text": "Barney iPad drawings\n\n\n\n\n\n\n\n\n\nStately\n\n\n\n\n\n\n\nWhere is everyone?\n\n\n\n\n\n\n\nWaiting for …"
  },
  {
    "objectID": "example.html#barney-photos",
    "href": "example.html#barney-photos",
    "title": "Art",
    "section": "Barney photos",
    "text": "Barney photos\n\n\n\nIn action"
  },
  {
    "objectID": "example.html#other-paintings",
    "href": "example.html#other-paintings",
    "title": "Art",
    "section": "Other paintings",
    "text": "Other paintings\n\n\n\n\n\n\n\n\n\nFlowers\n\n\n\n\n\n\n\nPoppies\n\n\n\n\n\n\n\nSheep\n\n\n\n\n\n\n\n\n\nFlowers1\n\n\n\n\n\n\n\nBunny\n\n\n\n\n\n\n\nFrom Sargent\n\n\n\n\n\n\n\n\n\nWinter\n\n\n\n\n\n\n\nWinter1\n\n\n\n\n\n\n\nZuna\n\n\n\n\n\n\n\n\nResting after a busy day\nPround profile\nEnjoying the sunshine\nWith a friend\nSnoozing\nSnoozing with a friend\nStately\nWhere is everyone?\nWaiting for …\nIn action\nFlowers\nPoppies\nSheep\nFlowers1\nBunny\nFrom Sargent\nWinter\nWinter1\nZuna"
  }
]