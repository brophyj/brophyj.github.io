{
  "hash": "7ea310477c71262bbae4d9927ff94b79",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: TAVR vs. surgery - NHST gets it wrong (again)\ndescription: \"Making intelligent inferences\"\nauthor:\n  - name: Jay Brophy\n    url: https://brophyj.github.io/\n    orcid: 0000-0001-8049-6875\n    affiliation: McGill University Dept Medince, Epidemiology & Biostatistics\n    affiliation-url: https://mcgill.ca \ntags: []\ncategories: [Bias, Statistical analysis]\nimage: preview-image.jpg\ncitation: \n  url: https://brophyj.github.io/posts/2024-02-19-my-blog-post/ \ndate: 2024-08-05T14:39:55-05:00\nlastmod: 2024-08-105T14:39:55-05:00\nfeatured: true\ndraft: false\n# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.\nprojects: []\ncode-fold: true\neditor_options: \n  markdown: \n    wrap: sentence\nbibliography: [bib.bib]\nbiblio-style: apalike\n---\n\n::: {.cell}\n\n:::\n\n\n## Background\n\nThe Notion-2 trial[@RN12215] was recently fast tracked for publication in the [European Heart Journal](https://pubmed.ncbi.nlm.nih.gov/38747246/). This study randomized low risk patients (≤75 years of age and median Society of Thoracic Surgeons (STS) risk score of 1.1%) with severe aortic stenosis (AS) to Transcatheter Aortic Valve Implantation (TAVI) or to conventional aortic valve surgery. The study population included both tricuspid and bicuspid AS.\n\nThe primary endpoint was a composite of all-cause mortality, stroke, or rehospitalization (related to the procedure, valve, or heart failure) at 12 months.\n\nA total of 370 patients were enrolled and 1-year incidence of the primary endpoint was 10.2% in the TAVI group and 7.1% in the surgery group [absolute risk difference 3.1%; 95% confidence interval (CI), −2.7% to 8.8%; hazard ratio (HR) 1.4; 95% CI, 0.7–2.9; P = .3]. The authors concluded\n\n[> \"Among low-risk patients aged ≤75 years with severe symptomatic AS, the rate of the composite of death, stroke, or rehospitalization at 1 year was similar between TAVI and surgery.\"]{style=\"color: red;\"}\n\n## How was the study designed?\n\nThe study was designed assuming a sample of 372 patients would provide the trial 90% power to show the non-inferiority of TAVI to surgery with regard to the primary endpoint at 1 year, assuming a Kaplan–Meier estimate of the primary endpoint of 10% in the TAVI group and 15% in the surgery group.  The authors stated \"**To test for non-inferiority, we determined whether the upper boundary of the 95% confidence interval (CI) for the difference in the rate of the primary endpoint between the TAVI and surgery group was less than the pre-specified non-inferiority margin of 5% points.**\"   \n\nIn other words, the null hypothesis ($H_O$) is $\\theta_{TAVI} - \\theta_{surgery} > 5%$ where $\\theta$ is the proportion of outcomes in the respective treatment arms. One hopes to reject this null hypothesis and accept the alternative hypothesis ($H_A$) that the difference is < 5% and therefore claim non-inferiority. Although the authors did claim non-inferiority, their data does not support this conclusions as the null hypothesis can't be rejected as the upper limit of the 95% confidence interval for the difference in outcomes between TAVR and surgery is 8.8% exceeding the prespecified non-inferiority margin of 5% points.    \n\n## How then did the authors reach their conclusion?\n\nThe authors apparently ignored their non-inferiority design and analysed their study with  conventional null hypothesis significance testing (NHST) and a null hypothesis $\\theta_{TAVI} - \\theta_{surgery} = 0$ which they were unable to reject it since the p value (0.3) exceeded the conventional  $\\alpha$ level (0.05)      \nNHST limitations and its tendency to cause cognitive errors has been described countless times in the medical literature[@RN3995][@RN5017]. This particular cognitive has even been described with the pithy aphorism **absence of evidence is not evidence of absence**[@RN4395].   \n\n## What is the strength of this conclusion?\n\nWith the NHST paradigm, it is impossible to quantify the evidence in favor of the null hypothesis. A non-significant finding can occur due to low power or a truly absent effect and the reporting of a p value simply can't  disentangled these two possibilities. An alternative to NHST is Null hypothesis Bayesian testing (NHBT) which allows the strength of evidence for (or against) $H_O$ and $H_A$ to be directly compared. This is most commonly achieved with Bayes factors, which quantifies the relative probabilities of the data under $H_O$ and $H_A$.    \n\n### BFs\nThe relationship between BFs and the relative support for for (or against) $H_O$ and $H_A$ is reflected in the following graphic.    \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(grid)\n\ng <- rasterGrob(c(\"lightgreen\", \"yellow\", \"orange\", \"red\"), \n                width=unit(1,\"npc\"), height = unit(1,\"npc\"), \n                interpolate = TRUE) \n# Create a continuous range for the y-axis\ny_values <- seq(0.01, 1000, length.out = 10000)\n\n# Create data frame for plotting\ndata <- data.frame(\n  BayesFactor = y_values\n)\n\n# Define a continuous color gradient from a deeper yellow to green to blue\ncolor_gradient <- c(\"#FFD700\", \"#00B300\", \"#0000FF\")\n\n# Plot the data\nggplot(data, aes(y = BayesFactor, fill = BayesFactor)) +\n  geom_tile(aes(x = 0.5), width = 0.2) +  # Greatly reduce the x-direction plot area\n  scale_y_continuous(trans = \"log10\", \n                     breaks = c(0.01, 1/3, 1, 3, 10, 30, 100, 1000), \n                     labels = c(\"0\", \"1/3\", \"1\", \"3\", \"10\", \"30\", \"100\", \"∞\"),\n                     expand = c(0, 0)) +\n  #scale_fill_gradientn(colors = color_gradient, name = NULL) +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 1/3, yend = 1/3), linetype = \"dashed\", color = \"black\") +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 3, yend = 3), linetype = \"dashed\", color = \"black\") +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 10, yend = 10), linetype = \"dashed\", color = \"black\") +\n  geom_segment(aes(x = 0.4, xend = 0.6, y = 30, yend = 30), linetype = \"dashed\", color = \"black\") +\n  annotate(\"text\", x = 0.5, y = sqrt(0.01 * 1/3), label = \"Evidence against\\ntreatment effect\", size = 4.5, hjust = 0.5, vjust = 0.5) +\n  annotate(\"text\", x = 0.5, y = 2, label = \"Not enough data to\\nknow if the drug works\", size = 4.5, hjust = 0.5, vjust = 0.5) +\n  annotate(\"text\", x = 0.5, y = 20, label = \"Evidence for\\ntreatment effect\", size = 4.5, hjust = 0.5, vjust = 0.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(30 * 500), label = \"{very strong pro-alternative}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(10 * 30), label = \"{strong pro-alternative}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(3 * 10), label = \"{moderate pro-alternative}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(1/3 * 3), label = \"{ambiguous}\", hjust = 0, size = 4.5) +\n  annotate(\"text\", x = 0.65, y = sqrt(0.01 * 1/3), label = \"{pro-null}\", hjust = 0, size = 4.5) +\n  coord_cartesian(xlim = c(0.4, 1), ylim = c(0.01, 1000), clip = \"off\") +  # Limit x-axis and ensure no clipping of text\n  theme_minimal() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"none\",\n    plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), \"cm\")  # Reduced overall plot area\n  ) +\n  labs(y = \"Bayes Factor\") +\n  annotation_custom(g, xmin=0.4, xmax=0.6, ymin=-2, ymax=Inf) +\n  ggtitle(\"Bayes factors and evidential strength\")\n\nggsave(\"output/BF.pdf\", dpi = 600, device = \"pdf\")\nggsave(\"output/BF.png\", dpi = 600, device = \"png\")\n```\n:::\n\n\n![](output/BF.png){width=70%}     \n\n### What is the Bayes Factor for the Notion-2 survival data? \nAn approach to calculating BFs within Cox proportional hazards survival models has been developed by Linde[@RN12229] and operationalized with the `baymedr` package[@baymedr].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# simulate Notion-2 dataset\nsim_data <- coxph_data_sim( # See ?coxph_data_sim for details\n  n_data = 100, # Number of data sets to be simulated\n  ns_c = 183, # Sample size (control condition)\n  ns_e = 187, # Sample size (experimental condition)\n  ne_c = 13, # Number of events (control condition)\n  ne_e = 19, # Number of events (experimental\n  # condition)\n  cox_hr = c(1.4, 0.7, 2.9), # HR, lower bound CI, upper bound CI\n  cox_hr_ci_level = 0.95, # Confidence level CI\n  maxit = 300, # Max number of PSO iterations (for\n  # psoptim())\n  maxit.stagnate = ceiling(300 / 5), # Max number of PSO iterations without\n  # reduction in loss (for psoptim())\n  cores = 5 # Number of cores to be used\n)\nsave(sim_data, file = \"output/sim_data.RData\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"~/Desktop/current/notion2/output/sim_data.RData\")\n\nsim_bf <- coxph_bf( # See ?coxph_bf for details\n  data = sim_data, # Object containing the data\n  null_value = 0, # H0 value\n  alternative = \"two.sided\", # H1 type (one- or two-sided)\n  direction = NULL, # H1 direction (low or high)\n  prior_mean = 0, # Beta prior mean\n  prior_sd = 1 # Beta prior SD\n)\n\nsim_bf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n******************************\nCox proportional hazards analysis\n---------------------------------\nH0:              beta == 0\nH1:              beta != 0\nNormal prior:    Mean = 0.000\n                 SD = 1.000\n\n    Median BF10 = 0.511\n    MAD SD BF10 = 1.439e-04\n******************************\n```\n\n\n:::\n:::\n\n\nThe median BF is 0.511 falls in the \"ambiguous\" range revealing that there is little evidence to support the null hypothesis of no difference between the two treatments.    \n\nThis result can be checked (approximately) by ignoring any time dependency and  simply considering  the outcome data in the form of a 2X2 contingency table. Here is the Notion-2 data in tabular form for the primary outcome.      \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#deaths\n#TAVR 187, 19\n#SAVR 183,\ndat <- matrix(c(19,168,13,170), nrow = 2, byrow = TRUE,\n              dimnames = list(c(\"TAVR\", \"SAVR\"),\n                              c(\"E+\", \"E-\")))\nkable(dat)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> E+ </th>\n   <th style=\"text-align:right;\"> E- </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> TAVR </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> SAVR </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 170 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe BF for this data structure is available with the `contingencyTableBF` function from the `BayesFactor` package[@bf]\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(BayesFactor)\nbf = contingencyTableBF(dat, sampleType = \"indepMulti\", fixedMargin = \"cols\")\nbf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBayes factor analysis\n--------------\n[1] Non-indep. (a=1) : 0.39 ±0%\n\nAgainst denominator:\n  Null, independence, a = 1 \n---\nBayes factor type: BFcontingencyTable, independent multinomial\n```\n\n\n:::\n:::\n\nThe result is, as expected, consistent with the earlier result again underlining the lack of any strong evidence to support the null hypothesis.    \nOf course, a close and proper interpretation of a standard statistical analysis provides the same inferences as shown by the binomial risk ratios and their 95% confidence intervals as shown below and published in the original manuscript.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(epiR)\nepi.2by2(dat, method = \"cohort.count\", digits = 2, conf.level = 0.95, \n         units = 100, interpret = FALSE, outcome = \"as.columns\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +           19          168        187      10.16 (6.23 to 15.41)\nExposed -           13          170        183       7.10 (3.84 to 11.84)\nTotal               32          338        370       8.65 (5.99 to 11.99)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 1.43 (0.73, 2.81)\nInc odds ratio                                 1.48 (0.71, 3.09)\nAttrib risk in the exposed *                   3.06 (-2.65, 8.77)\nAttrib fraction in the exposed (%)            30.08 (-37.37, 64.42)\nAttrib risk in the population *                1.54 (-3.15, 6.24)\nAttrib fraction in the population (%)         17.86 (-22.71, 45.02)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 1.094 Pr>chi2 = 0.296\nFisher exact test that OR = 1: Pr>chi2 = 0.356\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n```\n\n\n:::\n:::\n\n\n## Full Bayesian analysis\nThe BF approach has the advantage of not requiring a prior belief on the the relative risk of two interventions. However this comes at the expense of not being able to calculate the posterior probability distribution for the risk difference or ratio.     \nAs an initial approach one can assume a non-informative prior with a Beta(1,1) distribution so that the posterior distribution is completely determined by the observed Notion2 data.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npacman::p_load(brms, tidyverse, tidybayes, ggdist)\ndata_bin <- data.frame(N = c(183,187), y = c(13,19), grp2 = as.factor(c(\"Surgery\",\"TAVR\"))) \nf = bf(y | trials(N) ~ 0 + grp2)\n\n#get_prior(formula = f,data = data_bin,family = binomial(link = \"identity\"))\n\nm <- brm(\n  formula = f,\n  data = data_bin,\n  family = binomial(link = \"identity\"),\n  prior = c(prior(beta(1, 1), class = b, lb = 0, ub = 1)), # gets rid of a bunch of unhelpful warnings\n  chains = 4, warmup = 1000, iter = 2000, seed = 123,\n  refresh = 0\n)\n\nsave(m, file = \"output/m_brms\")\n```\n:::\n\n\nThis produces the following results for the risk differences\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nload(\"output/m_brms\")\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: binomial \n  Links: mu = identity \nFormula: y | trials(N) ~ 0 + grp2 \n   Data: data_bin (Number of observations: 2) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngrp2Surgery     0.08      0.02     0.04     0.12 1.00     3036     2467\ngrp2TAVR        0.11      0.02     0.07     0.15 1.00     3176     2066\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\nThese results can also be shown graphically\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraws <- brms::as_draws_df(m)\ndraws <- draws %>% \n  # rename and drop the unneeded columns\n  transmute(p0 = b_grp2Surgery,\n            p1 = b_grp2TAVR) %>% \n  # compute the OR\n  mutate(rr = p1 / p0, diff = p1-p0 )\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n\n# function that approximates the density at the provided values\napproxdens <- function(x) {\n  dens <- density(x)\n  f <- with(dens, approxfun(x, y))\n  f(x)\n}\n\nprobs <- c(0.145, 1) # sum(draws$diff>0)/4000\n\ndraws1 <- draws %>%\n  mutate(dy = approxdens(diff),                         # calculate density\n         p = percent_rank(diff),                        # percentile rank \n         pcat = as.factor(cut(p, breaks = probs,         # percentile category based on probs\n                              include.lowest = TRUE)))\n\nggplot(draws1, aes(diff, dy) ) +\n  geom_ribbon(aes(ymin = 0, ymax = dy, fill = pcat), alpha=.2) +\n  geom_line() +\n  scale_fill_brewer(guide = \"none\", palette = \"Set2\") +\n  labs(x = \"Risk difference (TAVR - surgery))\",\n       y = NULL) +\n  theme_classic() +\n  labs(title = \"Notion2 trial results with vague non-informative prior\", subtitle = \"Shaded area = probability increased TAVR risk (85.5%)\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis Bayesian analysis has provided a quantitative answer to the risk differences between TAVR and surgery and highlights the uncertainty that was missing from the original published conclusion. The BF approach to hypothesis testing provides an improvement over the traditional NHST by calculating the strength of the evidence, very weak support of the null hypothesis of no difference in this case.   \n\nOf course, rather than fixating on the p value, an examination of the 95% confidence interval (0.73, 2.81) shows that while this contains the null effect of 1, it is also compatible with a possible 27% reduction or 181% increase in risk with TAVR compared to SAVR. As these are meaningful differences, if becomes obvious that a claim of similarity in outcomes between the two procedures is not supported by the data, both from a frequentist and Bayesian viewpoints.   \n\n    \n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}